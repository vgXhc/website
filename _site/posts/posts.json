[
  {
    "path": "posts/2022-10-28-map-fun-with-city-of-madison-tax-parcel-data/",
    "title": "Map fun with City of Madison tax parcel data",
    "description": "Building age, property use, and architectural styles in Madison",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-02-21",
    "categories": [
      "Madison (WI)",
      "housing",
      "map"
    ],
    "contents": "\r\nThe City of Madison’s open data portal has a tax parcel dataset. Tax information, however, is not the only thing included in the dataset. There are over 140 variables and for this post we will look at three of them: The year the structure on the parcel was built, the use of the property, and the architectural style.\r\n\r\nCorrection 2023-02-21: The originally published version of this post used the BuiltYear variable for the map of the age of buildings. Jim Kreft pointed out that there is another variable in the dataset, Maxconstru, which has more complete data. The code and map have been updated accordingly.\r\n\r\n\r\nShow code\r\n\r\nlibrary(sf)\r\nlibrary(tidyverse)\r\n\r\n\r\nbuildings <- read_sf(\"/Users/user1/Downloads/Tax_Parcels_(Assessor_Property_Information)/Tax_Parcels_(Assessor_Property_Information).shp\")\r\nbuildings <- buildings |> \r\n  st_make_valid()\r\n\r\n\r\nBuilding age\r\nLet’s start with the YearBuilt variable. This allows us a glimpse into the development of Madison over time. First, a static map:\r\n\r\n\r\nShow code\r\n\r\nbuildings2 <- buildings |> \r\n  mutate(MaxConstru = na_if(MaxConstru, 0),\r\n         MaxConstru_labels = cut(MaxConstru,\r\n                                breaks = c(-Inf, 1900, 1945, 1980, 2000, 2010, Inf),\r\n                                labels = c(\"pre-1900\", \"1900 to 1944\", \"post-WWII to 1979\", \"1980 to 1999\", \"2000 to 2009\", \"2010 and newer\"),\r\n                                ordered_result = T))\r\n\r\nlibrary(tmap)\r\ntmap_mode(\"plot\")\r\ntmap_options(bg.color = \"black\", legend.text.color = \"white\")\r\np <- buildings2 |> \r\n  select(MaxConstru_labels) |> \r\n  tm_shape() +\r\n  tm_polygons(title = \"\", \"MaxConstru_labels\", border.alpha = 0, style = \"cont\",\r\n              colorNA = \"darkgrey\") +\r\n  tm_credits(\"Map: @HaraldKliems. Data: City of Madison Tax Parcels\", col = \"white\") +\r\n  tm_layout(title = \"How old are Madison’s buildings?\",\r\n            scale = 1,\r\n            bg.color = \"black\",\r\n            legend.text.color = \"white\",\r\n            title.color = \"white\",\r\n            legend.title.color = \"white\")\r\n\r\n# tmap_save(p, filename = \"c:/Users/user1/Desktop/map.png\", width = 12, height = 9, dpi = 600)\r\np\r\n\r\n\r\n\r\nIt’d be nice to have an interactive map, but the dataset is too large to not crash most browsers (stay tuned; I’m working on alternatives).\r\nIn the static map it’s not easy to see areas where parcels have redeveloped, such as along the East Washington Ave corridor, but we can make out some glimpses. We also see that a lot of parcels have missing data, and that missingness doesn’t seem to be random. It’d be interesting to know why the city has better data on some properties/areas/eras than on others!\r\nProperty use\r\nThe age of a structure only tells us part of a story. How is a building use, and how do these uses cluster? The property use data has many different categories, and to keep the map legible we need to lump a lot of them together. This is what we get for the 10 most common uses:\r\n\r\n\r\nShow code\r\n\r\np2 <- buildings2 |>\r\n  select(PropertyUs) |>\r\n  mutate(PropertyUs = case_when(\r\n    str_detect(PropertyUs, \"Apartment$\") ~ \"Apartment\",\r\n    str_detect(PropertyUs, \"^Condominium\") ~ \"Condominium\",\r\n    TRUE ~ PropertyUs\r\n  )) |>\r\n  mutate(PropertyUs_collapsed = fct_lump_n(PropertyUs, 10)) |>\r\n  tm_shape() +\r\n  tm_polygons(\r\n    title = \"\",\r\n    \"PropertyUs_collapsed\",\r\n    border.alpha = 0,\r\n    colorNA = \"ghostwhite\"\r\n  ) +\r\n  tm_credits(\"Map: @HaraldKliems. Data: City of Madison Tax Parcels\") +\r\n  tm_layout(\r\n    title = \"Property use for tax parcels in Madison\",\r\n    scale = .6,\r\n    bg.color = \"black\",\r\n    legend.text.color = \"white\",\r\n    title.color = \"white\",\r\n    legend.title.color = \"white\"\r\n  )\r\n\r\ntmap_mode(\"plot\")\r\np2\r\n\r\n\r\n\r\nHome style\r\nA third and final map looks at the “home style.”\r\n\r\n\r\nShow code\r\n\r\np3 <- buildings2 |>\r\n  select(HomeStyle) |>\r\n  mutate(\r\n    HomeStyle = case_when(\r\n      str_detect(HomeStyle, \"^Ranch\") ~ \"Ranch\",\r\n      str_detect(HomeStyle, \"^Townhouse\") ~ \"Townhouse\",\r\n      str_detect(HomeStyle, \"^Garden\") ~ \"Garden\",\r\n      TRUE ~ HomeStyle\r\n    ),\r\n    HomeStyle_collapsed = fct_lump_n(HomeStyle, 10)\r\n  ) |>\r\n  tm_shape() +\r\n  tm_polygons(\"HomeStyle_collapsed\",\r\n              border.alpha = 0,\r\n              colorNA = \"ghostwhite\") +\r\n  tm_credits(\"Map: @HaraldKliems. Data: City of Madison Tax Parcels\") +\r\n  tm_layout(\r\n    title = \"Architectural style of Madison buildings\",\r\n    scale = .6,\r\n    bg.color = \"black\",\r\n    legend.text.color = \"white\",\r\n    title.color = \"white\",\r\n    legend.title.color = \"white\"\r\n  )\r\n\r\ntmap_mode(\"plot\")\r\np3\r\n\r\n\r\n\r\nLumping together the less common styles under “Other” is a little unfortunate for this variable. It’ll make use miss out on less common but interesting architectural styles such as “Spanish mediterranean” or “New style modern international”. So let’s at least have a table with all the styles and how common they are.\r\n\r\n\r\nShow code\r\n\r\nbuildings2 |> st_drop_geometry() |>\r\n  select(HomeStyle) |>\r\n  mutate(HomeStyle = case_when(str_detect(HomeStyle, \"^Ranch\") ~ \"Ranch\",\r\n                               str_detect(HomeStyle, \"^Townhouse\") ~ \"Townhouse\",\r\n                               str_detect(HomeStyle, \"^Garden\") ~ \"Garden\",\r\n                               TRUE ~ HomeStyle)) |>\r\n           group_by(HomeStyle) |>\r\n           tally(sort = T) |>\r\n  DT::datatable()\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-28-map-fun-with-city-of-madison-tax-parcel-data/map-fun-with-city-of-madison-tax-parcel-data_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-02-21T06:54:24-06:00",
    "input_file": "map-fun-with-city-of-madison-tax-parcel-data.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-02-18-using-knitr-and-rmarkdown-to-format-google-forms-responses/",
    "title": "Candidate questionnaires for local elections",
    "description": "Using `knitr` and `RMarkdown` for formatting form responses",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-02-18",
    "categories": [
      "Madison (WI)",
      "Google Forms",
      "Madison Bikes",
      "RMarkdown",
      "knitr"
    ],
    "contents": "\r\n\r\nThe non-profit I am involved with, Madison Bikes, has been doing candidate questionnaires for local elections for several election cycles now.\r\nWith each iteration, we learn some things about the best process to make this happen.\r\nHow do you keep track of candidates?\r\nHow do you best collect responses?\r\nAnd then how do you present those responses to your community?\r\nFor the 2023 spring primary and general elections, we used a new process that takes some of those lessons learned and saved us a lot of work. In this post, I will describe the process with the goal of documenting for ourselves and also allowing others to adapt it to their own purposes. The first sections are about the logistics and of interest to anyone who wants to do candidate questionnaires. The second part gets into the technical process of using R programming to take the responses and format them.\r\nIf you want to see what the end result looks like before reading the post, here’s the link.\r\nIdentifying candidates and their contact information\r\nThe first step to a candidate questionnaire is to keep track of who is running.\r\nThe timeline for filing for candidacy is short:\r\nSource: City of Madison Clerk’s Office\r\nDecember 1, 2022\r\nFirst day nomination papers may be circulated.\r\nDecember 23, 2022\r\nDeadline for incumbents not seeking re-election to file Notice of Non-Candidacy.\r\nJanuary 3, 2023\r\nAll papers and forms due in City Clerk’s Office at 5 p.m.\r\nJanuary 6, 2023\r\nDeadline to challenge nomination papers.\r\nI set up a Google Sheet early on and manually kept it up to date as I saw new candidates announcing (or incumbents announcing their non-candidacy), mostly relying on the Clerk’s Office’s candidate filings but also media coverage.\r\nAs we got closer to the deadline, I also started collecting candidates’ websites, social media links, and email addresses.\r\nThis involved a lot of internet searching and was very time consuming.\r\nI later found out that candidates provide an email address to the Clerk’s Office on their official paperwork.\r\nWhen we do this again for the next election cycle, I would no longer manually search and just wait until after the deadline.\r\nDeveloping questions\r\nIn previous years Madison Bikes did their own questionnaire.\r\nWhen we learned that Madison is for People, a housing advocacy organization, was also interested in doing a questionnaire, we connected with them to coordinate efforts.\r\nWe also got Madison Area Bus Advocates on board and developed a joint questionnaire.\r\nPartnering with other organization has the advantage of being able to share some of the logistical work, but it also creates additional coordination work.\r\nOverall, this went very smoothly.\r\nWe needed to make sure that the questionnaire didn’t become too burdensome for the candidates, while still providing meaningful content for our organizations’ respective audiences.\r\nIn the end, each organization provided three to four questions, plus three short joint questions about how candidates get around the city.\r\nWe had slightly different questions for mayoral candidates and candidate for the common council.\r\nDistributing the questions, collecting answers\r\nWe split up the work of emailing candidates between three people (Thanks, Connor, Will, and Jonathan!).\r\nEmails were sent individually, and I think this is something that in the future we could consider automating, e.g. by using Mailchimp or Qualtrics.1\r\nWe tracked the send-out in the same Google Sheet where we kept the contact information.\r\nWe discussed several options for receiving candidate responses.\r\nShould they just sent their responses in an email?\r\nPut them into a Google Doc?\r\nBased on our prior experience we decided to go with a Google Form.\r\nWith over 40 candidates total, anything else would likely have turned into a nightmare.\r\nThe Google Form had a text field for the candidate’s name, a dropdown menu for their district (don’t use a text field for this!), and text fields for each candidate question.\r\nWe carefully tested the survey, for example making sure that the respondents didn’t need a Google account.\r\nThen a link to the survey was included in the emails.\r\nTurning a form into HTML\r\nThe deadline for candidates to respond was only a week before the primary election date.\r\nTurning the form responses into nicely formatted content for our website quickly was therefore very important.\r\nI ended up using RMarkdown and knitr as the main tools for this, relying heavily on the wonderful “R Markdown: The Definitive Guide” and the “R Markdown Cookbook”.\r\nIt’s very much possible that there are better/different tools for this out there, but RMarkdown is what I was familiar with and the final product turned out well.\r\nYou can find the repository with all code on Github: https://github.com/vgXhc/sheets-2-wp\r\nThe Madison Bikes website runs on WordPress, which limited our options of what we could do.\r\nIn the last round of candidate questionnaires, Ben had created a script to create html tags around the responses read in from a csv file.\r\nWe then copied that html into a WordPress custom html block.\r\nThis seemed to work well enough and so I also aimed to create an html document to paste into WordPress, but instead of bare html it would be an html document generated by knitr.\r\nLet’s walk through this step by step.\r\nThere are three main files:\r\ncombined-report.Rmd: This is the file that eventually gets knitted and produces the html and pdf output\r\noutput-council-child.Rmd: The template for generating content blocks within combined-report.Rmd for council candidates.\r\noutout-mayor-child.Rmd: Same, but for mayoral candidates\r\ncombined-report.Rmd has a full yaml header, both for html and pdf output.2\r\n---\r\ntitle: \"Candidate Q&A\"\r\noutput: \r\n  pdf_document:\r\n    toc: TRUE\r\n    toc_depth: 1\r\n  html_document:\r\n    toc: TRUE\r\n    toc_depth: 1\r\n    toc_float: true\r\ndocumentclass: scrreport\r\n---\r\nOne thing we didn’t have last time was a table of contents (TOC), which made it difficult to navigate through the Q&A.\r\nThe toc: true option easily generates the TOC.\r\nA floating TOC would have been even better (it really helps to keep track of where in the document you are) , but since that relies on Bootstrap and jQuery, it doesn’t work in a WordPress html block.\r\nSince Madison is for People and Madison Area Bus Advocates use different content management systems, I rendered a version with and without the floating TOC.\r\nFor the pdf version, I specified the LaTeX document class scrreport from the KOMA-Script bundle.\r\nAfter the yaml header there is a bunch of code to prep the data.\r\nThe form responses are read in with the googlesheets4 package.\r\nThey come in as wide data, with one row for each candidate and one column for each question.\r\nThe code then transforms the data into a long format, with one row for each question and answer.\r\nTo help with formatting, the question is split into a “topic” (everything before the colon) and the actual question.3\r\nThere’s some duplication in the code that could be cleaned up with a function.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(googlesheets4)\r\nlibrary(here)\r\nlibrary(knitr)\r\nresponses_council <-\r\n  read_sheet(\r\n    \"https://docs.google.com/spreadsheets/d/1zMTl2BQzQ231SFjN8RRKvZm3PidXSoS2tVDFXSn5AwU/\",\r\n    col_types = \"cccccccccccccccc\"\r\n  ) |>\r\n  mutate(\r\n    district = factor(\r\n      `Which district are you running for?`,\r\n      c(\r\n        \"District 1\",\r\n        \"District 2\",\r\n        \"District 3\",\r\n        \"District 4\",\r\n        \"District 5\",\r\n        \"District 6\",\r\n        \"District 7\",\r\n        \"District 8\",\r\n        \"District 9\",\r\n        \"District 10\",\r\n        \"District 11\",\r\n        \"District 12\",\r\n        \"District 13\",\r\n        \"District 14\",\r\n        \"District 15\",\r\n        \"District 16\",\r\n        \"District 17\",\r\n        \"District 18\",\r\n        \"District 19\",\r\n        \"District 20\"\r\n      )\r\n    ),\r\n    district_short = str_remove(district, \"istrict \")\r\n  ) |>\r\n  arrange(district, `Your name`)\r\nresponses_mayor <-\r\n  read_sheet(\r\n    \"https://docs.google.com/spreadsheets/d/1T3hQ20IDetkjQ71qj5nZqRiZ8fZ8XHoKDCiUKaIPs9Y/\"\r\n  )\r\nresponses_council_long <- responses_council |>\r\n  pivot_longer(\r\n    cols = 6:ncol(responses_council) - 2,\r\n    names_to = \"question\",\r\n    values_to = \"answer\"\r\n  ) |>\r\n  rename(name = `Your name`) |>\r\n  mutate(\r\n    topic = str_extract(question, \"^(.*?)\\\\:\"),\r\n    # everything before the colon\r\n    question = if_else(is.na(topic), question, str_remove(question, topic))\r\n  )\r\nresponses_mayor_long <- responses_mayor |>\r\n  mutate(district = \"Mayoral candidate\",\r\n         district_short = \"mayoral cand.\") |>\r\n  pivot_longer(\r\n    cols = 3:ncol(responses_mayor),\r\n    names_to = \"question\",\r\n    values_to = \"answer\"\r\n  ) |>\r\n  rename(name = `Your name`) |>\r\n  mutate(\r\n    topic = str_extract(question, \"^(.*?)\\\\:\"),\r\n    # everything before the colon\r\n    question = if_else(is.na(topic), question, str_remove(question, topic))\r\n  )\r\n\r\n\r\n\r\n\r\nstr(responses_council)\r\n\r\ntibble [19 × 18] (S3: tbl_df/tbl/data.frame)\r\n $ Timestamp                                                                                                                                                                                                                                                                                                                                                                                                             : chr [1:19] \"2/14/2023 22:30:06\" \"2/14/2023 19:08:25\" \"2/5/2023 22:06:43\" \"2/11/2023 8:42:33\" ...\r\n $ Your name                                                                                                                                                                                                                                                                                                                                                                                                             : chr [1:19] \"John W. Duncan\" \"Colin Barushok\" \"Juliana Bennett\" \"Derek Field\" ...\r\n $ Which district are you running for?                                                                                                                                                                                                                                                                                                                                                                                   : chr [1:19] \"District 1\" \"District 2\" \"District 2\" \"District 3\" ...\r\n $ When was the last time you took the bus?                                                                                                                                                                                                                                                                                                                                                                              : chr [1:19] \"The last time I rode a public transit bus was when I lived in Boston, where commuter routes were convenient for\"| __truncated__ \"September 2022\" \"Early this week, 1/31/2023\" \"Thursday, two days ago\" ...\r\n $ When was the last time you rode your bike to work, to school, or for an errand?                                                                                                                                                                                                                                                                                                                                       : chr [1:19] \"A lack of designated bike lanes along busy roads does not allow for safe commuting to work or to run errands in this area.\" \"During summer 2022\" \"Today, 2/2/2023\" \"Three years ago when I lived closer to downtown\" ...\r\n $ What is the primary way you move around the city?                                                                                                                                                                                                                                                                                                                                                                     : chr [1:19] \"The safest and best option for commuting for those of us on the far-west side is usually by car.\" \"Walking\" \"walk or carpool\" \"Bus for work, walking around the neighborhood, and car for errands\" ...\r\n $ General Vision:  2023 brings many significant changes for Metro including the beginning of Bus Rapid Transit implementation, a complete redesign of the transit network, and policy changes such as Transportation Demand Management and Transit Oriented Development. Do you support the current direction of Madison’s transit plans, and what is your vision for Madison’s transit system in the mid and long term?: chr [1:19] \"I support the decision to increase the frequency and capacity of bus service along high-demand routes in the ci\"| __truncated__ \"I support Bus Rapid Transit, efforts toward Transportation Demand Management, and Transit Oriented Development.\"| __truncated__ \"Over the past two years, I have been supportive of BRT, Metro Network Redesign, Vision Zero, TOD and more that \"| __truncated__ \"I'm excited for the areas of the city whose transit service is improving with higher frequency, including along\"| __truncated__ ...\r\n $ Sustainable Funding: In the Metro redesign, funding constraints limiting the budget to 2019 levels required compromises in network coverage, hours of service, and frequency outside a handful of core routes. What would you do to establish more sustainable funding to improve Metro’s quality of service?                                                                                                         : chr [1:19] \"The unfortunate reality facing growing cities throughout the country, including Madison, is that budget dollars\"| __truncated__ \"We should make sure BRT is efficient and convenient to encourage more riders for a wider variety of trips. This\"| __truncated__ \"We are in a budget deficit, yes. We are also at a point where we need to get our priorities in order of what we\"| __truncated__ \"Transit service must be a budget priority going into some tough upcoming budget years and I'll remain an advoca\"| __truncated__ ...\r\n $ Accessibility: The Metro network redesign has increased the distance to the nearest bus stop for some residents. This is a major concern for bus riders with limited mobility. What measures would you take to ensure riders with mobility limitations are well-served by our transit system? What do you think the role of paratransit is in this regard?                                                            : chr [1:19] \"The goal of our transit system should be to provide equitable access to fixed-route buses. In the long term, I \"| __truncated__ \"Madison is required to provide transportation for everybody regardless of their ability. The city should invest\"| __truncated__ \"From my time on Council, I have seen how the city is not always good at including community voices, especially \"| __truncated__ \"This is a concern in my district as well - one that I've discussed with a few residents at their doors while ca\"| __truncated__ ...\r\n $ Historic Preservation: There have been conflicts between the priorities of promoting new housing development and preserving historically significant buildings and neighborhoods in recent years. What specific factors would you consider when balancing new development against preservation, and how much weight would you give to the different factors?                                                          : chr [1:19] \"Historic preservation, while desirable, has often been used as a tool for excluding historically marginalized c\"| __truncated__ \"District 2 is unique because it is the most dense district in the city and the district with the highest concen\"| __truncated__ \"Oftentimes, we view historic preservation as binary subjects. Supporting historic preservation and the city’s n\"| __truncated__ \"Madison is experiencing a housing crisis. We need to balance preservation of historically significant sites wit\"| __truncated__ ...\r\n $ Housing Affordability: The City's 2022 Housing Snapshot indicated that more housing was needed at all income levels, including both affordable housing and market rate housing. What is your plan to ensure housing is built that is available at all income levels?                                                                                                                                                  : chr [1:19] \"Meeting our city's housing needs will require a multipronged approach that involves collaborating with develope\"| __truncated__ \"I would support policies that encourage density in more parts of the city, along transit routes, and in unused \"| __truncated__ \"Increasing affordable housing is one of my top priorities for the upcoming term. My plan to increase housing is\"| __truncated__ \"Housing is one of my core campaign issues. My partner and I experienced Madison's housing crisis for ourselves \"| __truncated__ ...\r\n $ Zoning Reform: Municipalities across the country, including Portland, Minneapolis, and Charlotte have taken steps to reform zoning by eliminating parking minimums and allowing for small multi-family buildings by-right throughout the city. Would you support similar reforms in Madison? Why or why not?                                                                                                          : chr [1:19] \"Yes, I support these types of initiatives. I believe that zoning reform is necessary to meet future housing dem\"| __truncated__ \"Yes I support these policies. We are trying to encourage less driving for environmental reasons, and we are als\"| __truncated__ \"Yes of course! I have been supportive of such reforms, including zoning reforms for missing middle housing. In \"| __truncated__ \"I believe that zoning is a policy tool, and that it was implemented by the community many decades ago in an att\"| __truncated__ ...\r\n $ Complete Green Streets: Madison recently adopted a Complete Green Streets policy that prioritizes walking, biking, transit, and green infrastructure over driving and car parking when it comes to allocating our public right of way. Are you committed to implementing this policy, especially when a project requires the removal of car parking or inconveniencing drivers?                                       : chr [1:19] \"I support the Complete Green Streets policy that the city has pursued, and I am committed to implementing it. W\"| __truncated__ \"I'm committed to implementing complete green streets, encouraging less single occupancy modes of travel, improv\"| __truncated__ \"Yes definitely! I was happy to support Complete Green Streets. Madison should be a 15 minute city, meaning anyo\"| __truncated__ \"Yes. I believe in the objectives of this policy.\" ...\r\n $ Vision Zero: Madison committed to eliminating all fatalities and serious injuries from traffic crashes by 2035. Yet in 2022, 14 people were killed, including 3 people riding bikes, and 74 were seriously injured. Which roadways and intersections in your district should be prioritized for safety improvements, and what strategies would you use to ensure improvements are implemented?                        : chr [1:19] \"Rapid development in my district continues to prioritize the needs of drivers. Long stretches of increasingly b\"| __truncated__ \"The most concerning intersection is W Gorham Street becomes University Avenue at Basset Street. Cars routinely \"| __truncated__ \"Vision Zero is an excellent program, but it is still in its pilot stage. In the next two years, I would like to\"| __truncated__ \"Traffic safety is one of the top issues residents raise with me as I canvass neighborhoods in my district. I ap\"| __truncated__ ...\r\n $ Bike Network: Madison Bikes wants all residents to have access to a low-stress bike network that makes biking safe and convenient for people of all ages and abilities, no matter where they live in the city. Where in your district do you see major gaps in this network and how would you propose to fix these gaps?                                                                                              : chr [1:19] \"Significant gaps currently exist in the bike network on most major roads in my district. As the city plans the \"| __truncated__ \"Westbound University Avenue has a non protected bike lane that is between auto lanes and bus lanes. This has lo\"| __truncated__ \"My district downtown is fairly bike and pedestrian friendly. I believe that we could add protected bike lanes a\"| __truncated__ \"I see major gaps in the network right in my district. Neighborhoods on the far east side are rigidly segmented \"| __truncated__ ...\r\n $ Transportation Climate Impact: In Madison, about 40% of greenhouse gas emissions come from transportation. How do you think the city should go about reducing emissions from that sector over the next 5 years?                                                                                                                                                                                                       : chr [1:19] \"In addition to making investments in expanding our public transit system and bike network, I believe the city s\"| __truncated__ \"We must remain steadfast in our efforts to provide the best possible infrastructure for bicycles, safer streets\"| __truncated__ \"I like that Bus Rapid uses electric buses. It is a great start to ensure that our entire city fleet uses electr\"| __truncated__ \"Within transportation, I think Madison should be reducing its greenhouse gas emissions by offering clear, conve\"| __truncated__ ...\r\n $ district                                                                                                                                                                                                                                                                                                                                                                                                              : Factor w/ 20 levels \"District 1\",\"District 2\",..: 1 2 2 3 3 4 5 6 6 9 ...\r\n $ district_short                                                                                                                                                                                                                                                                                                                                                                                                        : chr [1:19] \"D1\" \"D2\" \"D2\" \"D3\" ...\r\n\r\nstr(responses_council_long)\r\n\r\ntibble [247 × 8] (S3: tbl_df/tbl/data.frame)\r\n $ Timestamp                          : chr [1:247] \"2/14/2023 22:30:06\" \"2/14/2023 22:30:06\" \"2/14/2023 22:30:06\" \"2/14/2023 22:30:06\" ...\r\n $ name                               : chr [1:247] \"John W. Duncan\" \"John W. Duncan\" \"John W. Duncan\" \"John W. Duncan\" ...\r\n $ Which district are you running for?: chr [1:247] \"District 1\" \"District 1\" \"District 1\" \"District 1\" ...\r\n $ district                           : Factor w/ 20 levels \"District 1\",\"District 2\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n $ district_short                     : chr [1:247] \"D1\" \"D1\" \"D1\" \"D1\" ...\r\n $ question                           : chr [1:247] \"When was the last time you took the bus?\" \"When was the last time you rode your bike to work, to school, or for an errand?\" \"What is the primary way you move around the city?\" \"  2023 brings many significant changes for Metro including the beginning of Bus Rapid Transit implementation, a\"| __truncated__ ...\r\n $ answer                             : chr [1:247] \"The last time I rode a public transit bus was when I lived in Boston, where commuter routes were convenient for\"| __truncated__ \"A lack of designated bike lanes along busy roads does not allow for safe commuting to work or to run errands in this area.\" \"The safest and best option for commuting for those of us on the far-west side is usually by car.\" \"I support the decision to increase the frequency and capacity of bus service along high-demand routes in the ci\"| __truncated__ ...\r\n $ topic                              : chr [1:247] NA NA NA \"General Vision:\" ...\r\n\r\nThe next step took me a while to figure out: How can I use the long data frame and iterate both over each candidate but also over each question to produce the desired markdown content?\r\nI first tried rendering separate html documents for each candidate, but the downside of that approach is that then I’d have to set up separate pages on WordPress and copy-and-paste dozens of html files.\r\nRendering a single html file required a different approach: the knitr::knit_child function.\r\nThe documentation in the “R Markdown Cookbook” wasn’t super easy to follow, but eventually I got it!\r\nFirst we define a function that take the candidate name as its input.\r\nThe responses are then filtered to just those of that candidate.\r\nThe district and district_short variables for that candidate are created–those are used for formatting the output.\r\nThen comes the key piece: we ask knitr to knit a child document using the output_council_child.Rmd template.\r\n\r\n\r\n\r\nThe child template is pretty simple: No yaml header and just a bunch of markdown plus inline R code to format the output.\r\nHere are the first few lines.\r\nNow that we have the function to knit the child document defined, we can just iterate over all the candidate names with lapply and then turn the list output into simple text separated by line breaks (the \\n).\r\nBecause of the chunk option results='asis' the Markdown generated just gets inserted into the rest of the Markdown in combined-report.Rmd and then knitted into html (or pdf).\r\n{r results='asis', echo=FALSE}\r\nres <- lapply(responses_mayor$`Your name`, knit_answer_child_mayor)\r\ncat(unlist(res), sep = '\\n')\r\nNow we just do the same for the common council candidates (different child template because the number of questions is different) and that’s it!\r\nNow you can open the generated html file in your text editor (or your web browser’s source viewer) and copy the content into the custom html block in WordPress!\r\nAnd this is what the end result looks like: https://www.madisonbikes.org/madison-spring-elections-2023/\r\nEnhancements\r\nOne key enhancement would be to automate the publishing to WordPress.\r\nRather than copying-and-pasting the html into WordPress, wouldn’t it be nice if the content could be pushed through the WordPress API?\r\nThere are some packages and tutorials on how to do this within R, but the packages are very old and I didn’t have much luck with the tutorials.\r\n\r\nOn the other hand, this may decrease response rates.↩︎\r\nUsing the knit button in RStudio only seems to produce the first specified output format.\r\nSo you may have to manually edit the yaml header.↩︎\r\nFor the next iteration, it may make sense to fix this in the form itself.↩︎\r\n",
    "preview": "posts/2023-02-18-using-knitr-and-rmarkdown-to-format-google-forms-responses/vote.jpg",
    "last_modified": "2023-02-19T09:00:06-06:00",
    "input_file": "using-knitr-and-rmarkdown-to-format-google-forms-responses.knit.md"
  },
  {
    "path": "posts/2022-12-03-bicycle-and-pedestrian-intersection-crashes-in-madison/",
    "title": "Bicycle and pedestrian intersection crashes in Madison",
    "description": "A quick look at crash data to support efforts to no longer allow right turns on red.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-12-03",
    "categories": [
      "Madison (WI)",
      "Vision Zero",
      "map"
    ],
    "contents": "\r\nThere are some efforts underway to no longer allow right turns on red in Madison.\r\nFor people walking and biking, right-turning motor vehicles pose a threat, as is demonstrated in this video of an intersection where I myself recently had a very close call. Other cities in the US, such as Washington (DC) or cambridge (MA) are in the process of eliminating right turns on red.\r\nFor our local effort, I offered to help with some crash data analysis. It is difficult to determine how many crashes can be directly attributed to right on red, we can at least answer the question: How many crashes involving people walking and biking occur at signalized intersections?\r\nCrash data\r\nCrash data is available from Community Maps the statewide crash data reporting portal.\r\nFor registered users, one of the available filters is for “intersection” crashes.\r\nWe start by downloading all crashes in the City and Town of Madison that have an intersection flag as well as the bicycle or pedestrian flag.\r\nScreenshot of Community Maps search options\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\nlibrary(tmap)\r\ntmap_mode(\"view\")\r\n\r\n\r\nped_crashes <- read_csv(\"data/crash-records.csv\")\r\nbike_crashes <- read_csv(\"data/crash-records(1).csv\")\r\nall_crashes <- rbind(ped_crashes, bike_crashes)\r\n\r\nall_crashes <- all_crashes |> \r\n  mutate(bike_ped = case_when(\r\n    PEDFLAG == \"Y\" ~ \"ped\",\r\n    BIKEFLAG == \"Y\" ~ \"bike\"\r\n  ),\r\n  severity = case_when(\r\n    INJSVR == \"K\" ~ \"Fatality\",\r\n    INJSVR == \"A\" ~ \"Serious injury\",\r\n    INJSVR == \"B\" ~ \"Minor injury\",\r\n    INJSVR == \"C\" ~ \"Possible injury\",\r\n    INJSVR == \"O\" ~ \"No injury\"\r\n  ),\r\n  severity = factor(severity, \r\n                    levels = c(\"Fatality\",\r\n                               \"Serious injury\",\r\n                               \"Minor injury\",\r\n                               \"Possible injury\",\r\n                               \"No injury\"))\r\n  )\r\n\r\n\r\nMapping the crashes shows that something weird is going on:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(st_as_sf(all_crashes, coords = c(\"LONGITUDE\", \"LATITUDE\")))+\r\n  tm_dots()\r\n\r\n\r\n\r\nCrashes in the middle of the ocean?\r\nNo: Crashes that were not geocoded and therefore have coordinates of 0,0.\r\nI looked up the crash reports for all these and manually added coordinates:\r\n\r\n\r\nShow code\r\n\r\n#manually fix locations of one unmapped crash\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245X0S\",]$LATITUDE <- 43.08952868325321\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245X0S\",]$LONGITUDE <- -89.48608767371645\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WX5\",]$LATITUDE <- 43.0608101752285\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WX5\",]$LONGITUDE <- -89.50257794338208\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0FV1GG7\",]$LATITUDE <- 43.05027679273423\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0FV1GG7\",]$LONGITUDE <- -89.4000044121047\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L048CTMJ\",]$LATITUDE <- 43.059824781314845\r\nall_crashes[all_crashes$DOCTNMBR == \"01L048CTMJ\",]$LONGITUDE <- -89.40061771700978\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WVQ\",]$LATITUDE <- 43.03278427434284\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WVQ\",]$LONGITUDE <- -89.4597152111326\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L048CTLT\",]$LATITUDE <- 43.07325624571523\r\nall_crashes[all_crashes$DOCTNMBR == \"01L048CTLT\",]$LONGITUDE <- -89.40242558562917\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0CR2KRW\",]$LATITUDE <- 43.07325624571523\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0CR2KRW\",]$LONGITUDE <- -89.40242558562917\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0W78HMJ\",]$LATITUDE <- 43.06057645635835\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0W78HMJ\",]$LONGITUDE <- -89.50293626341433\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WSZ\",]$LATITUDE <- 43.0831299001578\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WSZ\",]$LONGITUDE <- -89.47595215839556\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L05QT5LB\",]$LATITUDE <- 43.114230724048014\r\nall_crashes[all_crashes$DOCTNMBR == \"01L05QT5LB\",]$LONGITUDE <- -89.35812769493853\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L06ZV8W2\",]$LATITUDE <- 43.07322351699731\r\nall_crashes[all_crashes$DOCTNMBR == \"01L06ZV8W2\",]$LONGITUDE <- -89.40072273726425\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0CS9M1F\",]$LATITUDE <- 43.04172765662615\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0CS9M1F\",]$LONGITUDE <- -89.39313149033856\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0TS2DB6\",]$LATITUDE <- 43.06769902424961\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0TS2DB6\",]$LONGITUDE <- -89.4008236046175\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L07LCTM4\",]$LATITUDE <- 43.04634165079267\r\nall_crashes[all_crashes$DOCTNMBR == \"01L07LCTM4\",]$LONGITUDE <- -89.48893584984673\r\n\r\n# not 100% sure about this one; description kinda confusing\r\nall_crashes[all_crashes$DOCTNMBR == \"01L06KHWV6\",]$LATITUDE <- 43.033329627105026\r\nall_crashes[all_crashes$DOCTNMBR == \"01L06KHWV6\",]$LONGITUDE <- -89.40788999291192\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0TS2D8H\",]$LATITUDE <- 43.06202954006571\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0TS2D8H\",]$LONGITUDE <- -89.40412105286146\r\n\r\n\r\n\r\nall_crashes_sf <- st_as_sf(all_crashes, coords = c(\"LONGITUDE\", \"LATITUDE\"))\r\n\r\n\r\nWe map again to make sure it looks good:\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"plot\")\r\ntm_shape(all_crashes_sf) +\r\n  tm_dots(\"bike_ped\", size = 1)\r\n\r\n\r\n\r\nSignalized intersections\r\nThe City’s open data portal has intersection controls for all intersections in the county.\r\nSignalized intersections are coded SL.\r\n\r\n\r\nShow code\r\n\r\nintersection_control <- st_read(\"data/Intersection_Control.geojson\")\r\n\r\nReading layer `Intersection_Control' from data source \r\n  `C:\\Users\\user1\\Documents\\website\\_posts\\2022-12-03-bicycle-and-pedestrian-intersection-crashes-in-madison\\data\\Intersection_Control.geojson' \r\n  using driver `GeoJSON'\r\nSimple feature collection with 8361 features and 5 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -89.83849 ymin: 42.84596 xmax: -89.00865 ymax: 43.29404\r\nGeodetic CRS:  WGS 84\r\n\r\nShow code\r\n\r\nsignals <- intersection_control |> \r\n  filter(IntControl == \"SL\")\r\n\r\n\r\nNow we create a 50-meter buffer1 around the signal locations and only keep crashes within that buffer:\r\n\r\n\r\nShow code\r\n\r\nst_crs(all_crashes_sf) <- 4326\r\n\r\n\r\ncrashes_at_signals <- all_crashes_sf[st_within(all_crashes_sf, st_buffer(signals, dist = 50)) %>% lengths > 0,]\r\n\r\n\r\nNow we can map all crashes at signalized intersections, separated into bike and bike crashes:\r\nBike crashes at signalized intersections\r\n\r\n\r\nShow code\r\n\r\npopup_vars <- c(\"DOCTNMBR\", \"bike_ped\", \"severity\", \"ACCDDATE\",\"ACCDHOUR\",\"ONRDWY\",\"ONRDWYDIR\",\"ATRDWY\",\"INTDIS\",\"INTDIR\",\"TOTFATL\",\"TOTINJ\",\"TOTVEH\",\"MNRCOLL\",\"ACCDTYPE\",\"AGGRFLAG\",\"BUSFLAG\",\"CMVFLAG\",\"CONSZONE\",\"DISTRCTD\",\"IMPAIRED\",\"INTRFLAG\",\"LANEDP1U\",\"LANEDP2U\",\"SPEEDFLAG\",\"TEENDRVR\",\"65+DRVR\")\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(crashes_at_signals |> filter(bike_ped == \"bike\")) +\r\n  tm_dots(\"severity\",\r\n          popup.vars = popup_vars) +\r\n  tm_layout(main.title = \"Bike intersection crashes\")\r\n\r\n\r\n\r\nPed crashes at signalized intersections\r\n\r\n\r\nShow code\r\n\r\ntm_shape(crashes_at_signals |> filter(bike_ped == \"ped\")) +\r\n  tm_dots(\"severity\",\r\n          popup.vars = popup_vars) +\r\n  tm_layout(main.title = \"Pedestrian intersection crashes\")\r\n\r\n\r\n\r\nSome summary statistics.\r\nOverall, there were 579 intersection crashes involving people walking and biking between January 2012 and now.\r\nOut of those, 337 (58%) happened at signalized intersections. These are the numbers of all intersection crashes, grouped by ped/bike and injury severity:\r\n\r\n\r\nShow code\r\n\r\nall_crashes |> \r\n  group_by(bike_ped, severity) |> \r\n  tally() |> \r\n  gt::gt()\r\n\r\n\r\nseverity\r\n      n\r\n    bike\r\n    Fatality\r\n2Serious injury\r\n32Minor injury\r\n176Possible injury\r\n58No injury\r\n33ped\r\n    Fatality\r\n5Serious injury\r\n55Minor injury\r\n137Possible injury\r\n62No injury\r\n19\r\n\r\nAnd here are the ones at signalized intersections only:\r\n\r\n\r\nShow code\r\n\r\ncrashes_at_signals |> \r\n  st_drop_geometry() |> \r\n  group_by(bike_ped, severity) |> \r\n  tally() |> \r\n  gt::gt()\r\n\r\n\r\nseverity\r\n      n\r\n    bike\r\n    Fatality\r\n1Serious injury\r\n19Minor injury\r\n103Possible injury\r\n37No injury\r\n20ped\r\n    Fatality\r\n4Serious injury\r\n31Minor injury\r\n75Possible injury\r\n37No injury\r\n10\r\n\r\n\r\n50 meters is based on some trial and error with different buffer sizes.\r\nToo small and you risk undercounting (e.g. crashes at large intersections); too big a buffer, and you may attribute crashes at a nearby unsignalized intersection to the signalized one.↩︎\r\n",
    "preview": "posts/2022-12-03-bicycle-and-pedestrian-intersection-crashes-in-madison/distill-preview.png",
    "last_modified": "2022-12-03T16:15:58-06:00",
    "input_file": "bicycle-and-pedestrian-intersection-crashes-in-madison.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-23-coffeeneuring-challenge-2022/",
    "title": "Coffeeneuring Challenge 2022",
    "description": "Making a map of this year's season",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-11-23",
    "categories": [
      "coffee",
      "map",
      "biking"
    ],
    "contents": "\r\nAlmost every year I participate in the Coffeeneuring Challenge. You can find out more what coffeeneuring is and check out my past adventures over on my bike blog. But basically you have to bike to seven different coffee locations over the course of about six weeks. Now that the 2022 season is over, it seemed like a good opportunity to use my coffeeneuring data for the #30DayMapChallenge.\r\n\r\n\r\nShow code\r\n\r\nlibrary(googlesheets4)\r\nlibrary(tidyverse)\r\nlibrary(tmap)\r\nlibrary(tmaptools)\r\nlibrary(sf)\r\nlibrary(extrafont)\r\n\r\n\r\nFirst we read in the spreadsheet where I kept track of my rides and destinations.\r\n\r\n\r\nShow code\r\n\r\ndestinations <- read_sheet(\"https://docs.google.com/spreadsheets/d/1QaZ0BPOxJs-wUg0LMardUEjCvDd8h6GUirUl4VOzBDU/edit?usp=sharing\") |> \r\n    separate(coordinates, into = c(\"lon\", \"lat\"), sep = \", \", convert = T) |> \r\n    st_as_sf(coords = c(\"lat\", \"lon\"))\r\n\r\ndestinations |> \r\n  kableExtra::kable()\r\n\r\n\r\nname\r\n\r\n\r\ndate\r\n\r\n\r\ncategory\r\n\r\n\r\nstrava\r\n\r\n\r\ngeometry\r\n\r\n\r\nCrema Café\r\n\r\n\r\n2022-10-15\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/7966997401\r\n\r\n\r\nPOINT (-89.32413 43.07862)\r\n\r\n\r\nParadiddle’s Café\r\n\r\n\r\n2022-10-09\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/7939047633\r\n\r\n\r\nPOINT (-88.99078 43.19471)\r\n\r\n\r\nAlice Good\r\n\r\n\r\n2022-10-21\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/7998212282\r\n\r\n\r\nPOINT (-89.53358 42.98808)\r\n\r\n\r\nHodge Podge\r\n\r\n\r\n2022-11-06\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/8079719303\r\n\r\n\r\nPOINT (-89.51824 42.99536)\r\n\r\n\r\nGrace\r\n\r\n\r\n2022-11-20\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/8146150714\r\n\r\n\r\nPOINT (-89.51179 43.09536)\r\n\r\n\r\nRock River, Byron\r\n\r\n\r\n2022-10-23\r\n\r\n\r\ncoffee outside\r\n\r\n\r\nhttps://www.strava.com/activities/8009683012\r\n\r\n\r\nPOINT (-89.25426 42.12329)\r\n\r\n\r\nLeopold Pump Track\r\n\r\n\r\n2022-10-30\r\n\r\n\r\ncoffee outside\r\n\r\n\r\nhttps://www.strava.com/activities/8044807447\r\n\r\n\r\nPOINT (-89.41957 43.03036)\r\n\r\n\r\nFirefly\r\n\r\n\r\n2022-10-16\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/7973704511\r\n\r\n\r\nPOINT (-89.38395 42.92652)\r\n\r\n\r\nWe can map the destinations:\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(destinations) +\r\n  tm_dots(\"category\")\r\n\r\n\r\n\r\nYou may wonder about the one dot very far south. This was on a very long two-day ride to Illinois I did in October!\r\nNext we add lines for the actual rides to these destinations. I don’t have access to the Strava API, and so I manually downloaded the gpx files and saved them in a folder. We use purrr to read them in and simplify the tracks with st_simplify to reduce the size.\r\n\r\n\r\nShow code\r\n\r\nprocess_gpx <- function(x){\r\n  df <- read_GPX(x)\r\n  df[[1]] |> #read_GPX outputs list\r\n    st_simplify(dTolerance = 100) #distance is in meters\r\n}\r\n\r\ntracks <- list.files(\"data/\", full.names = T) |> \r\n  map_dfr(process_gpx)\r\n\r\n\r\nLet’s add the tracks to the above map:\r\n\r\n\r\nShow code\r\n\r\n  tm_shape(destinations)+\r\n  tm_dots(\"category\") +\r\n  tm_shape(tracks) + \r\n  tm_lines()\r\n\r\n\r\n\r\nWe have all the data elements together now. Let’s create a nice static map. For that we need a base map. The Stamen Watercolor is always a nice option. Note the +c(...) after the bounding box command: This enlarges the bounding box in each direction so that our destinations aren’t at the very edge of the map and to make the map a little more square.\r\n\r\n\r\nShow code\r\n\r\nbasemap <- read_osm(bb(destinations)+c(-.3, -.1, .3, .1), \r\n                    zoom = 10, \r\n                    type = \"stamen-watercolor\")\r\n\r\n\r\nRather than using color to distinguish between the category of destination, we can use custom icons with the tmap_icons() function. I created two icons in Inkscape, based on Fontawesome icons.\r\n\r\nNow it’s just a matter of putting all the layers together and making adjustments:\r\n\r\n\r\nShow code\r\n\r\nmy_icons <- tmap_icons(c(\"img/coffee-inside.png\", \"img/coffee-outside.png\"))\r\n\r\ntmap_mode(\"plot\")\r\ntm_shape(basemap) +\r\n  tm_rgb() +\r\n  tm_shape(tracks) +\r\n  tm_lines(lwd = 1.5, \r\n           col = \"darkgreen\") +\r\n  tm_shape(destinations) +\r\n  tm_symbols(\r\n    shape = \"category\",\r\n    shapes = my_icons,\r\n    size = .1,\r\n    title.shape = \"Type of destination\",\r\n    border.col = NULL,\r\n    alpha = .8\r\n  ) +\r\n  tm_text(\r\n    \"name\",\r\n    size = .5,\r\n    remove.overlap = T,\r\n    col = \"white\",\r\n    just = \"left\",\r\n    bg.color = \"darkgrey\",\r\n    bg.alpha = .7\r\n  ) +\r\n  tm_compass() +\r\n  tm_layout(\r\n    legend.outside = T,\r\n    main.title = \"Harald’s 2022 Coffeeneuring Season\",\r\n    fontfamily = \"Roboto Condensed\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-23-coffeeneuring-challenge-2022/distill-preview.png",
    "last_modified": "2022-11-25T11:35:40-06:00",
    "input_file": "coffeeneuring-challenge-2022.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-19-the-color-of-roads/",
    "title": "The color of roads",
    "description": "What colors are most represented in the road names of Dane County?",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-11-19",
    "categories": [
      "Madison (WI)",
      "Dane County (WI)",
      "map"
    ],
    "contents": "\r\nSometimes you start a project and it ends up quite different from you had envisioned. I was going to do a quick and simple map for the #30DayMapChallenge theme “Blue.” Pull road data for Dane County, filter for names that contain “blue,” plot to a map with roads and road names. Sounds simple, right? Of course it is not.\r\nGetting all the data in is straightforward with tigris and tmaptools.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tigris)\r\noptions(tigris_use_cache = TRUE)\r\nlibrary(tmap)\r\nlibrary(tmaptools)\r\nlibrary(sf)\r\nlibrary(tidyverse)\r\nlibrary(extrafont)\r\nloadfonts(device = \"all\")\r\n\r\nwi_roads <- roads(\"WI\", \"Dane\")\r\n\r\n# county shapefile to calculate bounding box for basemap download\r\ndane <- tigris::counties(\"WI\", cb = T) %>% \r\n  filter(NAME == \"Dane\")\r\n\r\n#get basemap from OSM\r\ndane_osm <- read_osm(bb(dane), zoom = 10, type = \"stamen-toner\", )\r\n\r\nblue_dane <- wi_roads |> \r\n  mutate(blue = ifelse(str_detect(FULLNAME, \"[Bb]lue\"), \"blue\", \"not blue\")) |> \r\n  filter(blue == \"blue\") #keep only blue roads\r\n\r\nblue_dane\r\n\r\nSimple feature collection with 58 features and 5 fields\r\nGeometry type: LINESTRING\r\nDimension:     XY\r\nBounding box:  xmin: -89.83851 ymin: 42.9267 xmax: -89.02944 ymax: 43.23245\r\nGeodetic CRS:  NAD83\r\nFirst 10 features:\r\n        LINEARID            FULLNAME RTTYP MTFCC\r\n1  1103738774528 Blue Bill Park Dr S     M S1400\r\n2  1103738774530 Blue Bill Park Dr S     M S1400\r\n3   110689766034 N Blue Bill Park Dr     M S1400\r\n4  1106092748301    S Blue Mounds St     M S1400\r\n5   110689763919    E Blue Mounds Rd     M S1400\r\n6  1103676871652    E Blue Mounds Rd     M S1400\r\n7   110689763921    W Blue Mounds Rd     M S1400\r\n8   110689763922    W Blue Mounds Rd     M S1400\r\n9   110689762152       Blue Ridge Ct     M S1400\r\n10  110689765583   Blue Mountain Ave     M S1400\r\n                         geometry blue\r\n1  LINESTRING (-89.41119 43.14... blue\r\n2  LINESTRING (-89.41047 43.15... blue\r\n3  LINESTRING (-89.4134 43.158... blue\r\n4  LINESTRING (-89.74511 42.99... blue\r\n5  LINESTRING (-89.73772 42.95... blue\r\n6  LINESTRING (-89.74274 42.96... blue\r\n7  LINESTRING (-89.81902 42.96... blue\r\n8  LINESTRING (-89.81824 42.97... blue\r\n9  LINESTRING (-89.49303 43.07... blue\r\n10 LINESTRING (-89.81777 43.01... blue\r\n\r\nBut a map shows that more work is needed:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(dane_osm) +\r\n  tm_rgb() +\r\n  tm_shape(blue_dane) +\r\n  tm_lines(\"blue\", lwd = 2) +\r\n  tm_text(\"FULLNAME\", col = \"blue\")\r\n\r\n\r\n\r\nNot good: The labels are too crowded, and you can see that a single way with the same name often is split into multiple segments, e.g. Blue Mounds Trail on the western edge of the map. I spent hours trying to fix these things through one of the various geometric or other tools from the sf package. But in the end, nothing worked. (If you have suggestions on how to combine adjoining ways, I’m all ears!)\r\nInstead, I pivoted to making a map of all the road name colors in Dane County. The colors are extracted with simple regular expressions. This works well for all colors except “red” – RiveREDege Ct, LaREDo Ct, or EldRED St are just some of the false positives. For one county these can be cleaned manually, but for a larger dataset this would be a problem.\r\n\r\n\r\nShow code\r\n\r\n# deal with false-positive reds\r\nnot_red <- c(\"Riveredge Rd\", \r\n             \"Laredo Ct\",\r\n             \"Eldred St\", \r\n             \"Fredericksburg Ct\", \r\n             \"Frederick Ct\", \r\n             \"Arboredge Way\", \r\n             \"Frederick Cir\",\r\n             \"Redan Dr\",\r\n             \"Meredith Way\",\r\n             \"Claredon Dr\",\r\n             \"Frederick St\",\r\n             \"Fredericksburg Ln\",\r\n             \"Meredithe Ave\",\r\n             \"Arboredge Way\",\r\n             \"Fredenberg Rd\",\r\n             \"Mildred Ct\",\r\n             \"Hubred Ln\",\r\n             \"Covered Bridge Trl\",\r\n             \"Saddle Bred Ln\",\r\n             \"Saddlebred Ln\")\r\n\r\nblue_dane <- wi_roads |> \r\n  mutate(blue = case_when(str_detect(FULLNAME, \"[Bb]lue\") ~ \"blue\",\r\n                          str_detect(FULLNAME, \"[Bb]lack\") ~ \"black\",\r\n                          str_detect(FULLNAME, \"[Rr]ed\") ~ \"red\",\r\n                          str_detect(FULLNAME, \"[Yy]ellow\") ~ \"yellow\",\r\n                          str_detect(FULLNAME, \"[Bb]rown\") ~ \"brown\",\r\n                          str_detect(FULLNAME, \"[Gg]r[ae]y\") ~ \"grey\",\r\n                          str_detect(FULLNAME, \"[Gg]reen\") ~ \"green\",\r\n                          T ~ NA_character_)) |> \r\n  filter(!is.na(blue) & !blue %in% not_red)\r\n\r\n\r\nWhich color is most common?\r\n\r\n\r\nShow code\r\n\r\nblue_dane |> \r\n  st_drop_geometry() |>\r\n  group_by(blue) |> \r\n  tally(sort = T) |> \r\n  knitr::kable()\r\n\r\nblue\r\nn\r\ngreen\r\n88\r\nred\r\n69\r\nblue\r\n58\r\nblack\r\n40\r\ngrey\r\n13\r\nbrown\r\n8\r\nyellow\r\n8\r\n\r\nGreen! And here are the two maps, one dynamic and one static:\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(blue_dane) +\r\n  tm_lines(\"blue\",lwd = 3, alpha = .8,  id = \"FULLNAME\") +\r\n  tm_layout(title = \"The Color of Dane County Road Names\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"plot\")\r\ntm_shape(dane_osm) +\r\n  tm_rgb(alpha = .7) +\r\n  tm_shape(blue_dane) +\r\n  tm_lines(\"blue\",lwd = 3, alpha = .8) +\r\n  tm_layout(title = \"The Color of Dane County Road Names\", fontfamily = \"Roboto Condensed\", title.bg.color = \"lightgrey\", title.bg.alpha = .8)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-19-the-color-of-roads/distill-preview.png",
    "last_modified": "2022-11-19T19:51:08-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-16-food-establishment-data-from-madison/",
    "title": "#30DayMapChallenge: Restaurant heat map of Madison",
    "description": "Challenge 15: Food/drink",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-11-16",
    "categories": [
      "Madison (WI)",
      "map"
    ],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\nlibrary(tmap)\r\nlibrary(mapboxapi)\r\nlibrary(leaflet)\r\nlibrary(leaflet.extras)\r\n\r\n\r\nAnother quick #30DayMapChallenge post. The prompt for day 15: Food/drink. Public Health Madison & Dane County have a dataset on health inspections which includes all licensed restaurants in the county. The data are not geocoded, and so I use the mapboxapi package to geocode the locations:\r\n\r\n\r\nShow code\r\n\r\nestablishments <- read_csv(\"data/Licensed_Establishment.csv\")\r\n\r\n# geocode and save geocoded data\r\n# est_sf <- establishments %>% \r\n#   rowwise() %>% \r\n#   mutate(geometry = mb_geocode(AddressFull, output = \"sf\"),\r\n#          geometry = geometry$geometry)\r\n# \r\n# est_sf <- st_sf(est_sf)\r\n# \r\n# write_rds(est_sf, file = \"data/licensed_establishment_geocoded.RDS\")\r\nest_sf <- readRDS(\"data/licensed_establishment_geocoded.RDS\")\r\n\r\n\r\nThe dataset includes “all operational licensed establishments receiving routine inspections,” that is, not just restaurants. A quick tmap (we have to filter out a food cart that’s located in Milwaukee):\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"plot\")\r\nest_sf %>% \r\n  filter(AddrCity != \"MILWAUKEE\") %>% \r\n  tm_shape() +\r\n  tm_dots(\"EstablishmentType\") +\r\n  tm_layout(legend.outside = T)\r\n\r\n\r\n\r\nNow we filter to only establishments that are of the type “Primarily Restaurant” and create a heat map. Heat maps look cool, but I admittedly don’t fully understand how leaflet.extras generates them under the hood – certainly the map looks very different depending on the zoom level.\r\n\r\n\r\nShow code\r\n\r\nleaflet(est_sf %>% filter(EstablishmentType == \"Primarily Restaurant\")) %>% \r\n  addTiles() %>% \r\n  addHeatmap(radius = 6)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-16-food-establishment-data-from-madison/distill-preview.png",
    "last_modified": "2022-11-19T19:43:55-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-13-30daymapchallenge-5-minute-map/",
    "title": "#30DayMapChallenge: 5-minute map",
    "description": "5-minute isochrones around Metro Transit bus stops",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-11-13",
    "categories": [
      "Madison (WI)",
      "transit",
      "map"
    ],
    "contents": "\r\nUsually I don’t have the time to participate in the #30DayMapChallenge, but today I felt like putting together a map for challenge 13: 5-minute map. My interpretation of the prompt: How far can you walk within a 5 minutes from each of Madison Metro’s ~2100 bus stops?\r\n\r\n\r\nShow code\r\n\r\nlibrary(tmap)\r\nlibrary(sf)\r\nlibrary(mapboxapi)\r\nlibrary(tmaptools)\r\n\r\n# load bus stops from Madison Open Data portal\r\n# https://data-cityofmadison.opendata.arcgis.com/datasets/cityofmadison::metro-transit-bus-stops/explore\r\nstops <- st_read(\"www/data/Metro_Transit_Bus_Stops.geojson\")\r\n\r\nReading layer `Metro_Transit_Bus_Stops' from data source \r\n  `C:\\Users\\user1\\Documents\\website\\_posts\\2022-11-13-30daymapchallenge-5-minute-map\\www\\data\\Metro_Transit_Bus_Stops.geojson' \r\n  using driver `GeoJSON'\r\nSimple feature collection with 2103 features and 25 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -89.56386 ymin: 42.98772 xmax: -89.24482 ymax: 43.17654\r\nGeodetic CRS:  WGS 84\r\n\r\nShow code\r\n\r\n# create isochrones. API key required\r\n# walk_5min <- mb_isochrone(stops,\r\n#                           profile = \"walking\",\r\n#                           time = 5)\r\nwalk_5min <- readRDS(\"www/data/Metro_Transit_Bus_Stops.RDS\")\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# create bounding box around stop isochrones\r\nmetro_bb <- bb(walk_5min)\r\n\r\n#download basemap\r\nbasemap <- read_osm(metro_bb, zoom = 12, type = \"stamen-watercolor\")\r\n\r\ntmap_mode(\"plot\")\r\ntmap_options(check.and.fix = TRUE)\r\ntm_shape(basemap)+\r\n  tm_rgb(alpha = .8) +\r\ntm_shape(walk_5min) +\r\n  tm_polygons(alpha = .3) +\r\n  tm_shape(stops)+\r\n  tm_dots(alpha = .5) +\r\n  tm_layout(title = \"5-minute walksheds around\\nMadison Metro stops\", \r\n            title.position = c(\"LEFT\", \"TOP\")) +\r\n  tm_credits(\"Basemap: OpenStreetMap Contributors, Stamen, Data: City of Madison, Visualization: Harald Kliems\",\r\n             position = c(\"right\", \"BOTTOM\"),\r\n             size = 200, align = \"right\", )\r\n\r\n\r\nShow code\r\n\r\n            # title.bg.color = \"lightgrey\",\r\n            # title.bg.alpha = .8, title.position = c(\"LEFT\", \"TOP\"))\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-13-30daymapchallenge-5-minute-map/30daymapchallenge-5-minute-map_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-11-14T12:02:07-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/latest-commute-mode-share-madison-2021/",
    "title": "What's the latest on commute mode share in Madison?",
    "description": "Working from home has seen an unprecedented boom, while biking stagnates.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-10-12",
    "categories": [
      "Madison (WI)",
      "transportation",
      "American Community Survey"
    ],
    "contents": "\r\nOne of the high holidays for census data nerds is ACSmas: The annual release of 1-year estimates from the American Community Survey. Last year’s ACSmas was cancelled because of the pandemic, but this September we finally got our fix of data for 2021.\r\nACSmas is cancelled this year, everything sucks https://t.co/rGsjpg0WE7— dadbode miller (@bikepedantic) July 29, 2021 I was especially excited about the commuting data: What did the pandemic do to people’s trips to work? How much of a shift to working from home would we see? And was the data quality actually going to be good enough to draw any firm conclusions? What follows is an in-depth look at the ACS commuting data for the City of Madison.\r\nMode share over time\r\nACS mode share data is available all the way back to 2010. In previous posts on the Madison Bikes blog I had already looked at these time series data – and frankly, it was not that exciting: By and large the percentages of different modes of getting to worked stayed the same. Biking didn’t grow but neither shrunk. It seemed highly likely that the pandemic would have changed things up. And it did:\r\n\r\n\r\nShow code\r\n\r\nlibrary(tmap)\r\nlibrary(tidycensus)\r\nlibrary(tidyverse)\r\nlibrary(gt)\r\n\r\n# variables <- load_variables(dataset = \"acs1/subject\", year = \"2021\")\r\n\r\nget_msn_mode_share <- function(year) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"S0801\", geography = \"place\", state = 55, cache_table=T)\r\n  acs_data |> \r\n    filter(NAME == \"Madison city, Wisconsin\") |> \r\n    mutate(year = year)\r\n}\r\n\r\n\r\n# variable_readable = case_when(\r\n#   variable == \"S0801_C01_002\" ~ \"Drive\",\r\n#   variable == \"S0801_C01_009\" ~ \"Transit\",\r\n#   variable == \"S0801_C01_010\" ~ \"Walk\",\r\n#   variable == \"S0801_C01_011\" ~ \"Bike\",\r\n#   variable == \"S0801_C01_013\" ~ \"Work from home\",\r\n#   variable == \"S0801_C02_002\" ~ \"Drive, male\",\r\n#   variable == \"S0801_C02_009\" ~ \"Transit, male\",\r\n#   variable == \"S0801_C02_010\" ~ \"Walk, male\",\r\n#   variable == \"S0801_C02_011\" ~ \"Bike, male\",\r\n#   variable == \"S0801_C02_013\" ~ \"Work from home, male\",\r\n#   variable == \"S0801_C03_002\" ~ \"Drive, female\",\r\n#   variable == \"S0801_C03_009\" ~ \"Transit, female\",\r\n#   variable == \"S0801_C03_010\" ~ \"Walk, female\",\r\n#   variable == \"S0801_C03_011\" ~ \"Bike, female\",\r\n#   variable == \"S0801_C03_013\" ~ \"Work from home, female\",\r\n#   \r\n#   \r\n# )\r\n\r\n\r\n\r\nmsn_mode_share <- map_dfr(c(2010:2019, 2021), get_msn_mode_share)\r\n\r\nmsn_mode_share <- msn_mode_share |> \r\n  mutate(gender = case_when(str_detect(variable, \"^S0801_C01\") ~ \"total\",\r\n                            str_detect(variable, \"^S0801_C02\") ~ \"male\",\r\n                            str_detect(variable, \"^S0801_C03\") ~ \"female\"),\r\n         mode_readable = case_when(\r\n           str_detect(variable, \"S0801_C0[1-3]_002\") ~ \"Drive\",\r\n           str_detect(variable, \"S0801_C0[1-3]_009\") ~ \"Transit\",\r\n           str_detect(variable, \"S0801_C0[1-3]_010\") ~ \"Walk\",\r\n           str_detect(variable, \"S0801_C0[1-3]_011\") ~ \"Bike\",\r\n           str_detect(variable, \"S0801_C0[1-3]_013\") ~ \"Work from home\"))\r\n        \r\n# data frame for the ggrepel labels on the right of the plot\r\nmsn_mode_share_2021 <-  msn_mode_share |> \r\n  filter(year == 2021 & !is.na(mode_readable))\r\n\r\nmsn_mode_share |> \r\n  filter(!is.na(mode_readable) & gender == \"total\") |> \r\n  group_by(mode_readable, year) |> \r\n  ggplot(aes(year, estimate, color = mode_readable)) +\r\n  geom_line(size = 1.2) +\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate + moe), alpha = .8,\r\n                  size = 1,fatten = 1) +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_x_continuous(breaks = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2021), minor_breaks = NULL, limits = c(2010, 2022.5)) +\r\n  ylab(\"estimate (%)\") +\r\n  labs(title = \"Working from home almost quintupled\\nbetween 2019 and 2021\",\r\n       subtitle =\"City of Madison commute mode share, 2010-2021\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: @HaraldKliems\") +\r\n    ggrepel::geom_text_repel(data = msn_mode_share_2021 |> filter(gender == \"total\"), aes(label = paste0(mode_readable, \" \", estimate, \"%\")), nudge_x = 1) +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nFrom 2019 to 2021, the share of people working from home almost quintupled! Fewer people drove to work, and commuting by bus became much less common. For walking and biking the changes were downward too but within the margins of error.\r\n\r\n\r\nShow code\r\n\r\nmsn_mode_share |> \r\n  filter(!is.na(mode_readable) & gender == \"total\" & year >= 2019) |> \r\n  group_by(mode_readable, year) |> \r\n  pivot_wider(names_from = year, values_from = c(estimate, moe)) |> \r\n  ggplot() +\r\n    geom_segment(aes(x=mode_readable, xend=mode_readable, y=estimate_2019, yend=estimate_2021), color=\"grey\", arrow = arrow(length = unit(2, \"mm\"))) +\r\n  geom_point(aes(x = mode_readable, y = estimate_2019), color = 2019) +\r\n    geom_point(aes(x = mode_readable, y = estimate_2021), color = 2021) +\r\n  geom_text(aes(x = mode_readable, \r\n                y = (estimate_2021 + estimate_2019)/2, \r\n                label = paste0(estimate_2021-estimate_2019, \"%\")),\r\n            nudge_x = .25)+\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  hrbrthemes::theme_ipsum() +\r\n  coord_flip() +\r\n  theme(panel.grid.major.y = element_blank()) +\r\n  ylab(\"estimate (%)\") +\r\n  xlab(element_blank()) +\r\n  labs(title = \"Change in commute mode share\",\r\n       subtitle =\"City of Madison, 2019-2021. Margins of error not shown\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: @HaraldKliems\") \r\n\r\n\r\n\r\nFor transportation planning, it’s important to not only look at percentages but also at absolute numbers: Fewer people driving to work means less peak hour congestion (which is not necessarily a good thing); fewer transit riders means lower fare revenues; and fewer bike commuters may lead to less support of biking infrastructure. And a large number of people working from home may be a problem for commercial real estate or fewer customers for downtown restaurants.\r\n\r\n\r\nShow code\r\n\r\nget_commute_total <- function(year) {\r\n  get_acs(geography = \"place\",\r\n                           state = \"WI\",\r\n                           survey = \"acs1\",\r\n                           year = year,\r\n                           table = \"C08006\",\r\n          summary_var = \"C08006_001\",\r\n          cache_table = T) |> \r\n    mutate(year = year,\r\n           )\r\n}\r\n\r\n\r\ncommute_numbers <- map_dfr(c(2019, 2021), get_commute_total)\r\n\r\ncommute_numbers_msn <- commute_numbers |> \r\n  filter(NAME == \"Madison city, Wisconsin\") |> \r\n  mutate(mode_readable = case_when(\r\n           str_detect(variable, \"C08006_001\") ~ \"Total\",\r\n           str_detect(variable, \"C08006_002\") ~ \"Drive\",\r\n           str_detect(variable, \"C08006_008\") ~ \"Transit\",\r\n           str_detect(variable, \"C08006_009\") ~ \"Bike\",\r\n           str_detect(variable, \"C08006_010\") ~ \"Walk\",\r\n           str_detect(variable, \"C08006_012\") ~ \"Work from home\")) |> \r\n  filter(!is.na(mode_readable))\r\n\r\ncommute_numbers_msn |> \r\n  pivot_wider(id_cols = mode_readable, names_from = year, values_from = c(estimate, moe)) |> \r\n  mutate(mode_readable = fct_reorder(mode_readable, estimate_2021)) |> \r\n  group_by(mode_readable) |> \r\n  mutate(moe_diff = moe_sum(moe = c(moe_2021, moe_2019), estimate = c(estimate_2021, estimate_2019))) |> \r\n  ggplot() +\r\n    geom_segment(aes(x=mode_readable, xend=mode_readable, y=estimate_2019, yend=estimate_2021), color=\"grey\", arrow = arrow(length = unit(2, \"mm\"))) +\r\n  geom_point(aes(x = mode_readable, y = estimate_2019), color = 2019) +\r\n    geom_point(aes(x = mode_readable, y = estimate_2021), color = 2021) +\r\n  geom_text(aes(x = mode_readable, \r\n                y = (estimate_2021 + estimate_2019)/2, \r\n                label = paste0(scales::number_format(style_positive = \"plus\",big.mark = \",\")(estimate_2021-estimate_2019), \" (±\", round(moe_diff, 0), \")\")),\r\n            nudge_x = .25)+\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  hrbrthemes::theme_ipsum() +\r\n  coord_flip() +\r\n  theme(panel.grid.major.y = element_blank()) +\r\n  ylab(\"number of workers\") +\r\n  xlab(element_blank()) +\r\n  labs(title = \"Change in number of commuters, 2019-2021\",\r\n       subtitle =\"City of Madison. Statistically insignifant changes greyed out\",\r\n       caption = \"American Community Survey 1-year estimates, Table C08006\\nVisualization: @HaraldKliems\") +\r\n  gghighlight::gghighlight(abs(estimate_2021-estimate_2019) > moe_diff, use_direct_label = F)\r\n\r\n\r\n\r\nThe total number of workers and of bike and walk commuters didn’t change significantly. But there are about 21000 fewer drivers, 11,000 fewer bus commuters, and over 30,000 more people working from home. For a city of Madison’s size, these changes are big!\r\nWhat about those still going to the office?\r\nWhen I first shared the above graphs some people pointed out: “Working from home isn’t really commuting! What do these percentages look like when we only account for the people who still leave the house to get to work?” Or to put it differently: Before the pandemic, you and 99 of your coworkers went to the office every day, and 10 of you biked. In 2021, half of your coworkers now work from home. Of the remaining 50 people in the office, do you still have 5 (i.e. 10%) people who bike to work?\r\n\r\n\r\nShow code\r\n\r\nget_msn_mode_counts <- function(year) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"C08006\", geography = \"place\", state = 55, cache_table=T, summary_var = \"C08006_001\")\r\n  acs_data |> \r\n    filter(NAME == \"Madison city, Wisconsin\") |> \r\n    mutate(year = year)\r\n}\r\n\r\nmsn_counts <- map_dfr(c(2019, 2021), get_msn_mode_counts)\r\n\r\nmsn_counts <- msn_counts |> \r\n  mutate(summary_est = case_when(year == 2019 ~summary_est - 9050,\r\n                                 year == 2021 ~ summary_est - 40279),\r\n         mode_readable = case_when(\r\n    variable == \"C08006_001\" ~ \"Total\",\r\n      variable == \"C08006_002\" ~ \"Drive\",\r\n      variable == \"C08006_003\" ~ \"Drove alone\",\r\n      variable == \"C08006_004\" ~ \"Carpooled\",\r\n      variable == \"C08006_008\" ~ \"Transit\",\r\n      variable == \"C08006_009\" ~ \"Bike\",\r\n      variable == \"C08006_010\" ~ \"Walk\",\r\n      variable == \"C08006_011\" ~ \"Other\",\r\n      variable == \"C08006_012\" ~ \"Work from home\"\r\n  ),\r\n  mode_share = estimate/summary_est) |> \r\n  filter(!is.na(mode_readable)) |> \r\n  filter(mode_readable %in% c(\"Drive\", \"Transit\", \"Bike\", \"Walk\", \"Other\")) |> \r\n  mutate(mode_readable = fct_relevel(mode_readable, \r\n                                     \"Other\",\r\n                                                \"Bike\", \r\n                                                \"Walk\", \r\n                                                \"Transit\", \r\n                                                \"Drive\")) |> \r\n  group_by(year,mode_readable) %>% \r\n    mutate(pos = cumsum(mode_share) - mode_share/2)\r\nmsn_counts |> \r\n  ggplot(aes(as.factor(year), mode_share, fill = mode_readable)) +\r\n  geom_col(position = \"stack\") +\r\n  geom_text(aes(label = mode_readable), position = position_stack(vjust = 0.5), color = \"white\",\r\n            size = 3) +\r\n  scale_y_continuous(labels = scales::label_percent()) +\r\n    hrbrthemes::scale_fill_ipsum(name = \"element_blank()\") +\r\n  hrbrthemes::theme_ipsum() +\r\n  ylab(\"Commute mode share estimate\") +\r\n  xlab(element_blank()) +\r\n  theme(legend.position = \"none\",   panel.grid.major.x = element_blank()) +\r\n  labs(title = \"Transit commutes were the big loser\",\r\n       subtitle =\"City of Madison, working from home excluded\",\r\n       caption = \"American Community Survey 1-year estimates, Table B08006\\nVisualization: @HaraldKliems\")\r\n\r\n\r\n\r\nThis is the same data in table form.\r\n\r\n\r\nShow code\r\n\r\nmsn_counts |> \r\n  \r\n  pivot_wider(id_cols = mode_readable, names_from = year, values_from = mode_share, names_prefix = \"mode_share_\") |> \r\n  mutate(change = mode_share_2021- mode_share_2019) |> \r\n  select(mode_readable, mode_share_2019, mode_share_2021, change) |>\r\n  ungroup() |> \r\n  gt() |> \r\n  tab_header(title = \"Change in commute mode for workers not working from home\") |> \r\n  fmt_percent(columns = c(mode_share_2021, mode_share_2019, change), decimals = 0) |> \r\n  tab_spanner(\r\n    label = \"Mode share\",\r\n    columns = c(mode_share_2019, mode_share_2021)\r\n  ) |> \r\n  cols_label(mode_readable = \"Commute mode\", \r\n             mode_share_2021 = \"2021\",\r\n             mode_share_2019 = \"2019\",\r\n             change = \"Change (percentage points)\") |> \r\n   data_color(\r\n    columns = c(change),\r\n    colors = scales::col_numeric(\r\n      palette = \"viridis\",\r\n    domain = NULL )) |> \r\n   tab_source_note(\r\n    source_note = \"Data: American Community Survey 1-year estimates, Table B08006. Margins of error not shown.\"\r\n  )\r\n\r\n\r\nChange in commute mode for workers not working from home\r\n    Commute mode\r\n      \r\n        Mode share\r\n      \r\n      Change (percentage points)\r\n    2019\r\n      2021\r\n    Drive\r\n74%\r\n79%\r\n6%Transit\r\n10%\r\n4%\r\n−6%Bike\r\n4%\r\n4%\r\n−0%Walk\r\n11%\r\n11%\r\n1%Other\r\n1%\r\n2%\r\n0%Data: American Community Survey 1-year estimates, Table B08006. Margins of error not shown.\r\n    \r\n\r\nCalculating the margins of error for these estimates is complex and I have not done that. But similar to the error bars we have seen above, changes of less than 2% points are probably not meaningful. So what do we take away from the table and chart? Of the people who still commute to the office, taking the bus is much less common while driving has gone up. The other percentages are basically unchanged. This makes sense: Bus service was reduced and many people perceived riding the bus as a risk for infection. Conversely, peak hour congestion for drivers went down (remember: overall the number of people commuting by car went down even when their percentage here went up) and some employers reduced or waived parking fees.\r\nMode share by gender\r\nDid commuting trends in Madison differ by gender?\r\nEspecially for biking, research has shown that in the US women are underrepresented in bike commuting, take fewer trips on bike share bikes, and generally have different travel patterns compared to men.(Hosford and Winters 2019; Ravensbergen, Fournier, and El-Geneidy 2022) In the 2021 data for Madison, we do not see these differences.\r\nLooking only at the estimates, it may appear that for bike commuting there is indeed a gender difference: Bike% of men rode their bike to work whereas only Bike% of women did so. But when we look at the error bars around the estimates, we see that for all modes they overlap between the two genders and therefore the differences we see in the data may well be by chance.\r\n\r\n\r\nShow code\r\n\r\n# faceted plot by gender\r\nmsn_mode_share |> \r\n  filter(!is.na(mode_readable)) |> \r\n  group_by(mode_readable, year, gender) |> \r\n  ggplot(aes(year, estimate, color = mode_readable)) +\r\n  geom_line(size = 1.5) +\r\n  hrbrthemes::scale_color_ipsum(name = \"element_blank()\") +\r\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate + moe), size = 1.3, fatten = 1.5, alpha = .7) +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_x_continuous(breaks = c(2010, 2012, 2014, 2016,  2019, 2021), minor_breaks = NULL, limits = c(2010, 2023)) +\r\n  ylab(\"estimate (%)\") +\r\n  labs(title = \"How people get to work in Madison does not differ by gender\",\r\n       subtitle =\"City of Madison commute mode share, 2010-2021\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: @HaraldKliems\") +\r\n  geom_text(data = msn_mode_share_2021, aes(label = paste0(mode_readable, \" \", estimate, \"%\")), nudge_x = 1) +\r\n  theme(legend.position = \"none\") +\r\n  facet_wrap(~ fct_relevel(gender, \"total\", \"female\", \"male\"))\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nmsn_mode_share_2021 |> \r\n  filter(gender != \"total\") |> \r\n  ggplot(aes(fct_reorder(mode_readable, estimate), estimate, color = gender)) +\r\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate + moe), \r\n                  size = 1.3, \r\n                  fatten = 1.1,\r\n                  #alpha = .7,\r\n                  position = position_dodge(width = 0.3)) +\r\n  coord_flip() +\r\n  hrbrthemes::scale_color_ipsum(name = element_blank(),\r\n                                breaks = c(\"male\", \"female\"),\r\n                                guide = guide_legend(override.aes = list(shape = NA, size = 5))) +\r\n  hrbrthemes::theme_ipsum() +\r\n  theme(panel.grid.major.y = element_blank()) +\r\n  ylab(\"estimate (%)\") +\r\n  xlab(element_blank()) +\r\n  labs(title = \"How people get to work in Madison\\ndoes not differ by gender\",\r\n       subtitle = \"2021 estimates and 90% margin of error\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: @HaraldKliems\") +\r\n   annotate(\r\n    geom = \"curve\", x = \"Transit\", y = 15, xend = \"Bike\", yend = 3, \r\n    curvature = -.3, arrow = arrow(length = unit(2, \"mm\"))\r\n  ) +\r\n  annotate(geom = \"text\", y = 15, x = \"Transit\", label = \"Error bars still overlap\", hjust = \"left\")\r\n\r\n\r\n\r\nMode share by race and ethnicity\r\nMuch has been written about the racial disparities of the COVID epidemic. Infection rates, the outcomes of an infection, and who was and wasn’t able to work from home often followed the patterns of structural racism. The American Community Survey data does provide a breakdown of commute modes by race. However, the more you break down the data into smaller categories, the larger the margins of error get. We saw above that for the total population of Madison or for the mode share by gender, the margins of error were already sizable. If we now break down the data into race and ethnicity categories, this becomes even more problematic. And because Madison is predominantly White, the error margins are largest for non-White population groups. So when you look at the following graph, note the wide error bars and not just the point estimates. Whenever the error bars overlap, the difference between the two groups is likely due to chance.\r\n\r\n\r\nShow code\r\n\r\nget_commute_by_race <- function(table_id) {\r\n  get_acs(geography = \"place\",\r\n                           state = \"WI\",\r\n                           survey = \"acs1\",\r\n                           year = 2021,\r\n                           table = table_id,\r\n          summary_var = paste0(table_id, \"_001\"),\r\n          cache_table = T) |> \r\n    mutate(table = table_id,\r\n           )\r\n}\r\n\r\n\r\ntables <- c(\"B08105A\", \r\n            \"B08105B\", \r\n            \"B08105C\", \r\n            \"B08105D\", \r\n            \"B08105E\",\r\n            \"B08105F\",\r\n            \"B08105G\",\r\n            \"B08105H\",\r\n            \"B08105I\")\r\n\r\ncommute_by_race <- map_dfr(tables, get_commute_by_race)\r\n\r\ncommute_by_race_msn <- commute_by_race |> \r\n  filter(NAME == \"Madison city, Wisconsin\") |> \r\n  mutate(race_ethnicity = case_when(str_detect(table, \"A$\") ~ \"White Alone\",\r\n                                    str_detect(table, \"B$\") ~ \"Black/African American\",\r\n                                    str_detect(table, \"C$\") ~ \"American Indian and Alaska Native Alone\",\r\n                                    str_detect(table, \"D$\") ~ \"Asian\",\r\n                                    str_detect(table, \"E$\") ~ \"Native Hawaiian Alone\",\r\n                                    str_detect(table, \"F$\") ~ \"Other Race Alone\",\r\n                                    str_detect(table, \"G$\") ~ \"Two or more races\",\r\n                                    str_detect(table, \"H$\") ~ \"Non-Hispanic White\",\r\n                                    str_detect(table, \"I$\") ~ \"Hispanic/Latino\"),\r\n         mode_readable = case_when(\r\n           str_detect(variable, \"B08105[:alpha:]_001\") ~ \"Total\",\r\n           str_detect(variable, \"B08105[:alpha:]_002\") ~ \"Drove alone\",\r\n           str_detect(variable, \"B08105[:alpha:]_003\") ~ \"Carpooled\",\r\n           str_detect(variable, \"B08105[:alpha:]_004\") ~ \"Transit\",\r\n           str_detect(variable, \"B08105[:alpha:]_005\") ~ \"Walked\",\r\n           str_detect(variable, \"B08105[:alpha:]_006\") ~ \"Cab, motorcycle, bike, other\",\r\n           str_detect(variable, \"B08105[:alpha:]_007\") ~ \"Work from home\")) |> \r\n  filter(!is.na(estimate))\r\n\r\n\r\np <- commute_by_race_msn |> \r\n  mutate(proportion = estimate / summary_est,\r\n         prop_moe = moe_prop(estimate, summary_est, moe, summary_moe),\r\n         ratio_moe = moe_ratio(estimate, summary_est, moe, summary_moe)) |> \r\n  filter(mode_readable != \"Total\" & race_ethnicity != \"Two or more races\" & race_ethnicity != \"White Alone\") |> \r\n  ggplot(aes(fct_reorder(mode_readable, proportion), proportion, color = race_ethnicity)) +\r\n  geom_pointrange(aes(ymin = proportion - prop_moe, ymax = proportion + prop_moe),\r\n                  position = position_dodge(width = 0.3)) +\r\n  coord_flip() +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  hrbrthemes::scale_color_ipsum(name = element_blank()) +\r\n  hrbrthemes::theme_ipsum() +\r\n  theme(panel.grid.major.y = element_blank(),\r\n        legend.position = \"bottom\") +\r\n  ylab(\"mode share estimate and margin of error\") +\r\n  xlab(element_blank())+\r\nlabs(title = \"Madison commute mode share by race/ethnicity\",\r\n       subtitle = \"2021 estimates and 90% margin of error\",\r\n       caption = \"American Community Survey 1-year estimates, Tables B08105A-I\\nVisualization: @HaraldKliems\")\r\n\r\np+\r\n  #drive alone\r\n  annotate(\r\n    geom = \"curve\", x = 5.4, y = .70, xend = 6.1, yend = .561, \r\n    curvature = -.2, arrow = arrow(length = unit(2, \"mm\")), \r\n  ) +\r\n    annotate(\r\n    geom = \"curve\", x = 5.4, y = .70, xend = 5.95, yend = .575, \r\n    curvature = -.7, arrow = arrow(length = unit(2, \"mm\")), \r\n  ) +\r\n  annotate(geom = \"text\", y = .705, x = 5.4, label = \"Error bars don't overlap\", hjust = \"left\") +\r\n  #work from home\r\n    annotate(\r\n    geom = \"curve\", x = 5.7, y = .27, xend = 5, yend = .23, \r\n    curvature = -.2, arrow = arrow(length = unit(2, \"mm\")) \r\n  ) +\r\n  annotate(geom = \"text\", y = .26, x = 5.8, label = \"White and Asian commuters are\\nmore likely to work from home\", hjust = \"right\") +\r\n    #carpool\r\n    annotate(\r\n    geom = \"curve\", x = 3.8, y = .23, xend = 3.1, yend = .125, \r\n    curvature = .3, arrow = arrow(length = unit(2, \"mm\")) \r\n  ) +\r\n  annotate(geom = \"text\", y = .25, x = 3.8, label = \"LatinX workers carpool the most\\n(but note that wide margin of error)\", hjust = \"left\")\r\n\r\n\r\n\r\nLet’s go through mode by mode:\r\nFor driving alone, all the error bars overlap, except for non-Hispanic White compared to Black. Black commuters appear to drive to work at a higher proportion than non-Hispanic White ones.\r\nFor working from home we see a split between non-Hispanic White and Asian on the higher end and Black and Hispanic/Latino on the lower end. Much has been written how some jobs that were deemed essential, such as care or service work as well as many manufacturing jobs, are ones that can’t be done remotely and that are often done by Black or Hispanic workers.\r\nThe share of people walking to work doesn’t differ significantly between any of the groups.\r\nCarpooling appears to be especially common among Hispanic/LatinX workers. But note the wide error bar again: The only pairing where the bars do not overlap is between Hispanic/LatinX and non-Hispanic White.\r\nThe next category lumps together cabs, motorcycles, bicycles, and other means of transportation to work. Like with walking, there are no significant differences between the groups.\r\nThe same is true for transit: All the error bars overlap, i.e. no significant differences.\r\nThere’s more than ACS: Bike count analysis webinar\r\nWebinarThe American Community Survey only captures commuting, and trips to work only make up about 20% of all our trips. There are other limitations to the data, e.g. for work trips that involve more than one mode or for people on hybrid work arrangements. Therefore we can only conclude so much from it and should try to include other sources of data about biking in Madison.\r\nOne such source are the numerous bike counters that we have on our paths and roads. If you live in Madison, you are probably familiar with the Eco-Counter displays on the Southwest Path at Camp Randall and on the Cap City Trail at North Shore Drive.\r\nBut there are numerous other counters throughout the city, and we’re in the fortunate position to have help with analyzing the data from these counters: Madison Bikes together with Bike Fitchburg were awarded a data analysis grant from the League of American Bicyclists and Eco-Counter!\r\nWith the help from the city, we shared loads of counter data with them and the data analysis specialists at Eco Counter will analyze it.\r\nI haven’t see the results yet, but on October 26 you can join a free webinar where we will present results from the analysis and show how they are important for bike advocacy. Sign up here.\r\n\r\n\r\n\r\nHosford, Kate, and Meghan Winters. 2019. “Quantifying the Bicycle Share Gender Gap.” Findings, November. https://doi.org/10.32866/10802.\r\n\r\n\r\nRavensbergen, Léa, Juliette Fournier, and Ahmed El-Geneidy. 2022. “Exploratory Analysis of Mobility of Care in Montreal, Canada.” Transportation Research Record, July, 03611981221105070. https://doi.org/10.1177/03611981221105070.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/latest-commute-mode-share-madison-2021/analysis_2021_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-10-13T09:45:52-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 949
  },
  {
    "path": "posts/2022-08-26-using-r-markdown-and-msgxtractr-to-process-an-open-records-request/",
    "title": "Wrangling unwieldy open records data",
    "description": "Using R, {msgxtractor}, and R Markdown to wrangle a set of msg files",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-08-26",
    "categories": [
      "open records",
      "purrr",
      "RMarkdown"
    ],
    "contents": "\r\nA friend recently asked for assistance with data she received from an open records request.\r\n\r\n\r\nemotionally I was not prepared for WISDOT to send me a Box folder filled with documents that I can’t figure out how to download in response to my obnoxious open records request from ~6 months ago\r\n\r\n— marybeth (unfortunately) (@marbeff) August 16, 2022\r\n\r\nThe data she received consisted of a zip file with 70-something *.msg files. This is a proprietary Microsoft file format used by Outlook for storing emails. I quickly checked if there was an R package for handling the data, and indeed there is: msgextractor. With this package and some R and RMarkdown I figured converting the files to something more convenient would not be too difficult.\r\nIn the end it took me much longer than I had hoped, but it works. The process has two pieces: An R script with a function to read in a single email, extract some information, and save the email’s attachments in a separate folder. The second piece: A RMarkdown .Rmd template that uses variables generated in the function to create an HTML file with email metadata, the body, and links to the attachments. The R then script then iterates the function over the list of *.msg files with the walk function from the {purr} package. This creates one HTML file for each of the emails.\r\nHere’s the R script:\r\n\r\n\r\nlibrary(msgxtractr) #install with devtools::install_github(\"hrbrmstr/msgxtractr\") \r\nlibrary(tidyverse)\r\nlibrary(here)\r\n\r\n# read list of msg files in folders\r\nfiles <- list.files(\"data/All Files/\")\r\n\r\n# extract and save attachments and return paths\r\nwrite_msg_files <- function(msg_file){\r\n  msg <- read_msg(paste0(\"data/All Files/\", msg_file))\r\n  dir_name <- paste0(\"output/\", msg_file)\r\n  dir.create(dir_name)\r\n  save_attachments(msg_obj = msg, path = dir_name)\r\n  #return path and file names for attachments\r\n  attachment_names <- list.files(dir_name)\r\n  attachment_paths <- paste0(msg_file, \"/\", attachment_names)\r\n  # contents for output documents\r\n  email <- data_frame(email = msg_file,\r\n                   from = unlist(msg$headers$From),\r\n                   to = unlist(msg$headers$To),\r\n                   cc = unlist(msg$headers$CC),\r\n                   subject = unlist(msg$headers$Subject),\r\n                   date = unlist(msg$headers$Date),\r\n                   body = paste(msg$body$text, collapse = \"\\n\"),\r\n                   attachments = list(attachment_paths))\r\n  \r\n  #render output document\r\n  rmarkdown::render(\r\n    input = \"output_template.Rmd\",\r\n    output_file = here(\"output\", paste0(msg_file, \".html\"))\r\n  )\r\n}\r\n\r\n\r\n# iterate over list of files\r\nwalk(files, write_attachments)\r\n\r\n\r\n\r\nAnd this is what the output template looks like:\r\n\r\n    ---\r\n    output: html_document\r\n    ---\r\n\r\n    ```{r setup, include=FALSE}\r\n    knitr::opts_chunk$set(echo = F)\r\n```\r\n\r\n    ```{r}\r\n    library(msgxtractr)\r\n    library(tidyverse)\r\n    ```\r\n    # `r email[1,]$email`\r\n\r\n    **From:** `r email[1,]$from`\r\n\r\n    **To:** `r email$to`\r\n\r\n    **CC:** `r email$cc`\r\n\r\n    **Date:** `r email$date`\r\n\r\n    **Subject:** `r email$subject`\r\n\r\n    ```{r results='asis'}\r\n    cat(email[1,]$body)\r\n    ```\r\n\r\n    ## Attachments\r\n    ```{r results='asis'}\r\n    #creates markdown links to attachments\r\n    map_chr(email$attachments[[1]], ~paste0(\"[\", ., \"](\", ., \")\"))\r\n    ```\r\n\r\nThis works nicely, with one exception: There were two msg files whose content was another msg file as an attachment (which in turn had attachments). msgxtractor currently can’t deal with these and so I had to remove them from the dataset.\r\nHere’s what the output:\r\nScreenshot of html outputMaybe this can be useful for others. The repository is on Github\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-08-26T18:28:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/how-much-housing-is-being-built-in-madison/",
    "title": "How much housing is being built in Madison?",
    "description": "A look at housing permit data over time.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-08-06",
    "categories": [
      "housing",
      "Madison (WI)"
    ],
    "contents": "\r\n\r\nContents\r\nPopulation growth\r\nHousing permits over time\r\nTypes of housing\r\n\r\nMuch of the US is in a housing crisis, and Madison is no exception. Our population is growing, and the growth of housing stock has not kept pace, leading to higher rents and real estate prices. There have been iniatives to increase the production of housing, but how successful have they been? Inspired by a thread on Twitter, this post looks at building permit data.\r\n\r\n\r\nShow code\r\n\r\nlibrary(dplyr)\r\nlibrary(readr)\r\nlibrary(tidyr)\r\nlibrary(lubridate)\r\nlibrary(ggplot2)\r\nlibrary(tidycensus)\r\nlibrary(forcats)\r\n\r\n\r\n\r\nPopulation growth\r\nMadison has been and continues to be one of the fasted growing cities in the state of Wisconsin. Between 1970 and 2020, the city added almost 100,000 new residents. That is an increase of about 56 percent.1\r\n\r\n\r\nShow code\r\n\r\n# Population time series Madison city\r\npop <- read_csv(\"data/nhgis0002_ts_nominal_place.csv\")\r\n\r\n\r\npop %>% \r\n  filter(NHGISCODE == \"G55048000\" & YEAR %in% c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\", \"2020\")) %>% \r\n  # mutate(difference = AV0AA - lag(AV0AA)) %>% \r\n  # relocate(difference)\r\n  ggplot(aes(as.numeric(YEAR), AV0AA)) +\r\n  geom_line(size = 1.5) +\r\n  xlab(\"Year\") +\r\n  ylab(\"Population\")+\r\n  scale_y_continuous(labels = scales::comma, limits = c(0, 275000)) +\r\n  theme(panel.grid.minor.x = element_blank()) +\r\n  labs(caption = \"Data: US Census, IPUMS NHGIS, University of Minnesota, www.nhgis.org\\nVisualization: Harald Kliems\",\r\n       title = \"Madison's population grew from\\n173,000 in 1970 to 270,000 in 2020 (+56%)\") +\r\n  hrbrthemes::theme_ipsum_rc()\r\n\r\n\r\n\r\n\r\nThese new residents all need housing, and in general household sizes have been shrinking, requiring even more housing units. What does housing production look like in the city and the surrounding metro area?\r\nHousing permits over time\r\nThe Office of Policy Development and Research within the Department of Housing and Urban Development (HUD) keeps track of building permits issues on a monthly basis. Monthly data is choppy: A single large development can easily throw off the total in a month. Thus we look at the data in half year periods:\r\n\r\n\r\nShow code\r\n\r\n# housing permits Madison city\r\npermits <- read_csv(\"data/BuildingPermits(1).csv\")\r\n\r\n#housing permits for Madison CBSA\r\nCBSA <- read_csv(\"data/BuildingPermits - Madison CBSA.csv\")\r\n\r\n# combine and prep city and CBSA data\r\npermits_all <- permits %>% \r\n  rbind(CBSA) %>% \r\n    filter(Location == \"Madison, WI\" | Location == \"MADISON\") %>% \r\n    mutate(date = ym(paste(Year, Month)),\r\n         quarter = quarter(date),\r\n         semester = semester(date),\r\n         place = case_when(Location == \"MADISON\" ~ \"City of Madison\",\r\n                              Location == \"Madison, WI\" ~ \"Madison metro area\")) %>% \r\n  filter(date < ymd(\"2022-07-01\"))  #remove 0 values for future months\r\n\r\n  \r\n\r\n\r\npermits_all %>% \r\n  filter(`Series Code` == 1) %>% #total units\r\n  group_by(Year, semester, place) %>% \r\n  summarise(permits = sum(Permits), date) %>% \r\n  distinct(Year, semester, permits, place, .keep_all = T) %>% \r\n  pivot_wider(names_from = place, values_from = permits) %>% \r\n  mutate(metro_without_city = `Madison metro area` - `City of Madison`) %>% \r\n  pivot_longer(cols = c(metro_without_city, `City of Madison`, `Madison metro area`), names_to = \"place\", values_to = \"permits\") %>% \r\n  filter(place %in% c(\"metro_without_city\", \"City of Madison\")) %>% \r\n  ggplot(aes(date, permits, color = fct_reorder(place, permits))) +\r\n  geom_line(size = 1.5) +\r\n  ylim(0,2000)+\r\n  hrbrthemes::scale_color_ipsum(name = NULL, labels = c(\"Madison metro area,\\nwithout City\", \"City of Madison only\")) +\r\n  hrbrthemes::theme_ipsum_rc() +\r\n  theme(legend.position = \"right\") +\r\n  labs(title = \"New housing units permitted in the City of\\n Madison and the surrounding metro area\", subtitle = \"Both the total number and the trend over\\ntime are similar between the two\",\r\n       caption = \"Data: https://socds.huduser.gov/permits/\\nVisualization: Harald Kliems\\nMadison metro area encompasses Dane, Green, Iowa, and Columbia counties\")\r\n\r\n\r\n\r\n\r\nHousing production in the City of Madison was strong during the early aughts. And in the surrounding metro area it was even stronger. But once the financial crisis hit, it took almost a decade until housing construction returned to previous levels. We can also see the shock the early stage of the COVID-19 pandemic, and how that permitting backlog cleared over the course of 2020.\r\nHow do the 946 units permitted in Madison in the first half of 2022 compare to other cities? Not too badly:\r\n\r\nSome other housing start totals at the half-year mark:Seattle: 5,542 unitsDenver: 4,707 unitsBoston: 2,610 unitsWashington: 2,471 unitsPhiladelphia: 1,265 unitsPortland: 873 unitsSan Jose: 854 unitsOakland: 755 unitsSan Francisco: 660 units (lol, lmao) https://t.co/137zelFEsN— Alex Schieferdecker (@alexschief) August 2, 2022 We permitted more units than Portland (pop 652k), Oakland (441k), San Jose (1,013k), and San Francisco (874k)!\r\nTypes of housing\r\nThe previous section describes the growth in total units of housing. But were these newly permitted units single-family homes or parts of multi-family housing?\r\n\r\n\r\nShow code\r\n\r\npermits_all %>% \r\n  filter(`Series Code` <= 2) %>% \r\n  pivot_wider(names_from = Series, values_from = Permits, id_cols = c(Location, Year, Month)) %>% \r\n  janitor::clean_names() %>% \r\n  mutate(location = case_when(location == \"Madison, WI\" ~ \"madison_metro\",\r\n                              location == \"MADISON\" ~ \"madison_city\")) %>% \r\n  group_by(year, location) %>% \r\n  summarise(total_units = sum(total_units), sf_units = sum(units_in_single_family_structures)) %>% \r\n  pivot_wider(names_from = location, values_from = c(total_units, sf_units)) %>% \r\n  mutate(metro_without_city_sf = sf_units_madison_metro - sf_units_madison_city,\r\n         metro_without_city_total = total_units_madison_metro - total_units_madison_city,\r\n         prop_sf_madison_city = sf_units_madison_city/total_units_madison_city,\r\n         prop_sf_metro_without_city = metro_without_city_sf/metro_without_city_total) %>% \r\n  select(year, starts_with(\"prop\")) %>% \r\n  pivot_longer(cols = starts_with(\"prop\"), \r\n               names_to = \"place\", \r\n               values_to = \"prop_sf\",\r\n               names_prefix = \"prop_sf_\") %>% \r\nggplot(aes(year, prop_sf, color = fct_reorder(place, -prop_sf))) +\r\n  geom_line(size = 1.5) +\r\n  labs(title = \"Proportion of newly permitted single family homes\\nto total units permitted\",\r\n       subtitle = \"The share of SF homes to total units permitted has declined over time. In the City of Madison the\\nshare is much lower than in the surrounding metro area.\",\r\n       caption = \"Data: https://socds.huduser.gov/permits/\\nVisualization: Harald Kliems\",\r\n       x = \"year\",\r\n       y = \"Proportion single-family\") +\r\n  hrbrthemes::scale_color_ipsum(name = NULL, \r\n                       labels = c(\"Madison metro area,\\nwithout City\", \"City of Madison only\")) +\r\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1), \r\n                     limits = c(0,1)) +\r\n  hrbrthemes::theme_ipsum_rc()\r\n\r\n\r\n\r\n\r\nIt is probably no surprise that in the metro area, single-family homes make a higher share of newly permitted units than in Madison. But both in the metro area and the city, the proportion of newly built single-family homes has sharply declined from its heights during and shortly after the Great Recession. In the city, only about 20% of units built are single-family homes, and in the metro area the share has dipped below 50% in recent years.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSome part of the growth is the result of expanding city limits; however, in general the newly annexed areas did not have large populations.]↩︎\r\n",
    "preview": "posts/how-much-housing-is-being-built-in-madison/new_housing_madison_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-06T07:00:03-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/how-far-can-you-go/",
    "title": "How far can you go?",
    "description": "Comparing how far you can travel by bus, bike, and e-bike in Madison, before and after the bus network redesign",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-04-30",
    "categories": [
      "transit",
      "map",
      "Madison (WI)"
    ],
    "contents": "\r\n\r\nContents\r\nExtracting the maps\r\nIsochrones with openrouteservice\r\nThe app\r\n\r\nMadison Metro is redesigning their bus network. As someone who is passionate about transit and as a member of the Transportation Commission, I have been following the project closely. When I saw a number of isochrone maps showing how far you can get by transit within 45 minutes, I was curious: How do those isochrones compare to ones by bike from the same location? I decided to build a Shiny app. This post provides some technical detail. To go straight to the map, click here\r\n\r\n\r\n\r\nFigure 1: Screenshot of the app\r\n\r\n\r\n\r\nExtracting the maps\r\nMetro published the maps as appendices in pdf format, with one map per page. With a large number of maps, extracting them manually and renaming each one seemed like a lot of work. Instead I let R handle the process.\r\nExtracting the pages from the pdf as images is easy with the magick package. But how do you name the images in a way that reflects their location? So, for example, “Raymond at McKenna.png”? Well, all the maps have the name of the location in the same place on the page. And so can again use magick to crop the page to just that area and then use optical character recognition to extract the text.\r\nThe cropping required some of trial and error to get the area right. For example, I initially cropped the bottom of the area too close, leading to characters like “p” or “q” being mis-recognized as “o”. But once the area was correct, the OCR worked almost flawlessly.\r\nHere is the full function:\r\n\r\n\r\nread_page <- function(page, pdf_name){\r\n  p <- image_read_pdf(pdf_name, pages = page, density = 600)\r\n   location <- p %>% image_crop(geometry = \"2400x200+60+385\") %>% \r\n   image_ocr()\r\n   path_name = paste0(str_sub(location, end = -2), \".png\")\r\n   image_write(p, path = path_name)\r\n}\r\n\r\n\r\n\r\nAll that remains is then to iterate the function over the pages of the pdf files with map:\r\n\r\n\r\nmap(1:pdf_info(\"data/AdditionalIsochrones.pdf\")$pages, ~ read_page(page = ., pdf_name = \"data/AdditionalIsochrones.pdf\"))\r\n\r\n\r\n\r\nIsochrones with openrouteservice\r\nBefore we can generate isochrones, we need a shapefile of the start locations. I maybe could have used the names extracted in the previous step and run them through a geocoder to automate the process. Instead I put the locations on a map manually in QGIS and exported the locations as a geojson.\r\nNext, we need a function that takes three inputs (location [sf_point object], type of vehicle, time range) and return a location (sf_polygon).\r\n\r\n\r\ncreate_iso <- function(location, profile_1, range_1){\r\n  iso <- ors_isochrones(locations = st_coordinates(location), \r\n                                 profile = ors_profile(mode = profile_1),\r\n                                 range = range_1,\r\n                       output = \"sf\")\r\n  iso$geometry\r\n}\r\n\r\n\r\n\r\nThe OpenRouteService API has a limit of 5 locations per call, and so we create the isochrones one after the other with a mutate call. For this we need to set up the data frame of locations so that it has one row for each location, each vehicle type, and each time range.1\r\n\r\n\r\norigins <- st_read(\"data/network_redesign_travel_time_map_locations.geojson\")\r\n\r\n\r\nReading layer `network_redesign_travel_time_map_locations' from data source `C:\\Users\\user1\\Documents\\network_redesign_isochrones\\data\\network_redesign_travel_time_map_locations.geojson' \r\n  using driver `GeoJSON'\r\nSimple feature collection with 41 features and 2 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -89.52713 ymin: 43.01574 xmax: -89.26672 ymax: 43.14486\r\nGeodetic CRS:  WGS 84\r\n\r\norigins_2 <- origins %>% \r\n  mutate(mode_ebike = \"e-bike\", \r\n         mode_bike = \"bike\", \r\n         time_15 = 900, \r\n         time_30 = 1800, \r\n         time_45 = 2700) %>% \r\n  pivot_longer(starts_with(\"mode\"),\r\n               names_to = \"mode_var\",\r\n               values_to = \"bike_type\") %>% pivot_longer(starts_with(\"time_\"),\r\n                                                         names_to = \"time_var\", \r\n                                                         values_to = \"time\") %>% \r\n  select(-c(mode_var, time_var, id))\r\n\r\nhead(origins_2) %>% \r\n  kableExtra::kable()\r\n\r\n\r\n\r\nname\r\n\r\n\r\ngeometry\r\n\r\n\r\nbike_type\r\n\r\n\r\ntime\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\ne-bike\r\n\r\n\r\n900\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\ne-bike\r\n\r\n\r\n1800\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\ne-bike\r\n\r\n\r\n2700\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\nbike\r\n\r\n\r\n900\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\nbike\r\n\r\n\r\n1800\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\nbike\r\n\r\n\r\n2700\r\n\r\n\r\nNow we just run a mutate call with the rowwise function of dplyr and create isochrones for each row 2 :\r\n\r\n\r\nbike_isochrones <- origins_2 %>% \r\n  rowwise() %>% \r\n  mutate(iso = create_iso(geometry, bike_type, time))\r\n\r\n\r\n\r\nWe save the dataframe as an RDS file that we can then use in the app.\r\nThe app\r\nNow we have all the pieces in place for the Shiny app. To get a layout with the maps side-by-side I used the fluidpage way of designing the UI, which was pretty easy.\r\nThe exception was the scaling of the bus map image. In the UI, the map is added with imageOutput, which takes a width and height parameter; on the server side, the image is generate with a renderImage call, which also takes dimension parameters. In theory, the Shiny UI should resize things dynamically based on the screen size, but in practice this was difficult to get right. Through trial-and-error I figured out a scaling that works on most screen sizes, but in some cases the image will still overlap other elements of the app.\r\nOther than that, the code is pretty straightforward. You can see the latest version here.\r\n\r\nAnother way to do this would have been to use pmap, iterating over the three parameters each.↩︎\r\nShoutout to the UW-Madison Data Science Hub team and Martin Gal on StackOverflow to help me figure out the rowwise step.↩︎\r\n",
    "preview": "posts/how-far-can-you-go/img/app_screenshot.jpg",
    "last_modified": "2022-04-30T11:04:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/metro-madison-ridership-has-not-recovered-from-covid/",
    "title": "Metro Madison ridership has not recovered from COVID",
    "description": "A quick time series chart of Metro ridership going back to 2002",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-03-27",
    "categories": [
      "transit",
      "Madison (WI)",
      "time series",
      "chart"
    ],
    "contents": "\r\nA very quick chart: There are large differences between how transit agencies (in the US and in Europe) have managed to recover their pre-COVID ridership. Madison Metro’s general manager has always been very open about the fact that recovery would take several years: Metro is facing staffing shortages, which prevents them from expanding service. Funding right now is not a problem, but once pandemic recovery funds run out it will be. Finally, UW-Madison as well local and state government agency have been large drivers of ridership – it remains to be seen if a loss of peak commutes can be substituted with other transit trips.\r\nBut first we have to ask: What do the ridership numbers look like? The National Transit Database provides monthly ridership time series data all the way back to 2002. Here is a time series plot of that data:\r\n\r\n\r\nShow code\r\n\r\nlibrary(readxl)\r\nbus <- read_excel(\"data/January 2022 Ajusted Database.xlsx\", sheet = 3)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nbus_long <- bus %>% \r\n  pivot_longer(cols = 10:250, names_to = \"month\", values_to = \"value\") %>% \r\n  mutate(date = my(month)) %>% \r\n  filter(Agency == \"City of Madison\" & Modes == \"MB\")\r\n\r\nbus_long %>%\r\n  ggplot(aes(date, value/1000)) +\r\n  geom_line() +\r\n  labs(title = \"Monthly unlinked passenger trips for Madison Metro\",\r\n       subtitle = \"In January 2022, ridership still was only 23% of that in January 2020\",\r\n       caption = \"Data: National Transit Database Monthly Module Adjusted Data Release\\nVisualization: Harald Kliems\") +\r\n  ylab(\"Trips (1000s)\") +\r\n   scale_x_date(\r\n    NULL,\r\n    breaks = scales::breaks_width(\"2 years\"), \r\n    labels = scales::label_date(\"'%y\")\r\n  ) +\r\n  scale_y_continuous(\r\n     labels = scales::label_comma(),\r\n     limits = c(0, 1600)) +\r\n  geom_line(data = bus_long %>% filter(date == \"2020-01-01\" | date == \"2022-01-01\"), color = \"red\", size = 1.5, linetype = 2,\r\n            alpha = 0.4) +\r\n  geom_point(data = bus_long %>% filter(date == \"2020-01-01\" | date == \"2022-01-01\"), color = \"red\", size = 2.5) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nWe can see: While ridership is well above its worst pandemic lows, there still is a long way to go until we get back to pre-pandemic numbers. In January 2022, the latest month available, ridership was less than a quarter than what it was in January 2020.\r\n\r\n\r\n\r\n",
    "preview": "posts/metro-madison-ridership-has-not-recovered-from-covid/quick-chart-metro-ridership_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-03-27T13:18:53-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/boardings-per-bus-per-stop-a-useful-metric/",
    "title": "Boardings per bus per stop: A useful metric?",
    "description": "A follow-up on a post on Metro bus boardings per stop in Madison (WI)",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-03-06",
    "categories": [
      "transit",
      "Madison (WI)",
      "map"
    ],
    "contents": "\r\n\r\nContents\r\nRevision note 2022-03-07\r\nParsing pre-pandemic frequencies\r\nCombining frequency and boardings\r\nHow to interpret these numbers?\r\n\r\nRevision note 2022-03-07\r\nAfter the initial version of this post went up, Jonathan Mertzig pointed out that the number of departures per stop appeared to be wrong for at least one stop, Northern Lights & Epic Staff C (SB). In the post, the stop was listed with 30 departures, which is exactly twice as many as it actually has. I have identified what caused the issue and updated the post accordingly. If you notice any other errors, please let me know.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidytransit)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nlibrary(kableExtra)\r\n\r\n\r\n\r\nSomeone opened a Github issue on my blog post about creating a map of bus boardings:\r\n\r\nI know its kind of old but I was (re)visiting it in looking at the newest version of the redesign plan. It is difficult to compare ridership across stops when I know that some stops have more buses serving them than others. It would be cool to have ridership per bus or a similar measure. But I guess what we’d want is pre-pandemic frequencies. Is there even a source for old schedules were we to want to do it by hand?\r\n\r\nI responded:\r\n\r\nTo get the number of buses at each stop, you’d need 1) a pre-pandemic GTFS schedule file and 2) code to parse that data in the way you want. The former appears to be available via OpenMobilityData. For the latter, I’m not super proficient with GTFS data, but number of buses at a given stop I think is not that hard. \\[...\\] I’ll see if I find some time to play around with the old GTFS file.\r\n\r\nFortunately, there is an R package for working with GTFS data, tidytransit (Poletti et al. 2022), and the package includes helpful vignettes. So I tried answering the question.\r\nParsing pre-pandemic frequencies\r\nThe ridership data provided in my previous post uses a two-week sample from February 2020, right before the COVID-19 pandemic led to massive service reductions and ridership decline. To match the boarding with bus frequencies at that time, we download a GTFS feed from January 30, 2020 from the OpenMobilityData portal.\r\n\r\n\r\nShow code\r\n\r\ngtfs <- read_gtfs(\"data/gtfs_metro_madison_2020-01-30.zip\")\r\n\r\n\r\n\r\nWe start with a quick plot of the different service patterns in the first two months on 2020:\r\n\r\n\r\nShow code\r\n\r\nlibrary(lubridate)\r\njan_feb_2020 <- interval(\"2020-01-01\", \"2020-02-28\")\r\n\r\ngtfs$.$dates_services %>% \r\n  filter(date %within% jan_feb_2020) %>% \r\n  ggplot() + \r\n  theme_bw() + \r\n  geom_point(aes(x = date, y = service_id, color = wday(date, label = T)), size = 2)\r\n\r\n\r\n\r\n\r\nWe see that there are some services that run on some or all weekdays, some Saturday and Sunday and holiday services. We can also see that some services only run when the UW is in session/on break. Compared to the service pattern examples from the tidytransit vignettes, this is quite straightforward.\r\nThere is one exception to this straightforwardness: I realized that the final stop on a route gets counted as an additional departure. We can fix this by filtering out any stop time with a drop_off_type of 0 (i.e. passenger can only get off the bus).\r\n\r\n\r\nShow code\r\n\r\ngtfs$stop_times <- gtfs$stop_times %>% \r\n  filter(drop_off_type == 0)\r\n\r\n\r\n\r\nRather than worrying about different service patterns, we therefore simply pick a typical weekday in February: February 17, a Wednesday, and get the number of departures frequency for each stop:\r\n\r\n\r\nShow code\r\n\r\nstop_freq <- gtfs %>% \r\n  filter_feed_by_date(\"2020-02-17\") %>% \r\n  get_stop_frequency(start_time = 0*3600, end_time = 24*3600, \r\n                              by_route = F)\r\n\r\n\r\n\r\nstop_freq %>%\r\n  head(10) %>% \r\n  kbl() %>% \r\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\r\n\r\n\r\n\r\nstop_id\r\n\r\n\r\nservice_id\r\n\r\n\r\nn_departures\r\n\r\n\r\nmean_headway\r\n\r\n\r\n100\r\n\r\n\r\n88_WKD\r\n\r\n\r\n394\r\n\r\n\r\n219\r\n\r\n\r\n104\r\n\r\n\r\n88_WKD\r\n\r\n\r\n44\r\n\r\n\r\n1964\r\n\r\n\r\n107\r\n\r\n\r\n88_WKD\r\n\r\n\r\n65\r\n\r\n\r\n1329\r\n\r\n\r\n110\r\n\r\n\r\n88_WKD\r\n\r\n\r\n7\r\n\r\n\r\n12343\r\n\r\n\r\n1100\r\n\r\n\r\n88_WKD\r\n\r\n\r\n354\r\n\r\n\r\n244\r\n\r\n\r\n1101\r\n\r\n\r\n88_WKD\r\n\r\n\r\n434\r\n\r\n\r\n199\r\n\r\n\r\n1105\r\n\r\n\r\n88_WKD\r\n\r\n\r\n171\r\n\r\n\r\n505\r\n\r\n\r\n1107\r\n\r\n\r\n88_WKD\r\n\r\n\r\n81\r\n\r\n\r\n1067\r\n\r\n\r\n1112\r\n\r\n\r\n88_WKD\r\n\r\n\r\n134\r\n\r\n\r\n645\r\n\r\n\r\n1115\r\n\r\n\r\n88_WKD\r\n\r\n\r\n31\r\n\r\n\r\n2787\r\n\r\n\r\nNow we can join the departure data with the locations of the bus stops and draw a map:\r\n\r\n\r\nShow code\r\n\r\nlibrary(tmap)\r\ntmap_mode(\"view\")\r\n\r\ngtfs_shp <- gtfs %>% \r\n  gtfs_as_sf() \r\n\r\n\r\nstop_freq <- gtfs_shp$stops %>% \r\n  left_join(stop_freq, by = \"stop_id\") %>% \r\n  filter(n_departures >1) \r\n\r\nstop_freq %>% \r\n  tm_shape() +\r\n  tm_dots(size = \"n_departures\", col = \"n_departures\", alpha = .7, popup.vars = c( \"# daily departures\" = \"n_departures\"), id = \"stop_name\")\r\n\r\n\r\n\r\n\r\nWe can see the four transfer points; a cluster of stations with frequent service along the University Ave/Johnson St couplet, State St, and on the Capitol Square; and one busy quasi-transfer point at East Towne Mall.\r\nCombining frequency and boardings\r\nNow all that remains is to join our frequency data with the boardings, and then divide the boarding count by the number of departures.\r\n\r\n\r\nShow code\r\n\r\nlibrary(sf)\r\n\r\nboardings <- st_read(\"data/Metro_Transit_Ridership_by_Stop.shp\")\r\n\r\n\r\nReading layer `Metro_Transit_Ridership_by_Stop' from data source \r\n  `C:\\Users\\user1\\Documents\\bus_stop_frequencies\\data\\Metro_Transit_Ridership_by_Stop.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 2142 features and 15 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -89.56378 ymin: 42.98765 xmax: -89.2449 ymax: 43.17652\r\nGeodetic CRS:  WGS 84\r\n\r\nJust as a reminder, this is what the raw boarding numbers look like on a map:\r\n\r\n\r\nShow code\r\n\r\nboardings %>% \r\n  tm_shape() +\r\n  tm_dots(col = \"Weekday\", size = \"Weekday\", alpha = .7)\r\n\r\n\r\n\r\n\r\nNext, we join and create a map.\r\n\r\n\r\nShow code\r\n\r\nboardings <- boardings %>% \r\n  mutate(stop_id = as.character(StopID)) %>% \r\n  st_drop_geometry()\r\n\r\nstop_freq %>% \r\n  left_join(boardings, by = \"stop_id\") %>% \r\n  mutate(boardings_per_bus = Weekday/n_departures) %>% \r\n  tm_shape() +\r\n  tm_dots(col = \"boardings_per_bus\", \r\n          size = \"boardings_per_bus\", \r\n          alpha = .7, \r\n          popup.vars = c(\"Boardings per departure\" = \"boardings_per_bus\", \"Weekday departures\" = \"n_departures\", \"Weekday boardings\" = \"Weekday\"), \r\n          id = \"stop_name\",\r\n          title = \"Boardings per departure\")\r\n\r\n\r\n\r\n\r\nThis map looks quite different: The transfer points lose some prominence, whereas the stops on the UW Campus really stand out. And out in Verona there is one very prominent stop. Let’s look at the top 15 stops in a table.\r\n\r\n\r\nShow code\r\n\r\nstop_freq %>% \r\n  left_join(boardings, by = \"stop_id\") %>% \r\n  mutate(boardings_per_bus = Weekday/n_departures) %>%\r\n  st_drop_geometry() %>% \r\n  select(stop_name, boardings_per_bus, n_departures) %>% \r\n  arrange(desc(boardings_per_bus)) %>% \r\n  head(15) %>% \r\n  kableExtra::kbl(digits = 1, col.names = c(\"Stop name\", \"Boardings per bus\", \"Daily departures\")) %>% \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\r\n\r\n\r\n\r\nStop name\r\n\r\n\r\nBoardings per bus\r\n\r\n\r\nDaily departures\r\n\r\n\r\nObservatory & Elm (EB)\r\n\r\n\r\n20.9\r\n\r\n\r\n46\r\n\r\n\r\nObservatory & Natatorium (EB)\r\n\r\n\r\n17.8\r\n\r\n\r\n46\r\n\r\n\r\nNorthern Lights & Epic Staff C (SB)\r\n\r\n\r\n16.2\r\n\r\n\r\n15\r\n\r\n\r\nLinden & N Charter (WB)\r\n\r\n\r\n12.2\r\n\r\n\r\n46\r\n\r\n\r\nLinden & N Charter (EB)\r\n\r\n\r\n12.1\r\n\r\n\r\n55\r\n\r\n\r\nObservatory & Babcock (EB)\r\n\r\n\r\n9.4\r\n\r\n\r\n46\r\n\r\n\r\nObservatory & Highland (EB)\r\n\r\n\r\n8.0\r\n\r\n\r\n46\r\n\r\n\r\nHighland & Marsh (EB)\r\n\r\n\r\n7.9\r\n\r\n\r\n114\r\n\r\n\r\nN Lake & University (NB)\r\n\r\n\r\n7.1\r\n\r\n\r\n83\r\n\r\n\r\nLinden & Henry (EB)\r\n\r\n\r\n6.6\r\n\r\n\r\n55\r\n\r\n\r\nWright & Madison College (SB)\r\n\r\n\r\n6.2\r\n\r\n\r\n33\r\n\r\n\r\nSouth Transfer Point\r\n\r\n\r\n6.2\r\n\r\n\r\n203\r\n\r\n\r\nNorth Transfer Point\r\n\r\n\r\n5.1\r\n\r\n\r\n242\r\n\r\n\r\nS Broom & W Doty (NB)\r\n\r\n\r\n5.0\r\n\r\n\r\n60\r\n\r\n\r\nCremer & Park And Ride (NB)\r\n\r\n\r\n4.8\r\n\r\n\r\n8\r\n\r\n\r\nThe Observatory & Elm EB stop on average has on average 21 passengers boarding each bus. But the stop serving the Epic campus in Verona, with only 15 daily departures is really high up there as well. There are only a few other non-UW stops in the list: One serves the Madison College campus on the north side, and one is a Park & Ride lot; and two of the four transfer points made the cut.\r\nHow to interpret these numbers?\r\nWhat does it tell us that a high number of people board a bus at a given stop? Does it help make decisions for projects such as the ongoing Network Redesign? I’m curious what the person who initially asked the question thinks about this.\r\nI can see two possible insights from the metric: A large number of people boarding at a stop leads to longer dwell times. (Fricker 2011) It takes time for people to step on board, pay or validate their fare, and move through the vehicle, and so the bus will be stopped for a longer time. For schedule planning purposes it is therefore useful to have these numbers. Additionally, it may encourage an agency to think about improvements to vehicles serving such a stop or improving the stop itself: Buses with more doors, faster fare payment and validation, wider aisles, stops with level boarding. And finally, a high number of boardings per bus could indicate that a stop is underserved and needs more frequent buses!\r\nAn additional benefit of the metric is that it removes the prominence of the transfer points in the raw numbers. The four transfer points do not have much transit demand by themselves. This is aerial imagery of the North Transfer Point:\r\nAerial imagery of North Transfer Point. The area around the transfer point has parking lots, low-density residential housing, and the now-closed Oscar Meyer plant. Image: Google Maps.The reason for the high raw number of boardings is merely that the current route system is set up in a way that forces riders from outlying areas to transfer at the transfer points to get downtown or across town. That said, even after adjusting for the number of departures, the tranfers points still have a comparatively high number of boardings per bus.\r\n\r\n\r\n\r\nFricker, Jon D. 2011. “Transportation Research Board 90th Annual MeetingTransportation Research Board.” In. Washington, DC. https://trid.trb.org/view/1091291.\r\n\r\n\r\nPoletti, Flavio, Daniel Herszenhut, Mark Padgham, Tom Buckley, Danton Noriega-Goodwin, Angela Li, Elaine McVey, et al. 2022. Tidytransit: Read, Validate, Analyze, and Map GTFS Feeds. https://CRAN.R-project.org/package=tidytransit.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/boardings-per-bus-per-stop-a-useful-metric/img/boardings_per_departure_preview_map.png",
    "last_modified": "2022-03-27T13:39:56-05:00",
    "input_file": {},
    "preview_width": 572,
    "preview_height": 389
  },
  {
    "path": "posts/a-vision-zero-twitter-bot-for-madison/",
    "title": "A Vision Zero Twitter Bot for Madison",
    "description": "Using R, the rtweet package, and GitHub Actions to automate weekly tweets about traffic safety",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-02-18",
    "categories": [
      "Vision Zero",
      "Madison (WI)",
      "Twitter"
    ],
    "contents": "\r\n\r\nContents\r\nCrash data\r\nSetting up dates\r\nComposing the tweet\r\nAutomation with Github Actions\r\n\r\nI’m a big fan of Twitter bots. Accounts like EveryLot. And of course I myself run the content end of the Cyclists_of_MSN bot. I had ideas for other Twitter bots but lacked the technical skills to make them happen. So when I recently saw an article documenting how to create a Twitter bot by combining an R script with GitHub Actions, I was intrigued and got coding. The goal: A Twitter account automatically posting content related to Vision Zero in Madison, a policy and action plan that strives to eliminate all traffic fatalities and injuries in our city by 2030.\r\n\r\nI want to document that process here, focusing on things not described in other tutorials and helping others to set up their own Vision Zero bots. In general, I recommend following Matt Dray’s guide. Note his most recent update about needing to apply for elevated access for your Twitter development account!\r\nCrash data\r\nData about traffic crashes in Wisconsin is reported by police and collected and processed by the UW-Madison Traffic Operations and Safety Lab. A user-friendly interactive way to access the data is through the Community Maps portal. The web interface has no obvious option to download the underlying data, but I recently realized that there is indeed an API available, documented here1. First, we download a json file with the crash data for the whole county:\r\n\r\n\r\ndownload.file(\"https://CommunityMaps.wi.gov/crash/public/crashesKML.do?filetype=json&startyear=2022&injsvr=K&injsvr=A&county=dane\", \"crashes.json\")\r\n\r\n\r\n\r\nThe parameters for the query are mostly obvious: startyear, filetype, and county. Less obvious is the injsvr parameters: The severity of crashes is classified into five classes:\r\nK: Fatality\r\nA: Suspected Serious Injury\r\nB: Suspected Minor Injury\r\nO: No apparent Injury\r\nSo our query retrieves: a GeoJSON file of crashes flagged as fatal or resulting in serious injury2 that occurred in Dane County in 2022.\r\nWe then read in the file with the sf package:\r\n\r\n\r\ndf <- st_read(\"crashes.json\")\r\nglimpse(df)\r\n\r\n\r\n\r\nWe do a little bit of data cleaning and also drop the geometry data – for now, we’re not using the geocoded crash locations for anything. Now is also a good time to filter the data to crashes that occurred in the City of Madison.\r\n\r\n\r\ncrashes <- df %>%\r\n  mutate(date = mdy(date),\r\n         totfatl = as.numeric(totfatl),\r\n         totinj = as.numeric(totinj)) %>%\r\n  st_drop_geometry() %>% \r\n  filter(muniname == \"MADISON\")\r\n\r\n\r\n\r\nEach row in the data represents one crash – keep that in mind when producing summary data: One crash can lead to multiple injuries or fatalities.\r\nOne thing that still needs fixing is the flags variable. This is where each crash is coded for factors like driver impairment, whether speeding was involved, or if the crash involved a pedestrian or cyclists—or a deer. The problem is that the data is not well formatted. I’m not sure if this is a problem with sf reading the JSON file or something wrong with the JSON file itself. To fix this, we re-read the JSON file with the jsonlite package and append the respective columns to our file.\r\n\r\n\r\ncrashesJSON <- fromJSON(\"crashes.json\")\r\ncrashes <- crashes %>%\r\n  add_column(crashesJSON$features$properties)\r\n\r\n\r\n\r\nSetting up dates\r\nOne challenge I encountered was around setting up time intervals. I don’t know how frequently the data in Community Maps are being updated, but from comparing the data with news reports of crashes, it seems like generally data make it into the portal fairly quickly. Based on that and the overall frequency of deadly and serious crashes, I decided to calculate summary statistics weekly for the preceding week. That makes for a good compromise between timeliness and accuracy.\r\nImplementing this took me some thinking and testing with the lubridate package. First, we set a variable for today:\r\n\r\n\r\nd <- today()\r\n\r\n\r\n\r\nNext, we need to define an interval for last week:\r\n\r\n\r\nlast_week <- interval(start = floor_date(d, unit = \"week\")-8, end = floor_date(d, unit = \"week\")-1)\r\n\r\n\r\n\r\nThe way to do that is by using the rounding functions from lubridate. We round down from today’s date to the nearest week. And then we subtract 8 days to get the beginning of the previous week, and 1 day to get the end of the previous week. It’s possible there is a more elegant way to do this, but this works.\r\nFinally, we create a nicely formatted string for the interval:\r\n\r\n\r\nlast_week_formatted <- paste0(format(last_week@start, \"%d/%m\"),\r\n                         \"-\",\r\n                         format(floor_date(d, unit = \"week\")-1, \"%d/%m\"))\r\n\r\n\r\n\r\nSo this will produce something like “05/02-12/02”, which we’ll use for the content of the tweet.\r\nNext, we set up a number of summary variables that will be used for composing the tweet: The weekly and annual number of crashes and of fatalities/serious injuries resulting from those crashes. Lubridate’s %within% operator makes for easy temporal filtering.\r\n\r\n\r\ncrashes_wk <- crashes %>%\r\n  filter(date %within% last_week)\r\n\r\n#weekly numbers\r\ntot_crashes_wk <- crashes_wk %>% nrow()\r\ntot_fat_wk <- crashes_wk %>%\r\n  summarise(sum(totfatl)) %>%\r\n  pull()\r\ntot_inj_wk <- crashes_wk %>%\r\n  summarise(sum(totinj)) %>%\r\n  pull()\r\n\r\n# annual numbers\r\ntot_crashes_yr <- crashes %>% nrow()\r\ntot_fat_yr <- crashes %>%\r\n  summarise(sum(totfatl)) %>%\r\n  pull()\r\ntot_inj_yr <- crashes %>%\r\n  summarise(sum(totinj)) %>%\r\n  pull()\r\n\r\n\r\n\r\nComposing the tweet\r\nThere are two pieces to the tweet. First we create the text of the tweet, using the variables we just created. Keep in mind Twitter’s character limit when composing the text.\r\n\r\n\r\ntweet_1 <- paste0(\"Last week in Madison (\",\r\n                  last_week_formatted,\r\n                  \"), there were \",\r\n                tot_fat_wk,\r\n                \" traffic fatalities and \",\r\n                tot_inj_wk,\r\n                \" serious injury crashes. Since the beginning of the year, traffic violence has killed \",\r\n                tot_fat_yr,\r\n                \" people and seriously injured \",\r\n                tot_inj_yr,\r\n                \" people in our city. #VisionZero #StopTrafficViolence\")\r\n\r\n\r\n\r\nTo make the tweet more visually interesting, we add an automatically generated image to it. This is easy with the magick package. We start with a public domain png image (cropped to Twitter’s recommended aspect ratio and overlaid with a semi-transparent layer and our account’s Twitter handle in GIMP).\r\nA public domain image of the Wisconsin State Capitol\r\n\r\nbackground <- image_read(\"madison_1200.png\")\r\n\r\n\r\n\r\nAnd then we use image_annotate to add the crash stats to it:\r\n\r\n\r\nimage_text <- paste0(\"Vision Zero update \",\r\n                     last_week_formatted,\r\n                     \"\\n Fatalities: \",\r\n                     tot_fat_wk,\r\n                     \"\\n Serious injuries: \",\r\n                     tot_inj_wk,\r\n                     \"\\n Year-to-date fatalities: \",\r\n                     tot_fat_yr,\r\n                     \"\\n Year-to-date serious injuries:\",\r\n                     tot_inj_yr)\r\n\r\ntweet_1_img <- image_annotate(background,\r\n               image_text,\r\n               size = 60,\r\n               font = \"sans\",\r\n               weight = 700, #bold text\r\n               gravity = \"center\",\r\n               color = \"black\")\r\n\r\n\r\n\r\nGetting the text to fit the image in the right place and with the right size requires some experimenting with the parameters. Once finalized, we save the image as a file with image_write.\r\n\r\n\r\nimage_write(tweet_1_img,\r\n            path = \"tweet_1_img.png\")\r\n\r\n\r\n\r\nNow all that remains to be done in R is to post_tweet:\r\n\r\n\r\npost_tweet(status = tweet_1,\r\n           media = \"tweet_1_img.png\")\r\n\r\n\r\n\r\nThe result should look something like this:\r\nScreenshot of a tweetAutomation with Github Actions\r\nFor setting up the automation of the bot, I again followed the instructions for the London Map Bot. Make sure you load all required packages as part of your yaml file.\r\nA couple things that tripped me up in the process:\r\nStoring the API credentials as Github Secrets. The Settings > Secrets > Actions page in our Github repository has two types of secrets, Environment Secrets and Repository Secrets. You need to set up your credentials as Repository Secrets. The second (more embarrassing) issue I encountered: Matching the names of the secrets between Github, your R script, and the Github Actions yaml file. There’s a lot of copying and pasting, and the error messages from Github Actions aren’t the most helpful. And so it took me way too long to realize that the failure of my bot to run was caused by mismatched variable names. One thing that helped with troubleshooting was the ability to manually trigger the Github Action with the workflow_dispatch option.\r\nOnce everything was working, we change the trigger to a cron job: Once a week, on Wednesdays at 17:33 UTC (i.e. 12:33 PM Central), the script will run an post the tweet.\r\n\r\n\r\non:\r\n  schedule:\r\n    - cron: '33 17 * * Wed'\r\n\r\n\r\n\r\nI have some ideas for additional features for the bot, such as a map of crashes or tweets triggered by the number of crashes crossing certain thresholds. But for now the bot is up and running. For anybody wanting to run a bot for their city or county in Wisconsin, the code is very easy to modify. Just head to the bot’s Github repository and fork it! Feel free to reach out if you have questions or suggestions.\r\n\r\nThis link may break in the future. In that case go to https://transportal.cee.wisc.edu/partners/community-maps/crash/pages/help.jsp and click the link to the “Community Maps JSON/KMZ Data Service User Guide.”↩︎\r\nNote that the severity classification is mostly based on the initial police report. It does appear that crashes to get re-classified when a victim passes away at a later point.↩︎\r\n",
    "preview": {},
    "last_modified": "2022-03-27T13:48:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/do-crashes-have-a-history/",
    "title": "Do crashes have a history?",
    "description": "What the locations of past crashes can and cannot tell us about future crashes",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-01-27",
    "categories": [
      "Vision Zero",
      "map",
      "Madison (WI)"
    ],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\nlibrary(tmap)\r\nlibrary(lubridate)\r\n\r\n\r\n\r\nThe City of Madison is about to adopt their first Vision Zero action plan. The plan is central to guiding the city’s efforts to eliminate all fatal and serious traffic crashes by 2030. One main component of the plan is the high injury network. These are locations that in the past have seen a disproportionate number of crashes. As the draft action plan states: “50% of fatal and severe crashes occur on just 4% of city streets.” It intuitively makes sense: If we know that there are locations where a lot of crashes happen, let’s fix those locations and prevent crashes in the future.\r\nHowever, a comment about data from Montreal’s recently released 2020 Vision Zero report (Ville de Montréal 2021) made me curious. The comment pointed out that when you look at the locations of fatal and serious crashes that happened in 2020 and compare them with crash locations between 2015 and 2019, 85% of those crashes occurred at a location with no recent crash history. This raises the question to what extent past crashes should steer our mitigation efforts (and what alternative approaches there may be.) But first we need to know if these numbers also hold true for Madison.\r\nMap of crash locations in Montreal, comparing 2015-19 with 2020 (Image: City of Montreal)Crash data are available from Community Maps. As a first step we can look at the question visually. Here’s a map that compares 2020 and 2015-19 crashes:\r\n\r\n\r\nShow code\r\n\r\ndownload.file(\"https://CommunityMaps.wi.gov/crash/public/crashesKML.do?filetype=json&startyear=2015&en\r\ndyear=2021&injsvr=K&injsvr=A&county=dane\", \"test.json\")\r\n\r\ndf <- st_read(\"test.json\")\r\n\r\nmadison_KA <- df %>% \r\n  filter(muniname == \"MADISON\") %>% \r\n  mutate(date = mdy(date),\r\n         totfatl = as.numeric(totfatl))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\nmadison_KA %>% \r\n  mutate(yr = year(date),\r\n         yr2020 = case_when(yr == \"2020\" ~ \"2020\",\r\n                            yr %in% c(\"2015\", \"2016\", \"2017\", \"2018\", \"2019\") ~ \"2015-19\")) %>% \r\n  filter(!is.na(yr2020)) %>% \r\n  tm_shape() +\r\n  tm_dots(col = \"yr2020\", title = \"\", popup.vars = F, id = \"yr\") +\r\n  tm_layout(title = \"Fatal and serious crashes in Madison\")\r\n\r\n\r\n\r\n\r\nClearly there were crashes in 2020 at new locations. Getting to a percentage requires additional steps. First, we create 125 ft buffers1 around all crash locations.\r\n\r\n\r\nShow code\r\n\r\nmadison_KA_buffered <- madison_KA %>% \r\n  mutate(yr = year(date),\r\n         yr2020 = case_when(yr == \"2020\" ~ \"2020\",\r\n                            yr %in% c(\"2015\", \"2016\", \"2017\", \"2018\", \"2019\") ~ \"2015-19\")) %>% \r\n  filter(!is.na(yr2020)) %>% \r\n  st_transform(8193) %>% \r\n  st_buffer(125)\r\n\r\n\r\n\r\nNow we check for overlap between the buffers. If a 2020 crash buffer intersects with any 2015-19 buffer, we would say it occurred “at the same location.” In other words, a 2020 crash can be at most 250 ft away from a 2015-19 crash to be considered to have happened at the same location.\r\n\r\n\r\nShow code\r\n\r\npoly_1519 <- madison_KA_buffered %>% \r\n  filter(yr2020 == \"2015-19\") %>% \r\n  st_combine() %>% \r\n  st_make_valid()\r\n  \r\n\r\n\r\nmadison_KA_buffered %>% \r\n  filter(yr2020 == \"2020\") %>% \r\n  mutate(new = case_when(lengths(st_intersects(.,poly_1519)) > 0 ~ \"same location\",\r\n                         lengths(st_intersects(.,poly_1519)) == 0 ~ \"different location\")) %>% \r\n  group_by(new) %>% \r\n  st_drop_geometry() %>% \r\n  summarize(n = n()) %>% \r\n  ggplot(aes(new, n, fill = new)) +\r\n  hrbrthemes::theme_modern_rc() +\r\n  geom_col(show.legend = F) +\r\n  geom_text(aes(label = n), nudge_y = 4, color = \"white\")+\r\n  scale_fill_brewer() +\r\n  xlab(element_blank())+\r\n  ylab(\"number of crashes\") +\r\n  labs(title = str_wrap(\"Crashes in 2020 often occured in locations with no crash history\", 40),\r\n       caption = str_wrap(\"Data: CommunityMaps crash data for fatal (K) and severe (A) injuries, 2015-20. Analysis and visualization: Harald Kliems\", 65))\r\n\r\n\r\n\r\n\r\nThe result: Out of the 103 deadly and serious crashes that occurred in 2020, 29 (28%) were at a location where there had also been a crash between 2015 and 2019. The remaining 74 crashes (72%) happened at a location with no previous crash history. This is a lower proportion than what was found in Montreal, where 85% of crashes happened at locations without a crash history. It is possible that some or all of the difference between Madison and Montreal is an artifact of the analysis. For example, the Montreal figures mention “intersections” whereas this analysis does not match crashes to intersections. Changing the buffer around crash locations would also change the proportion (a larger buffer would lead to more matched crashes).\r\nMethodological issues aside, what about the conclusion of the Montreal findings? “This observation reinforces the need to act on the entire road system rather than on sites considered ‘accident-prone.’” Pragmatically, few measures act on the entire road system (e.g. a city-wide speed limit reduction) and we need a process for prioritizing scarce resources. Maybe the takeaway should be that past crash history should be only one part of that prioritization process and that we need to be aware of its limitations.\r\n\r\n\r\n\r\nVille de Montréal. 2021. “État de La Sécurité Routière 2020.” Montreal. https://portail-m4s.s3.montreal.ca/pdf/etat_de_la_securite_routiere_2020vdem_0.pdf.\r\n\r\n\r\nThe City uses 250 ft buffers around intersections to distinguish between intersection crashes and road segment crashes. Any crash within that buffer is counted as occurring at that intersection.↩︎\r\n",
    "preview": "posts/do-crashes-have-a-history/do_crashes_have_history_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-27T17:30:24-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/where-the-shelter-cant-go-using-data-to-dissect-city-policy-making/",
    "title": "Where the shelter can't go: Using data to dissect city policy-making",
    "description": "Policymakers proposed criteria for a where a shelter for homeless men in Madison (Wisconsin) could and couldn't go. This post uses public data to visualize these criteria, showing that in the end the resolution would have meant that the shelter can't go anywhere.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-07-24",
    "categories": [
      "housing",
      "Madison (WI)",
      "map"
    ],
    "contents": "\r\n\r\nContents\r\nCity limits\r\nChildcare locations and schools\r\nZoning districts\r\nClose to the Beacon\r\nWhat’s left?\r\n\r\nThis post is about using publicly available data to examine local policy proposals. A local blogger pointed out a resolution to the Madison Common Council that would have established a number of criteria for possible locations for a shelter for men experiencing homeless. The shelter had been debated for many years, and unsurprisingly it was controversial. The criteria specified in the draft resolution were as follows:\r\nThe site should be greater than .5 mile from any schools or daycares;\r\nThe site should not be located in a Census tract identified as a location of concentrated poverty\r\nThe site should include either an existing building or a lot large enough for new construction;\r\nThe site should include space for future expansion;\r\nThe site should be zoned for commercial or mixed use, and not adjacent to single-family homes;\r\nThe site should be within a few blocks of seven-days-a-week bus service, with a preference for close proximity to BRT routes;\r\nThe site location should be within a walking distance of within 3.5 miles from the Beacon on East Washington; and\r\nThe site should be an active real estate listing that is vacant or soon-to-be-vacant.\r\nThat’s a lot of places where the shelter must not be! It’s also a number of criteria that I could easily map with publicly available data. The meeting at which the resolution would be first discussed was on the same day and I only had the morning before work and my lunch break to put something together. Here’s what I submitted, and there was some post-meeting coverage from a local blogger. Because of the time constraint, the product was not the prettiest, and in this post I make some improvements and include the code.\r\nCity limits\r\nWhat I should have done first but didn’t was to start with the Madison city limits. The shelter is a city shelter and therefore must be within the City of Madison. There are multiple enclaves and exclaves, and so starting with the city limits as the most expansive possible search area should be step 1. Then we can use the other criteria to “stamp out” anything that doesn’t fit. City limits are available on the City of Madison Open Data portal.\r\n\r\n\r\nShow code\r\n\r\ncity_limit <- st_read(\"data/City_Limit.shp\") %>% \r\n  st_transform(6610)\r\n\r\n\r\nReading layer `City_Limit' from data source \r\n  `C:\\Users\\user1\\Documents\\shelter_locations\\data\\City_Limit.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 4 features and 3 fields\r\nGeometry type: POLYGON\r\nDimension:     XY\r\nBounding box:  xmin: -89.57165 ymin: 42.99815 xmax: -89.24663 ymax: 43.17202\r\nGeodetic CRS:  WGS 84\r\n\r\nShow code\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons(\"green\")\r\n\r\n\r\n\r\n\r\nChildcare locations and schools\r\nFirst of all, the idea that children need to be kept away from people experiencing homelessness is wrong. Second, most people don’t realize how many schools and childcare locations there are. For childcare providers, the Wisconsin Department of Health and Human Services provides a dataset of “[p]oint locations and attributes of certified childcare facilities licensed by the state of Wisconsin.” Public school data is available from the National Center for Education Statistics. Conveniently, both datasets are provided as shapefiles. I didn’t look for data sources for private schools.\r\nFirst we read in the child_care data. I noticed that there is a “capacity” attribute in the dataset. To avoid debates of whether the resolution sponsors would consider someone who provides care to only a few children a “childcare facility,” I filtered the data to only those providers with a capacity larger than 9. To create the half-mile buffers around the locations, I need to project the data to a different coordinate reference system (CRS). The crsuggest package helps identify an appropriate CRS. Note that the CRS I chose uses meters as its units, and so 0.5 miles needs to be provided as 805. Finally, I keep only the parts of the buffers that are within city limits.\r\n\r\n\r\nShow code\r\n\r\nchild_care <- st_read(\"data/Wisconsin_Licensed_and_Certified_Childcare.shp\")\r\n\r\n\r\nReading layer `Wisconsin_Licensed_and_Certified_Childcare' from data source `C:\\Users\\user1\\Documents\\shelter_locations\\data\\Wisconsin_Licensed_and_Certified_Childcare.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 4669 features and 31 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -10372680 ymin: 5197160 xmax: -9694399 ymax: 5946135\r\nProjected CRS: WGS 84 / Pseudo-Mercator\r\n\r\nShow code\r\n\r\nchild_care_buffers <- child_care %>% \r\n  filter(COUNTY == \"Dane\") %>% \r\n  filter(CAPACITY >9) %>% \r\n  st_transform(6610) %>% \r\n  st_buffer(dist = 805) %>% \r\n  st_intersection(city_limit)\r\n\r\n\r\n\r\nFor the schools, the process was pretty much the same:\r\n\r\n\r\nShow code\r\n\r\nschools <- readRDS(\"data/EDGE_GEOCODE_PUBLICSCH_1920.RDS\") #https://nces.ed.gov/opengis/rest/services/K12_School_Locations/EDGE_GEOCODE_PUBLICSCH_1920/MapServer\r\n\r\nschools_buffers <- schools %>% \r\n  st_transform(6610) %>% \r\n  st_buffer(dist = 805) %>% \r\n  st_intersection(city_limit)\r\n\r\n\r\n\r\nNow we can “cut out” the schools and childcare buffers from the city limit map:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons(\"green\") +\r\n  tm_shape(child_care_buffers) +\r\n  tm_polygons(\"red\", alpha = 1, border.alpha = 0) +\r\n  tm_shape(schools_buffers) +\r\n  tm_polygons(\"red\", alpha = 1, border.alpha = 0)\r\n\r\n\r\n\r\n\r\nIt’s obvious: This criterion alone eliminate huge swaths of the city as possible locations.\r\nZoning districts\r\n\r\nThe site should be zoned for commercial or mixed use, and not adjacent to single-family homes;\r\n\r\nI’ve posted about zoning in Madison previously and so I’m pretty familiar with the data. Zoning data is on the Open Data portal.\r\n\r\n\r\nShow code\r\n\r\nzoning <- st_read(\"data/Zoning_Districts.shp\")\r\n\r\n\r\nReading layer `Zoning_Districts' from data source \r\n  `C:\\Users\\user1\\Documents\\shelter_locations\\data\\Zoning_Districts.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 2343 features and 9 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: -89.57153 ymin: 42.99815 xmax: -89.24663 ymax: 43.17202\r\nGeodetic CRS:  WGS 84\r\n\r\nWhich zoning districts should be included? Ordinance does have a high-level category called “Mixed use and commercial districts,” which includes\r\nLMX Limited Mixed-Use\r\nNMX Neighborhood Mixed-Use District\r\nTSS Traditional Shopping Street District\r\nMXC Mixed-Use Center District\r\nCC-T Commercial Corridor - Transitional District\r\nCC Commercial Center District\r\nRMX Regional Mixed-Use District\r\nArguably, many would also consider some of the zoning districts listed under the “Downtown and Urban Core” heading as mixed use. As it has “mixed use” in its name, I’ll include UMX Urban Mixed-Use.\r\n\r\n\r\nShow code\r\n\r\nmixed_use <- c(\"NMX\", \"TSS\", \"MXC\", \"CC-T\", \"CC\", \"UMX\", \"RMX\")\r\n\r\nzoning_mixed_use <- zoning %>% \r\n  filter(!(ZONING_COD %in% mixed_use)) %>% \r\n  st_transform(st_crs(city_limit))\r\n\r\n\r\n\r\nI’ll overlay these in red again and change the color of the previously cut out areas to grey.\r\n\r\n\r\nShow code\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons(\"green\") +\r\n  tm_shape(zoning_mixed_use) +\r\n  tm_polygons(\"red\", alpha = 1, border.alpha = 0) +\r\n  tm_shape(child_care_buffers) +\r\n  tm_polygons(alpha = 1, border.alpha = 0) +\r\n  tm_shape(schools_buffers) +\r\n  tm_polygons(alpha = 1, border.alpha = 0)\r\n\r\n\r\n\r\n\r\nAnd there goes much of the rest of the city.\r\nClose to the Beacon\r\nI probably could have stopped here, but as it’s an interesting one, I’ll add one more: The 3.5 mile walkshed around the Beacon, an existing homeless services day center on East Washington Ave. For sending my letter, I used the OpenRouteService (ORS) web interface to generate the walkshed. However, it’s also possible to access ORS through an API with the R package openrouteservice-r, which I will do here. First, we geocode the Beacon’s address:\r\n\r\n\r\nShow code\r\n\r\nlibrary(openrouteservice)\r\nbeacon <- ors_geocode(\"The Beacon, Madison, Wisconsin, USA\", output = \"sf\", size = 1)\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons()+\r\n  tm_shape(beacon) +\r\n  tm_dots(\"red\")\r\n\r\n\r\n\r\n\r\nNow we generate the 3.5 mile walking distance. The ors_isochrone functions requires the location to be a long/lat coordinate pair.\r\n\r\n\r\nShow code\r\n\r\nbeacon_coord <- st_coordinates(beacon)\r\n\r\nbeacon_walkshed <- ors_isochrones(st_coordinates(beacon), \r\n                                  profile = ors_profile(\"walking\"),\r\n                                  units = \"mi\",\r\n                                  range = 3.5,\r\n                                  range_type = \"distance\",\r\n                                  #smoothing = \"15\", \r\n                                  output = \"sf\") %>% \r\n  st_transform(st_crs(city_limit))\r\n\r\n\r\n\r\nQuick visual check:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons(alpha = .1) +\r\ntm_shape(beacon_walkshed) +\r\n  tm_polygons(\"red\", alpha = .3)\r\n\r\n\r\n\r\n\r\nAgain, we’ll change the previously cut out parts to grey and show everything outside the walk shed in red.\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"plot\")\r\ntm_shape(city_limit) +\r\n  tm_polygons(\"green\") +\r\n  tm_shape(zoning_mixed_use) +\r\n  tm_polygons(\"red\", alpha = 1, border.alpha = 0) +\r\n  tm_shape(child_care_buffers) +\r\n  tm_polygons(alpha = 1, border.alpha = 0) +\r\n  tm_shape(schools_buffers) +\r\n  tm_polygons(alpha = 1, border.alpha = 0) +\r\n  tm_shape(st_difference(city_limit, beacon_walkshed)) +\r\n  tm_polygons(\"red\", border.alpha = 0)\r\n\r\n\r\n\r\n\r\nWhat’s left?\r\nWe could keep going with the remaining criteria, but a) they’re harder to implement and b) it looks like not much green is left in the map above. This is a good moment to change to interactive viewing and see what those green areas are:\r\n\r\n\r\nShow code\r\n\r\nwhats_left <- st_intersection(beacon_walkshed, city_limit)\r\n\r\n#helper function to \"erase\" geometry y from x\r\nst_erase <-  function(x, y) st_difference(x, st_union(y))\r\n\r\nwhats_left <- st_erase(whats_left, child_care_buffers)\r\n\r\nwhats_left <- st_erase(whats_left, schools_buffers)\r\n\r\nwhats_left <- st_erase(whats_left, zoning_mixed_use)\r\n\r\ntmap_options(check.and.fix = TRUE)\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(whats_left) +\r\n  tm_polygons(\"green\", alpha = .5)\r\n\r\n\r\n\r\n\r\nLeft as possible locations are: The two largest areas are on the water, on Lake Wingra and Monona Bay. Other than some thin slivers of land that are clearly too small (and probably artefacts), there are three remaining spots. Two small triangles in the Eken Park neighborhood near the Oscar Mayer area (one currently a rental car parking lot; the other a single-family home zoned Commercial). And finally a cluster of properties at the intersection of Milwaukee St and Fair Oaks Ave, currently occupied by gas station, a mixed used building with a bridal store, and a restaurant.\r\nWithout investigating further, I’m confident that any of those non-water sites would fail one or more of the other criteria. And even if they didn’t: If your criteria set up that in a city the size of Madison your choices are this narrow, you either didn’t understand the implications of this (no shelter) or you did understand them and wanted to hide your policy goals behind seemingly reasonable criteria.\r\nDid you like this post? Consider making a donation to Urban Triage, a Madison-based organization that provides resources to people experiencing or at risk of homelessness. Or support the Madison Tenant Resource Center.\r\n\r\n\r\n\r\n",
    "preview": "posts/where-the-shelter-cant-go-using-data-to-dissect-city-policy-making/where_shelter_cant_go_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-03-27T17:20:35-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/scraping-4s/",
    "title": "Scraping 4S",
    "description": "Web scraping and text analysis of the Society of Social Studies of Science's 2021 annual meeting panels.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-04-10",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nScraping the data\r\nText Mining SSSS panels\r\nUnigrams, bigrams, trigrams\r\nTerm frequency–inverse document frequency\r\n\r\nVisualizations\r\n\r\nI recently went though Julia Silge’s great interactive tidytext tutorial to learn some basics of text analysis. Last weekend, my SO was reading through the panel descriptions of the Society for the Social Studies of Science’s (4S) annual meeting to figure out which panel to submit an abstract to. There were over 200 panels to choose from, and so this would be a good opportunity to practice some of the techniques I learned.\r\nThe tidytext tutorial works with data sets that are already nice and clean. For the 4S panels we need to do some web scraping and data cleaning first. The web scraping with rvest is also a first for me. The first part of this post will cover the scraping and cleaning. If you want to read about the text analysis, skip ahead to the second part.\r\nScraping the data\r\nAll panels are listed on this page: https://www.4sonline.org/meeting/accepted-open-panels/. We could scrape the panel titles and descriptions right there. Note, however, that the description here is only a snippet, not the full panel description. We’ll deal with this later.\r\nStep one is to load the relevant libraries and read in the html page.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(rvest)\r\nlibrary(rmarkdown)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\npanel_url <- \"https://www.4sonline.org/meeting/accepted-open-panels\"\r\npanels <- read_html(panel_url)\r\npanels\r\n\r\n\r\n{html_document}\r\n<html lang=\"en-US\">\r\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; cha ...\r\n[2] <body class=\"page-template-default page page-id-12988 page-chil ...\r\n\r\nThe next step is to extract the relevant information from that HTML document. If you have a basic understanding of how HTML and CSS work, you could do this by opening the source code of the page in your browser and manually identify the relevant elements.\r\nScreenshot source codeHere you see that the panel titles are all level 3 headings (<h3>) of the class entry-title. An easier way to identify the element is by using your browser’s element picker. In Firefox, press F12 and choose the element picker. Then choose the element you want to identify and look at the code in the Inspector window. Here’s a screenshot of what this looks like for the panel description:\r\nScreenshot element pickerSo the panel description is a div of the class post-inner-content. (There is a third method to identify relevant content, which we’ll use later).\r\nWith this information, we’re ready to extract panel titles and descriptions:\r\n\r\n\r\nShow code\r\n\r\npanel_titles <- panels %>% html_nodes(\".entry-title\") %>% html_text()\r\n\r\npanel_desc <- panels %>% html_nodes(\".post-content-inner\") %>% html_text()\r\n\r\npanels_all <- tibble(title = panel_titles, desc = panel_desc)\r\n\r\n\r\n\r\nA quick bit of data cleaning to remove the numbers from the titles and use them as an ID column instead:\r\n\r\n\r\nShow code\r\n\r\npanels_all <- panels_all %>% \r\n  mutate(id = str_extract(title, \"^\\\\d*\"),\r\n         title = str_remove(title, \"^\\\\d+.\\\\s\"))\r\n  \r\npanels_all\r\n\r\n\r\n# A tibble: 210 x 3\r\n   title                          desc                           id   \r\n   <chr>                          <chr>                          <chr>\r\n 1 (Bio)Engineering Nature: Edit~ Since the discovery in 2015 o~ 1    \r\n 2 (Dis)Trust in Public-Sector D~ STS scholarship on technopoli~ 2    \r\n 3 (Im)material streams in the c~ The deepening datafication of~ 3    \r\n 4 (Re)configuring care practice~ Care practices were addressed~ 4    \r\n 5 (Re)materialising Cancer: Bod~ Bioscientific approaches to i~ 5    \r\n 6 A Good Life, In Theory: Think~ In recent years, an increasin~ 6    \r\n 7 Academic Automation, Machine ~ Over the last 70 years, compu~ 7    \r\n 8 Aerotechnologies: Air as Elem~ “Objects, processes, or event~ 8    \r\n 9 Aging with other-than-humans:~ In recent years, Science and ~ 9    \r\n10 AI and Feminist STS            Much excellent work has docum~ 10   \r\n# ... with 200 more rows\r\n\r\nLooks pretty good, doesn’t it? Let’s do a simple analysis: What are the most frequent words in the panel titles (after removing common stop words)?\r\n\r\n\r\nShow code\r\n\r\npanels_all %>% \r\n  unnest_tokens(word, title, token = \"words\") %>% \r\n  anti_join(get_stopwords()) %>% \r\n  count(word, sort = T)\r\n\r\n\r\n# A tibble: 723 x 2\r\n   word             n\r\n   <chr>        <int>\r\n 1 sts             29\r\n 2 relations       24\r\n 3 science         23\r\n 4 technologies    15\r\n 5 technology      15\r\n 6 global          14\r\n 7 good            13\r\n 8 governance      13\r\n 9 knowledge       12\r\n10 research        12\r\n# ... with 713 more rows\r\n\r\nUnsurprisingly, STS has the top spot – it’s the acronym for the whole field: Science and Technology Studies. Before diving deeper into the analysis, let’s get the complete descriptions. This requires following the links from the panel titles and scraping those pages. This is a good opportunity to use a third option of identifying elements in an HTML page: Rvest’s Selector Gadget. By loading the bookmarklet on the overview page we can identify the URL elements for scraping. We extract the URLs like so:\r\n\r\n\r\nShow code\r\n\r\nurls <- panels %>% \r\n  html_nodes(\"#post-12988 a\") %>% \r\n  html_attr(\"href\") %>% \r\n  tibble()\r\n\r\ntail(urls)\r\n\r\n\r\n# A tibble: 6 x 1\r\n  .                                                                   \r\n  <chr>                                                               \r\n1 https://www.4sonline.org/category/op21/race-and-racism-21/          \r\n2 https://www.4sonline.org/category/op21/science-communication-public~\r\n3 https://www.4sonline.org/category/op21/social-justice-social-moveme~\r\n4 https://www.4sonline.org/category/op21/transnational-sts-21/        \r\n5 https://www.4sonline.org/category/op21/other-21/                    \r\n6 #                                                                   \r\n\r\nThe list of URLs is fairly clean, but there are a few irrelevant URLs that slipped through. We’ll filter them out with a regular expression.\r\n\r\n\r\nShow code\r\n\r\nurls <- urls %>% \r\n  rename(url = \".\")\r\n\r\n#filter out irrelevant URLs\r\nurls <- urls %>% \r\n  filter(str_detect(url, \"https://www.4sonline.org/\\\\d\")) %>% \r\n  pull(url)\r\n\r\n\r\n\r\nNow we can get all pages by using map_df across all URLs and a custom function to extract the relevant information. Let’s develop the function using the first URL. Again using the Selector Gadget, it seems like these should be the relevant elements to extract:\r\n\r\n\r\nShow code\r\n\r\npage <- read_html(urls[1])\r\n\r\npage_2 <- page %>% \r\n  html_nodes(\".et_pb_text_inner , .et_pb_post_content_0_tb_body p, .et_pb_module_header\") %>% \r\n  html_text() \r\n\r\n\r\ntibble(\r\n  title = page_2[1],\r\n  organizer = page_2[2],\r\n  posted = page_2[3],\r\n  desc = page_2[4],\r\n  contact = page_2[5],\r\n  keywords = page_2[6]\r\n)\r\n\r\n\r\n# A tibble: 1 x 6\r\n  title       organizer    posted   desc       contact   keywords     \r\n  <chr>       <chr>        <chr>    <chr>      <chr>     <chr>        \r\n1 1. (Bio)En~ Elliott Rei~ \"Posted~ \"Since th~ Contact:~ Keywords: ge~\r\n\r\nNow we can just run the function over the length of the URL vector with map_df. Don’t run this yet, though.\r\n\r\n\r\nShow code\r\n\r\npages_full <- map_df(1:length(urls), function(i) {\r\n  page <- read_html(urls[i])\r\n\r\npage_2 <- page %>% \r\n  html_nodes(\".et_pb_text_inner , .et_pb_post_content_0_tb_body p, .et_pb_module_header\") %>% \r\n  html_text() \r\n\r\ntibble(\r\n  title = page_2[1],\r\n  organizer = page_2[2],\r\n  posted = page_2[3],\r\n  desc = page_2[4],\r\n  contact = page_2[5],\r\n  keywords = page_2[6]\r\n)\r\n  \r\n}\r\n  )\r\n\r\n\r\n\r\nIf you ran the code and took a look at the resulting data frame, you’d see that the code breaks for panels that have multiple <p> paragraphs in the description. There may be a more elegant fix, but for now we change the code for the html_elements to be less specific and not split out individual paragraphs. The downside is that the contact info and keywords get lumped in with the description. We can fix that later.\r\n\r\n\r\nShow code\r\n\r\npages_full <- map_df(1:length(urls), function(i) {\r\n  page <- read_html(urls[i])\r\n\r\npage_2 <- page %>% \r\n  html_nodes(\".et_pb_post_content_0_tb_body , .et_pb_text_inner, .et_pb_module_header\") %>% \r\n  html_text() \r\n\r\ntibble(\r\n  title = page_2[1],\r\n  organizer = page_2[2],\r\n  posted = page_2[3],\r\n  desc = page_2[4]\r\n)\r\n\r\n  \r\n}\r\n  )\r\n\r\n\r\n\r\nAs scraping 210 pages takes a long time, we’ll save the results as an rds file.\r\n\r\n\r\nShow code\r\n\r\nwrite_rds(pages_full, paste0(\"data/pages_full_\", Sys.Date(), \".rds\"))\r\n\r\n\r\n\r\nNext we’ll clean the panel data, including fixing the lumped together panel description, contact info, and keywords. This requires some regular expression magic.\r\n\r\n\r\nShow code\r\n\r\npages_full <- read_rds(\"data/pages_full_2021-03-07.rds\")\r\n\r\npanels_full_wide <- pages_full %>% \r\n  separate(organizer, c(\"organizer_1\", \"organizer_2\", \"organizer_3\", \"organizer_4\", \"organizer_5\", \"organizer_6\", \"organizer_7\"), \"; \") %>% \r\n  mutate(id = str_extract(title, \"^\\\\d*\"),\r\n         title = str_remove(title, \"^\\\\d*.\\\\s\"),\r\n         keywords = str_extract(desc, \"(?<=(Keywords\\\\:\\\\s))(.*)\"),\r\n         desc = str_extract(desc, \"(.|\\\\n)*(?=\\\\n\\\\nContact)\")) %>% #return everything before contact\r\n  separate(keywords, c(\"keyword_1\", \"keyword_2\", \"keyword_3\", \"keyword_4\", \"keyword_5\", \"keyword_6\", \"keyword_7\", \"keyword_8\"), \",|;\")\r\n\r\npanels_full_wide\r\n\r\n\r\n# A tibble: 210 x 19\r\n   title  organizer_1  organizer_2 organizer_3 organizer_4 organizer_5\r\n   <chr>  <chr>        <chr>       <chr>       <chr>       <chr>      \r\n 1 (Bio)~ \"Elliott Re~ <NA>        <NA>        <NA>        <NA>       \r\n 2 (Dis)~ \"danah boyd~ Janet Vert~ Alondra Ne~ <NA>        <NA>       \r\n 3 (Im)m~ \"Morgan Mou~ Ryan Burns~ <NA>        <NA>        <NA>       \r\n 4 (Re)c~ \"Stefan Nic~ Cristina P~ <NA>        <NA>        <NA>       \r\n 5 (Re)m~ \"Emily Ross~ Julia Swal~ <NA>        <NA>        <NA>       \r\n 6 A Goo~ \"Claire Oli~ <NA>        <NA>        <NA>        <NA>       \r\n 7 Acade~ \"Jeremy Hun~ <NA>        <NA>        <NA>        <NA>       \r\n 8 Aerot~ \"Boyd Ruamc~ Jia Hui Le~ <NA>        <NA>        <NA>       \r\n 9 Aging~ \"Daniel Lop~ Nete Schwe~ <NA>        <NA>        <NA>       \r\n10 AI an~ \"Rachel Ber~ <NA>        <NA>        <NA>        <NA>       \r\n# ... with 200 more rows, and 13 more variables: organizer_6 <chr>,\r\n#   organizer_7 <chr>, posted <chr>, desc <chr>, id <chr>,\r\n#   keyword_1 <chr>, keyword_2 <chr>, keyword_3 <chr>,\r\n#   keyword_4 <chr>, keyword_5 <chr>, keyword_6 <chr>,\r\n#   keyword_7 <chr>, keyword_8 <chr>\r\n\r\nIn addition to the what is contained in the individual panel pages, there are also topic areas/themes for panels:\r\nTopic areasGetting theme urls and labels is easy:\r\n\r\n\r\nShow code\r\n\r\ntheme_urls <- panels %>% \r\n  html_elements(\"#menu-op21 a\") %>%\r\n  html_attr(\"href\")\r\n\r\ntheme_labels <- panels %>% \r\n  html_elements(\"#menu-op21 a\") %>%\r\n  html_text()\r\n\r\n\r\n\r\nWe’ll download all theme pages to then extract the panel IDs.\r\n\r\n\r\nShow code\r\n\r\nthemes_full <- map_df(1:length(theme_urls), function(i) {\r\n  page <- read_html(theme_urls[i])\r\ntheme <- theme_labels[[i]]\r\n\r\npage_2 <- page %>% \r\n  html_nodes(\".entry-title\") %>% \r\n  html_text()\r\n\r\ntibble(title = page_2, theme) %>% \r\n  mutate(id = str_extract(title, \"^\\\\d*\"),\r\n         title = str_remove(title, \"^\\\\d*.\\\\s\"))\r\n}\r\n)\r\n\r\nglimpse(themes_full)\r\n\r\n\r\nRows: 582\r\nColumns: 3\r\n$ title <chr> \"The Prediction Factor: Medical Decision in the Age of~\r\n$ theme <chr> \"Big Data, AI and Machine Learning\", \"Big Data, AI and~\r\n$ id    <chr> \"187\", \"197\", \"200\", \"177\", \"180\", \"143\", \"144\", \"152\"~\r\n\r\nWe can see that there are 582 observations for 210 panels. So a panel can be listed in more than one theme. Different types of analysis require different data formats, and so we’ll create two data frames: For the first one, we want to keep one row per panel. This requires a sequence of pivot_wider, unite, and separate on the themes data before doing a join with the panels_full_wide data frame.\r\n\r\n\r\nShow code\r\n\r\nthemes_full_wide <- themes_full %>% \r\n  pivot_wider(id_cols = c(title, id), names_from = \"theme\", values_from = theme) %>% \r\n  unite(\"theme\", 3:26, sep = \";\", remove = T, na.rm = T) %>% \r\n  separate(theme, into = c(\"theme_1\", \"theme_2\", \"theme_3\"), sep = \";\")\r\n\r\npanels_full_wide <- panels_full_wide %>% \r\n  left_join(themes_full_wide, by = c(\"id\", \"title\"))\r\n\r\n\r\n\r\nWith the multiple keyword_n, organizer_n, and theme_n columns, the data does not lend itself to an analysis by these variables. For that, we need to pivot the data to a longer format.\r\n\r\n\r\nShow code\r\n\r\npanels_full_long <- panels_full_wide %>% \r\n  pivot_longer(starts_with(\"organizer_\"), names_prefix = \"organizer_\", names_to = \"organizer_order\", values_to = \"Organizer_name\", values_drop_na = T) %>% \r\n  pivot_longer(starts_with(\"keyword_\"), names_prefix = \"keyword_\", names_to = \"keyword_order\", values_to = \"keyword\", values_drop_na = T) %>% \r\n  pivot_longer(starts_with(\"theme_\"), names_prefix = \"theme_\", names_to = \"theme_order\", values_to = \"theme\", values_drop_na = T)\r\n\r\nglimpse(panels_full_long)\r\n\r\n\r\nRows: 6,305\r\nColumns: 10\r\n$ title           <chr> \"(Bio)Engineering Nature: Editing environmen~\r\n$ posted          <chr> \"Posted: January 27, 2021\\n\", \"Posted: Janua~\r\n$ desc            <chr> \"\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\tSince the dis~\r\n$ id              <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",~\r\n$ organizer_order <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",~\r\n$ Organizer_name  <chr> \"Elliott Reichardt, Stanford University\", \"E~\r\n$ keyword_order   <chr> \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"3\", \"3\", \"3\",~\r\n$ keyword         <chr> \"gene drives\", \"gene drives\", \"gene drives\",~\r\n$ theme_order     <chr> \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\",~\r\n$ theme           <chr> \"Environmental/Multispecies Studies\", \"Genet~\r\n\r\nThat’s it for data prep! For your convenience, here are the two data frames:\r\nOne row per panel (wide format)\r\nrds\r\ncsv\r\n\r\nmultiple rows per panel (long format)\r\nrds\r\ncsv\r\n\r\nText Mining SSSS panels\r\nLet’s do some basic text mining on the cleaned panel data. What are the words most commonly used in the panel descriptions, with stop words like “the,” “you,” “a,” … removed?\r\nUnigrams, bigrams, trigrams\r\n\r\n\r\nShow code\r\n\r\npanels_full_wide %>% \r\n  unnest_tokens(word, desc, token = \"words\") %>% \r\n  anti_join(get_stopwords()) %>% \r\n  count(word, sort = TRUE)\r\n\r\n\r\n# A tibble: 7,420 x 2\r\n   word          n\r\n   <chr>     <int>\r\n 1 sts         280\r\n 2 panel       267\r\n 3 social      244\r\n 4 science     233\r\n 5 practices   228\r\n 6 data        188\r\n 7 research    186\r\n 8 relations   185\r\n 9 can         176\r\n10 new         165\r\n# ... with 7,410 more rows\r\n\r\nNot terribly exciting, is it? Let’s do the same but for bigrams.\r\n\r\n\r\nShow code\r\n\r\npanels_full_wide %>% \r\n  unnest_tokens(word, desc, token = \"ngrams\", n = 2) %>% \r\n  count(word, sort = TRUE)\r\n\r\n\r\n# A tibble: 35,269 x 2\r\n   word            n\r\n   <chr>       <int>\r\n 1 of the        218\r\n 2 in the        213\r\n 3 this panel    168\r\n 4 and the       145\r\n 5 to the        117\r\n 6 on the         99\r\n 7 as a           87\r\n 8 such as        79\r\n 9 papers that    67\r\n10 with the       62\r\n# ... with 35,259 more rows\r\n\r\n\r\n\r\nShow code\r\n\r\nbigram <- panels_full_wide %>% \r\n  unnest_tokens(bigram, desc, token = \"ngrams\", n = 2)\r\n\r\nbigrams_separated <- bigram %>% separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\r\n\r\nbigrams_filtered <- bigrams_separated %>% \r\n    filter(!word1 %in% stop_words$word) %>%\r\n  filter(!word2 %in% stop_words$word)\r\n\r\n\r\n\r\nbigrams_filtered %>% count(word1, word2, sort = TRUE) %>% \r\n  head(500) %>% \r\n  paged_table()\r\n\r\n\r\n\r\n\r\n\r\nMuch better? If two is better than one, is three better than two? Let’s try trigrams, without stop word filtering.\r\n\r\n\r\nShow code\r\n\r\npanels_full_wide %>% \r\n  unnest_tokens(word, desc, token = \"ngrams\", n = 3) %>% \r\n  count(word, sort = TRUE)\r\n\r\n\r\n# A tibble: 49,575 x 2\r\n   word                       n\r\n   <chr>                  <int>\r\n 1 as well as                40\r\n 2 what are the              31\r\n 3 in this panel             30\r\n 4 science and technology    30\r\n 5 this panel we             29\r\n 6 this open panel           27\r\n 7 in order to               23\r\n 8 we invite papers          23\r\n 9 not limited to            22\r\n10 the covid 19              22\r\n# ... with 49,565 more rows\r\n\r\nThis does still have a lot of not-so-meaningful rows, and we’ll see what happens if we filter for stop words:\r\n\r\n\r\nShow code\r\n\r\ntrigram <- panels_full_wide %>% \r\n  unnest_tokens(trigram, desc, token = \"ngrams\", n = 3)\r\n\r\ntrigrams_separated <- trigram %>% separate(trigram, c(\"word1\", \"word2\", \"word3\"), sep = \" \")\r\n\r\ntrigrams_filtered <- trigrams_separated %>% \r\n    filter(!word1 %in% stop_words$word) %>%\r\n  filter(!word2 %in% stop_words$word) %>% \r\n  filter(!word3 %in% stop_words$word)\r\n\r\ntrigrams_filtered %>% \r\n  count(word1, word2, word3, sort = T) %>% \r\n  filter(n<1) %>% \r\n  paged_table()\r\n\r\n\r\n\r\n\r\n\r\nFor trigrams, filtering for stop words is tricky. The filtered analysis has fewer irrelevant trigrams such as “in order to” or “as well as,” but it also filters out meaningful phrases like “the global south” that otherwise would feature prominently.\r\nTerm frequency–inverse document frequency\r\nGoing further than just counting words and ngrams, we can look at their relevance in comparison to the whole of all panel descriptions. This addresses the issue that terms like “science and technology” or “this open panel” are going to feature in a large number of panel descriptions and therefore don’t add a lot of information. To do this, we look at the term frequency–inverse document frequency (tf-idf), “a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.”(“Tfidf” 2021) The tidytext package has the convenient bind_tf_idf() function for this:\r\n\r\n\r\nShow code\r\n\r\npanels_full_wide %>% \r\n  unnest_tokens(word, desc, token = \"words\") %>% \r\n  count(word, id) %>% \r\n  bind_tf_idf(word, id, n) %>%\r\n  left_join(panels_full_wide, by = \"id\") %>% \r\n  select(word, n, tf_idf, title) %>% \r\n  arrange(-tf_idf) %>% \r\n  head(500) %>% \r\n  paged_table()\r\n\r\n\r\n\r\n\r\n\r\nWe can do the same analysis but group the panels by theme to identify distinctive trigrams for each of the themes. This is where the long format of the panel data is needed. We won’t filter the trigrams for stop words here, as the tf-idf function will by itself get rid of phrases such as “as well as” or “this open panel.”\r\n\r\n\r\nShow code\r\n\r\npanels_full_long %>% \r\n  unnest_tokens(word, desc, token = \"ngrams\") %>% \r\n  group_by(theme) %>% \r\n  count(word, id) %>% \r\n  bind_tf_idf(word, id, n) %>%\r\n  select(theme, word, n, tf_idf) %>% \r\n  arrange(-tf_idf) %>% \r\n  head(500) %>% \r\n  paged_table()\r\n\r\n\r\n\r\n\r\n\r\nVisualizations\r\nTables are nice, but graphs are great too. Let’s create a network graph of the most common bigrams, using the igraph and ggraph packages.\r\n\r\n\r\nShow code\r\n\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\nbigram_count <- bigrams_filtered %>% count(word1, word2, sort = TRUE)\r\n\r\n\r\nbigram_graph <- bigram_count %>%\r\n  filter(n > 8) %>%\r\n  graph_from_data_frame()\r\n\r\na <- grid::arrow(type = \"closed\", length = unit(.10, \"inches\"))\r\n\r\nggraph(bigram_graph, layout = \"fr\") +\r\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,\r\n                 arrow = a, end_cap = circle(.05, 'inches')) +\r\n  geom_node_point(color = \"lightblue\", size = 5) +\r\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\r\n  theme_void()\r\n\r\n\r\n\r\n\r\nAnd let’s end on the OG of text visualizations: the word cloud (or in this case: the bigram cloud). To make it pretty, we filtered for a number of manually defined stop words such as “sts,” “panel,” or “university press.”\r\n\r\n\r\nShow code\r\n\r\nlibrary(wordcloud)\r\ncustom_stop_words <- c(\"panel\",\r\n                       \"panels\",\r\n                       \"sts\",\r\n                       \"paper\",\r\n                       \"papers\",\r\n                       \"mit\",\r\n                       \"press\")\r\ncustom_stop_bigrams <- c(\"university press\",\r\n                       \"conference theme\",\r\n                       \"durham duke\",\r\n                       \"duke university\",\r\n                       \"encourage papers\",\r\n                       \"science technology\",\r\n                       \"science studies\",\r\n                       \"social studies\",\r\n                       \"technology studies\")\r\n\r\nbigrams_filtered %>% \r\n  filter(!word1 %in% custom_stop_words) %>% \r\n  filter(!word2 %in% custom_stop_words) %>% \r\n  mutate(word = paste(word1, word2)) %>% \r\n  filter(!word %in% custom_stop_bigrams) %>% \r\n  count(word, sort = T) %>% \r\n  with(wordcloud(word, n, max.words = 25, scale = c(2.5, .5),random.order = FALSE, colors = brewer.pal(8,\"Dark2\")))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n“Tfidf.” 2021. https://en.wikipedia.org/w/index.php?title=Tf.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/scraping-4s/exploring_SSSS_panels_files/figure-html5/unnamed-chunk-23-1.png",
    "last_modified": "2021-04-10T15:32:26-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/madison-common-council-advisory-referendum-2021/",
    "title": "Madison Common Council Advisory Referendum 2021",
    "description": "How precincts voted about the questions whether Madison Alder should be a full-time job and how long terms should be",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-04-07",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nPrecincts with fewer than 25 total votes are not shown. Data: Dane County Elections\r\nShould Alder be a full-time job?\r\n\r\n\r\n\r\n\r\n\r\n\r\nShould terms for Alders be two or four years?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/madison-common-council-advisory-referendum-2021/screenshot_map.png",
    "last_modified": "2021-04-07T11:01:04-05:00",
    "input_file": {},
    "preview_width": 927,
    "preview_height": 569
  },
  {
    "path": "posts/dane-county-migration-flows/",
    "title": "Dane County Migration Flows",
    "description": "The `tidycensus` package got an exciting new function",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-04-02",
    "categories": [],
    "contents": "\r\nI’m a heavy user of the tidycensus package, and just yesterday I learned that its development version has a great new feature: You can now retrieve migration flows for counties and metro areas! Of course I needed to test this out, looking at Madison (or to be precise: Dane County). Our city and region have been steadily growing, and so where have people been moving from? And where do the people go who leave? It now only takes a few lines of code to find out.\r\n\r\nNote that you need the install the Github version of tidycensus for this to work. To do so, use remotes::install_github(\"https://github.com/walkerke/tidycensus/\").\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidycensus)\r\nlibrary(tidyverse)\r\nlibrary(mapdeck)\r\ndane_flows <- get_flows(\r\n  geography = \"county\",\r\n  state = \"WI\",\r\n  county = \"Dane\",\r\n  year = 2018, #this is the latest data available currently\r\n  geometry = TRUE\r\n  )\r\n\r\n\r\n\r\nWhere people are moving from\r\n\r\n\r\nShow code\r\n\r\ntop_move_in_table <- dane_flows %>% \r\n  filter(variable == \"MOVEDIN\") %>% \r\n  slice_max(n = 25, order_by = estimate) %>% \r\n  mutate(\r\n    width = estimate / 400,\r\n    tooltip = paste0(\r\n      scales::comma(estimate * 5, 1),\r\n      \" people moved from \", FULL2_NAME,\r\n      \" to \", FULL1_NAME, \" between 2014 and 2018\"\r\n      )\r\n    )\r\nrmarkdown::paged_table(top_move_in_table %>% select(FULL2_NAME, estimate))\r\n\r\n\r\n\r\n\r\n\r\nThe top county for in-migration is actually a continent: Asia. And another continent, Europe, also makes the top-10. Most of the counties people are coming from are in-state, and probably a good portion is students moving to attend UW-Madison. Cook County – in other words: Chicago – takes first spot for out-of-state mover, and Hennepin County (Minneapolis) follows closely. Farther away, there is Hamilton County in Ohio (Cincinatti), Middlesex County in Massachussetts, just outside Boston, and one West Coast county: San Diego, California.\r\nWe can map these flows, but for non-US origins/destinations, no geometry is provided. If we wanted to map these, we’d manually have to add these. For this article, we’ll filter them out.\r\n\r\n\r\nShow code\r\n\r\ntop_move_in_table %>% \r\n  filter(!is.na(GEOID2)) %>% \r\n  mapdeck(style = mapdeck_style(\"light\"), pitch = 45) %>% \r\n  add_arc(\r\n    origin = \"centroid1\",\r\n    destination = \"centroid2\",\r\n    stroke_width = \"width\",\r\n    auto_highlight = TRUE,\r\n    highlight_colour = \"#8c43facc\",\r\n    tooltip = \"tooltip\"\r\n  ) \r\n\r\n\r\n\r\n\r\nWhere people are moving to\r\n\r\n\r\nShow code\r\n\r\ntop_move_out_table <- dane_flows %>% \r\n  filter(variable == \"MOVEDOUT\") %>% \r\n  slice_max(n = 25, order_by = estimate) %>% \r\n  mutate(\r\n    width = estimate / 400,\r\n    tooltip = paste0(\r\n      scales::comma(estimate * 5, 1),\r\n      \" people moved from \", FULL1_NAME,\r\n      \" to \", FULL2_NAME, \" between 2014 and 2018\"\r\n      )\r\n    )\r\n\r\nrmarkdown::paged_table(top_move_out_table %>% select(FULL2_NAME, estimate))\r\n\r\n\r\n\r\n\r\n\r\nOne limitation of the data is that for out-migration (and consequently net migration), there is no data for people leaving the US – someone who has moved to South America won’t receive an American Community Survey. With that in mind, destinations are spread more widely. The West Coast draws heavily with Seattle, the Bay Area, Los Angeles, and San Diego. Austin (Texas), and of course New York City, and the head scratcher of Harrisburg (Pennsylvania). Closer by, the same places that show up for in-migration also feature here: Minneapolis, Chicago, Milwaukee, and various counties across Wisconsin.\r\n\r\n\r\nShow code\r\n\r\ntop_move_out_table %>% \r\n  filter(!is.na(GEOID2)) %>% \r\n  mapdeck(style = mapdeck_style(\"light\"), pitch = 45) %>% \r\n  add_arc(\r\n    origin = \"centroid1\",\r\n    destination = \"centroid2\",\r\n    stroke_width = \"width\",\r\n    auto_highlight = TRUE,\r\n    highlight_colour = \"#8c43facc\",\r\n    tooltip = \"tooltip\"\r\n  ) \r\n\r\n\r\n\r\n\r\nNet migration\r\nThe data also include a variable for net migration, MOVEDNET.\r\n\r\n\r\nShow code\r\n\r\ntop_move_net_table <- dane_flows %>% \r\n  filter(variable == \"MOVEDNET\") %>% \r\n  slice_max(n = 25, order_by = estimate) %>% \r\n  mutate(\r\n    width = estimate / 400,\r\n    tooltip = paste0(\r\n      scales::comma(estimate * 5, 1),\r\n      \" more people moved from \", FULL2_NAME,\r\n      \" to \", FULL1_NAME, \" between 2014 and 2018 than in the reverse direction\"\r\n      )\r\n    )\r\nrmarkdown::paged_table(top_move_net_table %>% select(FULL2_NAME, estimate))\r\n\r\n\r\n\r\n\r\n\r\nThe numbers here are smaller, but you can see that Waukesha, which was at the top of the total in-migration list also tops the net migration. That is, 1445 more people moved from Waukesha to Dane County than in the other direction. Overall, the numbers here are much smaller, and the maps looks a little different again.\r\n\r\n\r\nShow code\r\n\r\ntop_move_net_table %>% \r\n  filter(!is.na(GEOID2)) %>% \r\n  mapdeck(style = mapdeck_style(\"light\"), pitch = 45) %>% \r\n  add_arc(\r\n    origin = \"centroid1\",\r\n    destination = \"centroid2\",\r\n    stroke_width = \"width\",\r\n    auto_highlight = TRUE,\r\n    highlight_colour = \"#8c43facc\",\r\n    tooltip = \"tooltip\"\r\n  ) \r\n\r\n\r\n\r\n\r\nAge groups\r\nThe UW-Madison is probably a big driver of migration in Dane County, and we can look at this by grouping in-migration by age group.\r\n\r\nNote that this is 2011–2015 data, not the 2014–2018 data used in the rest of the article.\r\n\r\n\r\nShow code\r\n\r\nage_flows <- get_flows(\r\n  geography = \"county\",\r\n  county = \"Dane\",\r\n  state = \"WI\",\r\n  breakdown = \"AGE\",\r\n  breakdown_labels = TRUE,\r\n  year = 2015\r\n  )\r\n\r\n\r\nage_flows %>% \r\n  filter(variable == \"MOVEDIN\", AGE_label != \"All ages\") %>% \r\n  group_by(AGE_label, FULL2_NAME) %>% \r\n  summarize(estimate) %>% \r\n  slice_max(n = 5, order_by = estimate) %>% \r\n  arrange(AGE_label,-estimate) %>% \r\n  DT::datatable(rownames = FALSE)\r\n\r\n\r\n\r\n\r\nThese are the top-5 origins for each age group. If you sort by the estimate column, you see the largest in-migrant group are 20–24 year-olds from Asia, and overall the top groups are indeed heavily undergraduate- and graduate-student aged. Electronic health record company Epic, one of the largest employers in the country, is probably driving some of these numbers as wel. Sort by AGE_label to see the top origins for each age group.\r\n\r\n\r\n\r\n",
    "preview": "posts/dane-county-migration-flows/img/screenshot_flow_map.png",
    "last_modified": "2021-04-02T08:56:56-05:00",
    "input_file": {},
    "preview_width": 935,
    "preview_height": 576
  },
  {
    "path": "posts/raceethnicity-by-ward/",
    "title": "Race demographics by aldermanic district",
    "description": "What are the racial demographics of Madison's aldermanic districts",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-02-11",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe American Community Survey includes data on race and ethnicity. Madison’s aldermanic districts are comprised of census blocks, but for privacy reason, most ACS data is not available down to the block or even block group level. Data on race is available at the block group level, and we can approximate district boundaries based on this. Note that the maps below only display race, not ethnicity. Ethnicity data unfortunately is not available at the block group level. Data are 5-year estimates, covering 2015–19.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNote that each map has its own legend and the colors denote different percentages! Click on the aldermanic districts to see the percentages for each group.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-11T13:13:09-06:00",
    "input_file": {}
  },
  {
    "path": "posts/commuteflowsinmadison/",
    "title": "Visualizing commute flows in Wisconsin",
    "description": "LODES provides origin-destination data for connecting work and home. This article looks at patterns in those flows in Wisconsin, and especially in Madison",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIn previous posts I have worked with data from the US Census Bureau’s American Community Survey (ACS). ACS is great for data on people and households. For employment data, however, there is a better (or maybe complementary) source: The LEHD Origin-Destination Employment Statistics (LODES). The data can be accessed via a somewhat clunky web interface, On the Map or downloaded and then analyzed in your software of choice. One unique feature of LODES is the “OD” part in its name: It provides the number of people who commute from one census block to another. This can be helpful in transportation or general urban planning. For example, are there major employment centers that have a large share of its workers living in the same part of town, and could they therefore easily be served by transit?\r\nIn this post I will look at LODES data for Madison and the state of Wisconsin as a whole. The latest LODES data available is from 2018.\r\n\r\n\r\n\r\nData preparation\r\n\r\n\r\n\r\nThe file structure is explained here, page 6. There are two geography identifier variables, w_geocode and h_geocode, and a number of variables for the number of jobs for that geography pair. S000 is the one for total jobs.\r\n\r\n\r\n\r\nThe mapdeck arc visualization requires either two pairs of coordinates or sfc columns.\r\nI will get TIGER geographies with the tigris package, calculate centroids with sf, and then merge with the LODES data. Note that LODES uses the 2010 vintage for its geographies, whereas tigris defaults to the most recent vintage.\r\n\r\n\r\n\r\n\r\n\r\n\r\nLet’s look at the distribution of the number of jobs per origin-destination pair:\r\n\r\n\r\n\r\nThis does not look terribly promising–maybe block pairs are the wrong unit of analysis, with a huge number of them only having a single job-home pair. Aggregation to block groups or tracts may make more sense, but for now I’ll proceed with the blocks.\r\nCommute flows in Wisconsin\r\nThe number of block pairs is large, and so I’ll sample the 100 blocks with the largest number of commutes.\r\n\r\n\r\n\r\nNow we can do a double join with the block centroids: First, join on the work geocode, then do another join on the home geocode. This will create a new dataframe with two geometry columns. In order to work with the mapdeck package, the dataframe needs to be turned into an sf object. Finally, we add a thickness helper variable based on the number of jobs in the OD pair. This variable will specify the width of the arcs in the visualization.\r\n\r\n\r\n\r\nCensus blocks can be oddly shaped, and so it’s a good idea to plot the actual blocks and not just their centroids.\r\n\r\n\r\n\r\nVisualizing the flows\r\nNow we can visualize the top-100 commute flows in the state:\r\n\r\n\r\n\r\nWow, it is immediately obvious how dominant commute flows in the Madison area are! And by zooming in you can identify some other patterns:\r\nIn Madison two employers capture almost all commute flows: The UW-Madison downtown1 and electronic health record company Epic in suburban Verona. The only other employer making an appearance is the university hospital (which is a separate entity from the university)\r\nIn Milwaukee, by far the state’s largest city, there are only three OD pairs in total: Two point at the 32-story Northwestern Mutual Tower downtown; the third leads to the large suburban medical campus that includes the Medical College of Wisconsin, Froedtert Hospital, and the Childrens’ Hospital of Wisconsin.\r\nSheboygan is probably best known for plumbing product company Kohler, and the Kohler Co. headquarters make a prominent appearance on the map, with 8 arcs pointing at it.\r\nFor the remainder of OD pairs around the state, it’s primarily meat processing, healthcare facilities, and tribal facilities that appear as destinations.\r\nFor two OD pairs, home and work are actually in the same block: One is the UW in Platteville (presumably students living and working on campus), and a strangely empty looking wooded block near Hayward. My best guess is that it’s a farm or forestry business.\r\nMadison flows\r\nGiven the visual prominence of Madison on the map (and my personal interest in Madison), it makes sense to do a separate analysis just for flows within Dane County.\r\n\r\n\r\n\r\n\r\n\r\n\r\nWell, this doesn’t look all that different, does it? More arcs, but all of them still point at the UW, Epic, and the university hospital. This doesn’t change even if you look at the top-150 OD pairs. Only once you bump it up to 300, a few more work locations appear.\r\n\r\n[1] \"550250011021004\" \"550250108003008\" \"550250032001008\"\r\n[4] \"550250114022021\" \"550250114022035\" \"550250017042003\"\r\n\r\nPlotting 300 arcs of course is a bit of a mess:\r\n\r\n\r\n\r\nIf you scroll around enough you see the City-County Building, American Family Insurance, and a block with several employers near the Am Fam campus.\r\nAmerican Family Insurance and some other employment at the nothern edge of townSome observations:\r\nThere are about 20 contiguous blocks in the Bassett and Mifflin neighborhood that have a lot of people commuting to Epic. That area is well served by the 75 bus providing peak-hour service to and from Epic in 35 to 40 minutes\r\nThere are surprisingly few commute blocks on the east side. For Epic employees that makes sense, as traveling through the isthmus takes a good amount of time, but I would have expected more prominent UW flows\r\nThe “Epic blocks” downtownAggregating to block groups\r\nI mentioned at the beginning of the article that census blocks tend to be fairly small. What happens when we aggregate them into block groups?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWith the cut-off set to the 100 most frequent OD pairs, a few additional employment centers make an appearance, along the Beltline in Middleton. Other than that, there is not much insight to be gained. And especially along the edges of town, the block groups are quite large. If, for example, your goal is transit planning, using blocks is probably the better choice.\r\nConclusion\r\nThis is my first time extensively working with LODES origin-destination data and I have learned a lot in terms of data prep and visualization. In terms of actual insights, I am a bit disappointed. Everything being so centered on the UW and Epic leaves little opportunity to learn about other employment centers. But maybe the realization just how dominant these employers are in terms of commute flows is a valuable insight in itself. How, for example, could all those Epic employees on the west side of Madison and in Middleton be better served by transit? And why do Epic employees seemingly cluster more in where they live? The trope of “Epic luxury apartments” downtown is well known—and appears to have some truth to it.\r\n\r\nFor some reason, the university reports all(?) of its employees as working at a single location. In reality, UW employment is more spread out.↩︎\r\n",
    "preview": "posts/commuteflowsinmadison/images/preview.png",
    "last_modified": "2021-01-25T17:10:35-06:00",
    "input_file": {},
    "preview_width": 1225,
    "preview_height": 714
  },
  {
    "path": "posts/getting-to-work-in-madison/",
    "title": "Getting to work in Madison",
    "description": "Commute mode share by minority status and income",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2020-12-25",
    "categories": [],
    "contents": "\r\nHow do people get to work and back in Madison? And how does this differ for people of different economic status and racial and ethnic backgrounds? I had some idea about what this looked like at the national level, but I was always curious what the numbers would be for Madison. I have previously written about commute mode share in Madison over time, based on data from the American Community Survey (ACS). The ACS does have detailed information on race, ethnicity, and income, but what is publicly available on the ACS website doesn’t lend itself to this type of analysis. Fortunately, there is another data source: CTPP. This stands for Census Transportation Planning Products, “a State DOT-funded, cooperative program that produces special tabulations of American Community Survey (ACS) data that have enhanced value for transportation planning, analysis, and strategic direction.” So same data source, but organized in a different way. And the latest data available is for 2012–2016.\r\n\r\nI recommend reading my other article on bike mode share in Madison to get a sense of the limitations of ACS data. It only captures commute trips, and the margins of error can be sizable.\r\nCommuting and minority status\r\nThe way CTPP treats race and ethnicity is by putting people into two categories: White people who aren’t Latinx/Hispanic, and everyone else. There are all kinds of issues with splitting things up this way, but using a more fine-grained approach to race and ethnicity would lead to small groups in each category and less reliable data.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis is the same data but in a table.\r\n\r\n\r\nCommute mode\r\n\r\n\r\nNon-Hispanic White\r\n\r\n\r\nRacial/ethnic minority\r\n\r\n\r\nTotal\r\n\r\n\r\nDifference\r\n\r\n\r\nDrove alone\r\n\r\n\r\n65.3%\r\n\r\n\r\n55.8%\r\n\r\n\r\n63.4%\r\n\r\n\r\n9.5%\r\n\r\n\r\nBus\r\n\r\n\r\n7.7%\r\n\r\n\r\n15.4%\r\n\r\n\r\n9.3%\r\n\r\n\r\n-7.7%\r\n\r\n\r\nCarpooled\r\n\r\n\r\n6.5%\r\n\r\n\r\n11.7%\r\n\r\n\r\n7.6%\r\n\r\n\r\n-5.1%\r\n\r\n\r\nBiked\r\n\r\n\r\n5.7%\r\n\r\n\r\n3.1%\r\n\r\n\r\n5.2%\r\n\r\n\r\n2.6%\r\n\r\n\r\nWorked at home\r\n\r\n\r\n4.6%\r\n\r\n\r\n2.4%\r\n\r\n\r\n4.1%\r\n\r\n\r\n2.2%\r\n\r\n\r\nWalked\r\n\r\n\r\n9.3%\r\n\r\n\r\n10.3%\r\n\r\n\r\n9.5%\r\n\r\n\r\n-1.0%\r\n\r\n\r\nother\r\n\r\n\r\n0.8%\r\n\r\n\r\n1.4%\r\n\r\n\r\n1.0%\r\n\r\n\r\n-0.5%\r\n\r\n\r\nThe rightmost column shows the difference in percentage points between the two population groups (i.e. the distance between the two dots on the previous plot). That difference is largest for driving to work alone: The rate of driving alone is almost 10 percentage points higher for non-Hispanic White commuters. Depending on what question we’re trying to answer, it may be more useful to put this difference in the context of overall commute rates. Yes, there is a large difference in driving alone rates, but it also the overall most common commute mode. Compare that to bus commuters: The rate of bus commuting for people belonging to a racial or ethnic minority is twice as high as that for non-Hispanic White commuters, 15.4% versus 7.7%. With the difference in the reverse direction, the rate of bike commuters and people working from home is close to twice as high for non-Hispanic White workers. The only mode where rates are more or less the same for both groups is walking to work, at around 10 percent.\r\nIncome\r\nA different way to look at commute mode is by household income. Of course, income and race/ethnicity are not independent of each other. But especially with the coarse distinction between non-Hispanic White versus everyone else, looking at income can bring additional insights.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFor most commute modes, there is a clear trend across income groups: The more you make, the more likely you are to drive alone, and the less likely you are to walk or take the bus to work. It’s a little more complicated for other modes:\r\nBiking is most common for people in low-income households and then drops for with increasing income. However, at the top of the scale, in households with an income over $150,000, the trend reverses and more people bike to work.\r\nThe inverse is true for working from home: The rate is highest at the highest incomes and then drops with lower income. But for people in households making less than $15,000, it goes up again.\r\nFinally, for carpooling there doesn’t seem to be a clear trend. Between $15,000 and $100,000 household income, the rates are more or less the same. Very low income household don’t carpool much; and for some reason, incomes between $125,000 and $150,000 have the highest rate of carpooling.\r\n\r\n\r\n\r\n",
    "preview": "posts/getting-to-work-in-madison/article_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-12-25T15:25:33-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/bus-boardings-in-madison/",
    "title": "Bus boardings in Madison",
    "description": "Using the `mapdeck` package to visualize Metro Transit bus boardings in Madison (WI)",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2020-10-18",
    "categories": [],
    "contents": "\r\nMadison Metro publishes data boardings per stop on the City of Madison Open Data Portal. The data is also made available on what colloquially is known as the “red dot map” – which isn’t the most appealing visualization format:\r\nThe “red dot map.” Screenshot from City of Madison Open Data PortalHere I’m using the mapdeck package to create an interactive 3D map visualization of the data, augmented with bus routes and a population density heatmap.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBoardings only\r\nZoom in to see the data. Hover over the columns for more information. Right-click and drag to change the view angle and orientation.\r\n\r\n\r\n\r\nThe map really shows how much ridership is driven by the UW Campus and downtown. Also very visible are the four transfer points.\r\nAdd Metro routes\r\nI’m using the Metro GTFS data to add all routes to the map. You will notice that there are stops on the map that aren’t on any route. This is because the boardings data is from just before the pandemic started, whereas the routes are current and include the ones that have been cut.\r\n\r\n\r\n\r\n\r\n\r\n\r\nAdd population density\r\nDensity is one important condition for high transit ridership. So let’s add a heatmap with population density as a layer. Job density would be another interesting variable to look at, but I’ll leave that for another day.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/bus-boardings-in-madison/data/screenshot_red_dot_map.png",
    "last_modified": "2021-01-28T10:31:18-06:00",
    "input_file": {},
    "preview_width": 1917,
    "preview_height": 976
  },
  {
    "path": "posts/2020-03-05-transportation-funding-wisconsin/",
    "title": "Transportation funding in Wisconsin remains car centric",
    "description": "A look at the 2020 Multimodal Local Supplement Awards in Wisconsin to assess how multimodal they really are",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2020-03-05",
    "categories": [],
    "contents": "\r\nTony Evers, Wisconsin’s Democratic governor, announced $75 million of funding for local transportation projects yesterday. The title of the program, “Multimodal Local Supplement Awards,” may imply that the money supports projects that aren’t just road repairs and highway expansions. But of course it’s good to be skeptical and look at the actual project list.\r\nThe list is available as a PDF document and so the first step is to use Acrobat to convert it into an Excel sheet. (If there is an R package to directly read in PDFs, let me know.). The resulting XLSX file comes out pretty clean and doesn’t need much prep after reading it in with readxl.\r\n\r\n\r\n\r\nOne thing that needs to be fixed are some of the factors of the modal_type variable. I’ll combine all projects that don’t include anything related to pedestrians and bikes into one factor and all multimodal projects that do include a ped/bike component into a second factor; all other factors are kept as is. The fct_collapse function from the forcats package is great for this:\r\n\r\n\r\n\r\nNow all that remains is to create summary measures by modal_type:\r\n\r\n\r\n\r\nFor plotting it’s a good idea to reorder the bars by total amount with fct_reorder. And here’s the final product:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-03-05-transportation-funding-wisconsin/transportation-funding-wisconsin_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-12-25T18:16:30-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-06-25-some-furter-zoning-analyses-for-madison/",
    "title": "Some furter zoning analyses for Madison",
    "description": "A follow-up post on zoning and density in Madison and how it compares to other cities in the US.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2019-06-25",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nI got some good feedback after posting my article on Madison’s zoning restrictions on Facebook and Reddit, prompting some additional analyses.\r\nWhy do you look only at residentially zoned land?\r\nSeveral people pointed out that comparing the zoning districts that only allow detached single-family homes (SFR) to all residential zoning districts may be misleading. In many cities the densest parts are not in residential zoning districts but in downtown areas zoned for mixed use. I think that’s a fair point, but it was the original NYT article that chose only residential areas as the comparator, and I wanted to make Madison comparable to the other cities in the NYT analysis.\r\nNonetheless, I quickly ran the analysis comparing SFR to any district except “special districts.” These includes parks, conservancies, agricultural land, airports, or the UW-Campus—in other words districts that are highly unlikely to ever have residential use.\r\n\r\n\r\n\r\n\r\n\r\n#detached single-family only\r\nres_sfr <- data %>%\r\n  filter(ZONING_CODE %in% sfr_zones) %>%\r\n  summarize(sum(ShapeSTArea))\r\n# only special district area\r\nother_area <- data %>%\r\n  filter(ZONING_CODE %in% other) %>%\r\n  summarise(sum(ShapeSTArea))\r\ntotal <- data %>% summarize(sum(ShapeSTArea))\r\nres_sfr/(total-other_area)\r\n\r\n\r\n  sum(ShapeSTArea)\r\n1        0.4873358\r\n\r\nThe result? 49% of all land that reasonably could have residential units on it allows only single-family attached housing. Since there is no point of comparison to other cities, it’s hard to interpret that number.\r\nIs there a correlation between the percentage of SFR zoning and overall density?\r\nIt’s a good question: Does the metric the NYT used actually have an association with overall density of a city? (Note that even if there is a correlation, this absolutely doesn’t mean there is a causal connection! There are many, many factors that impact a city’s overall density.)\r\nThere are few data points to work with. I retrieved density information for all cities in the NYT article and Madison from Wikipedia:\r\n\r\n\r\nCity\r\n\r\n\r\nDensity (pop./sq.mi.)\r\n\r\n\r\nSingle-family (%)\r\n\r\n\r\nNew York\r\n\r\n\r\n27751\r\n\r\n\r\n15\r\n\r\n\r\nWashington\r\n\r\n\r\n11367\r\n\r\n\r\n36\r\n\r\n\r\nSeattle\r\n\r\n\r\n8642\r\n\r\n\r\n81\r\n\r\n\r\nLos Angeles\r\n\r\n\r\n8483\r\n\r\n\r\n75\r\n\r\n\r\nMinneapolis\r\n\r\n\r\n7821\r\n\r\n\r\n70\r\n\r\n\r\nSan Jose (CA)\r\n\r\n\r\n5776\r\n\r\n\r\n94\r\n\r\n\r\nPortland (OR)\r\n\r\n\r\n4504\r\n\r\n\r\n77\r\n\r\n\r\nArlington (TX)\r\n\r\n\r\n3810\r\n\r\n\r\n89\r\n\r\n\r\nMadison\r\n\r\n\r\n3233\r\n\r\n\r\n75\r\n\r\n\r\nSandy Springs (GA)\r\n\r\n\r\n2707\r\n\r\n\r\n85\r\n\r\n\r\nCharlotte (NC)\r\n\r\n\r\n2400\r\n\r\n\r\n84\r\n\r\n\r\nLet’s see what the overall correlation is:\r\n\r\n# A tibble: 1 x 1\r\n  `cor(density, sfr)`\r\n                <dbl>\r\n1              -0.885\r\n\r\nHm, -0.89. That’s a very, very high correlation. But it’s always good to look at your data points in a scatterplot to see what’s actually going on:\r\n\r\n\r\n\r\nOkay, New York is clearly way out there. Let’s exclude NYC and do the correlation again:\r\n\r\n\r\n dens_zon %>% \r\n   spread(Measure, Value) %>%\r\n   filter(City != \"New York\") %>%\r\n   summarize(cor(density, sfr))\r\n\r\n\r\n# A tibble: 1 x 1\r\n  `cor(density, sfr)`\r\n                <dbl>\r\n1              -0.693\r\n\r\nThat gets us a correlation of -0.69. Much lower, but still pretty high. Is DC also an outlier?\r\n\r\n\r\n dens_zon %>% \r\n   spread(Measure, Value) %>%\r\n   filter(City != \"New York\" & City != \"Washington\") %>%\r\n   summarize(cor(density, sfr))\r\n\r\n\r\n# A tibble: 1 x 1\r\n  `cor(density, sfr)`\r\n                <dbl>\r\n1              -0.366\r\n\r\nThis lower the correlation to -0.37. That is, there is a small negative association between how dense a city is and how much of its residential land is zoned exclusively for detached single-family homes. But more data is needed to confirm this.\r\n\r\n\r\n\r\n",
    "preview": "posts/2019-06-25-some-furter-zoning-analyses-for-madison/some-furter-zoning-analyses-for-madison_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2020-12-27T19:33:59-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/how-much-of-madison-allows-only-detached-single-family-housing/",
    "title": "How much of Madison allows only detached single-family housing?",
    "description": "Recreating a _New York Times_ analysis of how much of Madison's residential areas only allowed single-family detached homes",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2019-06-19",
    "categories": [],
    "contents": "\r\nBackground\r\nThe New York Times recently published an analaysis of how much land in several US cities only allows the construction of detached single-family houses. https://www.nytimes.com/interactive/2019/06/18/upshot/cities-across-america-question-single-family-zoning.html\r\nThe cities analyzed were\r\nCity\r\nProportion detached single-family to all housing\r\nNew York\r\n15%\r\nWashington\r\n36%\r\nMinneapolis\r\n70%\r\nLos Angeles\r\n75%\r\nPortland (OR)\r\n77%\r\nSeattle\r\n81%\r\nCharlotte (NC)\r\n84%\r\nSandy Springs (GA)\r\n85%\r\nArlington (TX)\r\n89%\r\nSan Jose (CA)\r\n94%\r\nI immediately wondered how my current hometown, Madison (WI), would compare.\r\nData sources\r\nMadison zoning data is available from the city’s Open Data portal: https://data-cityofmadison.opendata.arcgis.com/datasets/zoning-districts\r\nlicensed under the [City of Madison Data Policy}(https://www.cityofmadison.com/policy/data)\r\nalready contains the area for each polygon\r\n\r\nThe meaning of each zoning district is documented in MuniCode, specifically 28.032 and 28.033\r\nAnalysis\r\nI’ll work with R’s tidyverse package.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nLoad the data with the read_csv function.\r\n\r\n\r\ndata <- read_csv(\r\n  \"https://opendata.arcgis.com/datasets/4bfd0deffa8f4d5f8eefe868ab91493c_9.csv\",\r\n  col_types = \"iccccccdd\")\r\n\r\n\r\n\r\nDetermine what counts as residential\r\nThe New York Times analysis focused exclusively on residential districts, leaving out commercial or mixed use districts: “These maps highlight the land exclusively set aside for housing.” Ordinance 28.032 defines the following as residential districts: SR-C1, SR-C2, SR-C3, SR-V1, SR-V2, TR-C1, TR-C2, TR-C3, TR-C4, TR-V1, TR-V2, TR-U1, TR-U2, TR-R, TR-P\r\nI will therefore excldue all other Madison zoning codes from the analysis.\r\n\r\n\r\n#create variables for residential zoning and single-family zoning codes\r\nres_zones <- c(\"SR-C1\", \r\n                 \"SR-C2\", \r\n                 \"SR-C3\", \r\n                 \"SR-V1\",\r\n                 \"SR-V2\",\r\n                 \"TR-C1\",\r\n                 \"TR-C2\", \r\n                 \"TR-C3\",\r\n                 \"TR-C4\",\r\n                 \"TR-V1\",\r\n                 \"TR-V2\", \r\n                 \"TR-U1\", \r\n                 \"TR-U2\", \r\n                 \"TR-R\",\r\n                 \"TR-P\")\r\n\r\n\r\n\r\nDetermine what counts as “zoned for detached single-family homes”\r\nThe Times focuses on “codes devoted to detached single-family homes, grouping rowhouses more common in older East Coast cities like Washington and New York into a second category covering all other housing types.”\r\nOrdinance 28.033 lists the following building forms:\r\nTable of Residential Building FormsDetermining what falls under detached single-family homes appears straightforward: SR-C1, SR-C2, TR-C1, TR-C2, TR-C3, TR-R.\r\n\r\n\r\nsfr_zones <- c(\"SR-C1\", \r\n         \"SR-C2\",\r\n         \"TR-C1\", \r\n         \"TR-C2\", \r\n         \"TR-C3\", \r\n         \"TR-R\")\r\n\r\n\r\n\r\nNow all that remains is to sum up the areas for all single-family detached and divide by the total residential area.\r\n\r\n\r\nres_total <- data %>%\r\n  filter(ZONING_CODE %in% res_zones) %>%\r\n  summarize(sum(ShapeSTArea))\r\n\r\nres_sfr <- data %>%\r\n  filter(ZONING_CODE %in% sfr_zones) %>%\r\n  summarize(sum(ShapeSTArea))\r\n\r\nSFR_ratio <- round((res_sfr/res_total)*100, digits = 0)\r\n\r\n\r\n\r\nResults\r\nSo pretty much exactly 75 percent of all residentially zoned land in Madison allows only detached single-family housing. When I had first read the NYT article, my guess was Madison would be between somewhere 75 and 85 percent. I’m happy to see that the actual number is at the lower bounds of my guess, and fairly good compared to the cities mentioned in the original article:\r\nCity\r\nProportion single-family detached to all housing\r\nNew York\r\n15%\r\nWashington\r\n36%\r\nMinneapolis\r\n70%\r\nMadison (WI)\r\n75%\r\nLos Angeles\r\n75%\r\nPortland (OR)\r\n77%\r\nSeattle\r\n81%\r\nCharlotte (NC)\r\n84%\r\nSandy Springs (GA)\r\n85%\r\nArlington (TX)\r\n89%\r\nSan Jose (CA)\r\n94%\r\nBut of course it also means that a huge proportion of our residential lands is off limits for even medium density development.\r\nA map created in QGIS and styled to look roughly similar to the NYT maps helps visualize this:\r\nMap of all residential and single-family detached zoning in MadisonLimitations\r\nIn a Twitter thread, Christopher Schmidt pointed out that even cities that seemingly don’t limit most parts of the city to single-family homes, requirements such as lot sizes or building setbacks effectively still make it impossible to build anything but single-family homes.\r\n\r\n\r\n\r\n",
    "preview": "posts/how-much-of-madison-allows-only-detached-single-family-housing/images/residential-zoning-map-madison.png",
    "last_modified": "2020-12-25T18:00:11-06:00",
    "input_file": {},
    "preview_width": 3507,
    "preview_height": 2480
  }
]
