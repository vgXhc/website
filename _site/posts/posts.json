[
  {
    "path": "posts/do-speed-limit-reductions-reduce-speeds/",
    "title": "Do speed limit reductions reduce speeds?",
    "description": "Comparing pre-post data in Madison, Wisconsin",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2024-08-14",
    "categories": [
      "Vision Zero",
      "transportation",
      "Madison (WI)",
      "Vision Zero"
    ],
    "contents": "\nThe City of Madison adopted a Vision Zero policy in 2020. Its goal: “to eliminate all fatalities and severe injuries that occur as the result of traffic collisions on city streets by 2035.” One of many measures the city took to achieve this goal were speed limit reductions. Despite evidence from other cities (e.g. Hu and Cicchino 2020), many people questioned the effectiveness of speed limit reductions without changes to the physical environment. The city compiled pre- and post data on vehicle speeds to assess the effectiveness of the speed limit reductions. The data were published as a series of charts in a pdf document. I manually extracted the data from the document and combined them into a single chart.\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(colorspace)\n\nspeed <- read_csv(\"data/Vision_Zero_speed_limit_reduction_data.csv\")\n\nchart <- speed |> \n  mutate(inc_dec = if_else(pct_y_1 > pct_y_2, \"decrease\", \"increase\"),\n         inc_dec_pct = pct_y_2 - pct_y_1,\n         volume_y_2 = pct_y_2 * n_y_2) |> \n  ggplot(aes(pct_y_1, pct_y_2, color = inc_dec_pct, size = volume_y_2)) +\n  geom_point() +\n  scale_color_continuous_divergingx(name = \"% pt decrease/increase\", palette = 'RdBu', mid = 0) +\n  # scale_color_distiller(name = \"% pt decrease/increase\", type = \"div\", palette = \"BrBG\") +\n  scale_size(name = \"Traffic volume post\") +\n  geom_abline(slope = 1, intercept = 0, linetype = 3) +\n  labs(x = \"Pre reduction\",\n       y = \"Post reduction\",\n       title = \"Pre-post comparison of speed limit reductions in Madison\",\n       subtitle = \"% of vehicles 5+ mph over initial speed limit\",\n       caption = \"Data: City of Madison\\nData extraction and visualization: Harald Kliems\") +\n  xlim(0,45) +\n  ylim(0,45) +\n    theme_minimal()\n\nchart\n\n\n\nThe chart shows that in a large majority of locations, the percentage of people going 5 mph or more over the initial speed limit dropped after the speed limit reduction. In some locations that drop was large (up to 15 percentage points), while in others the changes were more moderate. A few locations saw increases in the percentage of speeding vehicles.\nA pre-post comparison cannot establish a causal link between speed limit reductions and reductions in speeds. But given that speed limit reductions are easy and cheap, this data appears to support the city’s decision to reduce speed limits.\n\n\n\n",
    "preview": "posts/do-speed-limit-reductions-reduce-speeds/speed_limits_pre_post_madison_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2024-08-14T07:53:03-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-07-01-bicycle-network-analysis-and-bicycle-commute-mode-share/",
    "title": "Bicycle Network Analysis and Bicycle Commute Mode Share",
    "description": "Do better bike networks lead to more bike commuters?",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2024-07-01",
    "categories": [
      "biking",
      "transportation",
      "American Community Survey"
    ],
    "contents": "\r\nPeople for Bikes just published the city rankings from their Bicycle Network Analysis (BNA). Like every year, people will complain about their city scoring too high or too low, or they’ll identify problems with the methodology of the rating. One criticism I have encountered is that the BNA doesn’t take into account actual bike ridership.\r\nThere are many good reasons for that, but I was curious: How does a city’s BNA score compare to their bicycle commute mode share? People for Bikes are very transparent about their methods and data, and so this was an easy question to answer. Just download the spreadsheet with the BNA scores, download bicycle commute data from the American Community Survey and plot them against each other.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidycensus)\r\nlibrary(hrbrthemes)\r\nextrafont::loadfonts()\r\n\r\n# # AcS data for commuting\r\n# places_bike_commute <- get_acs(geography = \"place\", table = \"S0801\", year = 2022)\r\n# \r\n# saveRDS(places_bike_commute,\"places_bike_commute.RDS\")\r\nplaces_bike_commute <- readRDS(file = \"data/places_bike_commute.RDS\")\r\n\r\n# BNA ratings\r\nbna <- read_csv(\"data/city-ratings-v24.4.csv\")\r\n\r\n\r\n# data cleaning\r\nbna_plot <- places_bike_commute %>% \r\n  mutate(GEOID = as.numeric(GEOID)) %>% \r\n  filter(variable == \"S0801_C01_011\") %>% \r\n  filter(estimate > moe) %>% \r\n  inner_join(bna, by = join_by(GEOID == census_fips_code)) %>% \r\n  select(NAME, city, census_population, estimate, moe, bna_overall_score, pop_size, state) %>% \r\n  mutate(pop_size = case_match(pop_size, \"large\" ~ \"large (>300k population)\",\r\n                               \"medium\" ~ \"medium (50k-300k population)\",\r\n                               \"small\" ~ \"small (<50k population)\"),\r\n         name_label = paste0(city, \", \", state),\r\n         NAME = str_remove(NAME, \" city\"))\r\n  \r\n\r\n# plot\r\n\r\nbna_plot %>% \r\nggplot(aes(bna_overall_score, estimate), group = pop_size) +\r\n  geom_point(aes(color = pop_size), alpha = .5) +\r\n  geom_smooth(method = \"lm\") +\r\n  labs(x = \"BNA score\", y = \"Bike commute share (%)\",\r\n       title = \"BNA Score and Bike Commute Mode Share\",\r\n       caption = \"Data: People for Bikes; American Community Survey 5-year estimates 2018-2022\\nPlaces where the mode share estimate is smaller than the margin of error were excluded\\nVisualization: Harald Kliems\") +\r\n  theme_ipsum_rc() +\r\n  ggrepel::geom_label_repel(data = bna_plot %>% filter(name_label %in% c(\"Madison, WI\", \"Davis, CA\", \"Portland, OR\", \"Leadville, CO\", \"Mackinac Island, MI\", \"Chicago, IL\", \"Milwaukee, WI\", \"Mount Hope, OH\", \"Washburn, WI\")), aes(label = name_label)) +\r\n  geom_point(data = bna_plot %>% filter(name_label %in% c(\"Madison, WI\", \"Davis, CA\", \"Portland, OR\", \"Leadville, CO\", \"Mackinac Island, MI\", \"Chicago, IL\", \"Milwaukee, WI\", \"Mount Hope, OH\", \"Washburn, WI\")), color = \"red\") +\r\n  facet_wrap(~ pop_size, scales = \"free_y\") +\r\n  scale_color_ipsum() +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nNote the different y-axis scales for each city size category. Highlighted are a few cities:\r\nChicago: As Streetsblog Chicago keeps saying over and over again: The BNA method leads to an awfully low score, and yet, lots of people bike in Chicago\r\nMilwaukee: On the other hand, Milwaukee made the top 10 of large cities in terms of BNA score; and yet, the bike commute numbers are much lower than one may expect\r\nPortland tops the bike mode share and also scores high on the BNA score\r\nMedium sized cities have more variability in bike commute mode share. Madison seems to perform in line with its BNA score; outliers like Davis excel in both mode share and BNA score.\r\nFor small cities, things are more chaotic. The bike commute mode share is probably much less reliable, and so we get weird results like Mount Hope, a tiny unincorporated community in Ohio, where supposedly almost 60% of its residents bike to work!1 Washburn, “the little town on the big lake,” had the highest BNA score in all of Wisconsin, and yet few people bike to work. Whereas Leadville in Colorado has a very low BNA score but a large bike commute mode share. And then of course there’s Mackinac Island, the car-free bicycle paradise in Michigan.\r\nOverall it appears that BNA score and bike commute mode share are positively correlated. We can’t draw any causal conclusions from this, and the plot shows that there is a lot of variability.\r\n\r\nThere appears to be a large Amish population in the area, which would explain the high bike mode share.↩︎\r\n",
    "preview": "posts/2024-07-01-bicycle-network-analysis-and-bicycle-commute-mode-share/bicycle-network-analysis-and-bicycle-commute-mode-share_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2024-07-01T18:08:17-05:00",
    "input_file": "bicycle-network-analysis-and-bicycle-commute-mode-share.knit.md",
    "preview_width": 2016,
    "preview_height": 1152
  },
  {
    "path": "posts/pedestrian-fatalities-soar-just-after-sunset/",
    "title": "After the sun sets, pedestrian fatalities spike",
    "description": "Pedestrian fatality data from 2001-2021 and how the time of fatal crashes varies over the day",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      },
      {
        "name": "Austin Griesbach",
        "url": {}
      }
    ],
    "date": "2024-03-05",
    "categories": [
      "Vision Zero",
      "walking",
      "transportation",
      "pedestrian fatalities",
      "FARS"
    ],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(haven)\r\nlibrary(sf)\r\nlibrary(lutz)\r\nlibrary(suncalc)\r\nlibrary(extrafont)\r\nlibrary(hrbrthemes)\r\nlibrary(gt)\r\noptions(timeout = 300) # files are large...\r\n\r\n\r\nThere is a pedestrian fatality crisis in the United States After years of progress, the number of people killed while walking started rising again around 2010, contrary to what’s happening outside of the US.\r\nA recent article in the New York Times took a closer look at the fatality data and made a convincing case that the crisis specifically is a crisis happening at night.\r\nThis chart shows the peak of pedestrian fatalities shifting over the course of the year, presumably corresponding with when the sun rises and sets.\r\nThis chart shows the deadliest time of day for pedestrians, averaging data from 2000 to 2021 over a whole calendar year. Image: New York Times.It occurred to me and my fellow data nerd Austin Griesbach that we can explicitly look at sunrise and sunset times and how they correlate with the time of a crash.\r\nThis turned out to be a more complicated process than expected and I document my part of it at the end of the article, in the Methods section.\r\nMost people probably won’t care about that though, and so let’s dive into our findings.\r\nThe hour after sunset is the worst time for pedestrian deaths\r\n\r\n\r\nShow code\r\n\r\n# load cleaned data. Data cleaning is documented in Methods section at the end of this article\r\npeds <- readRDS(\"data/peds_with_sunset.rds\")\r\npeds <- peds |>\r\n  st_drop_geometry()\r\n\r\npeds |>\r\n  filter(YEAR > 2000) |>\r\n  filter(abs(as.numeric(time_from_sunset_min, units = \"hours\")) < 6.125) |>\r\n  ggplot(aes(as.numeric(time_from_sunset_min, units = \"hours\"))) +\r\n  geom_histogram(binwidth = 1 / 4) +\r\n  geom_vline(xintercept = 0, color = \"red\", alpha = 0.3, linewidth = 1) +\r\n  # facet_wrap(~YEAR) +\r\n  ylab(\"Pedestrian fatalities\") +\r\n  xlab(\"Hours from sunset\") +\r\n  labs(\r\n    title = \"Crashes are highly concentrated right after sunset\",\r\n    subtitle = \"Fatal pedestrian crashes in the US, 2001-2021; 15-minute bins\",\r\n    caption = \"Data: Fatality Analysis Reporting System (FARS)\\nVisualization: Harald Kliems\"\r\n  ) +\r\n  theme_ipsum(base_family = \"Roboto Condensed\") +\r\n  annotate(geom = \"text\", x = .5, y = 5400, label = \"Peak about 20-50\\n minutes after sunset\", family = \"Roboto Condensed\") +\r\n  annotate(\"segment\",\r\n    x = 22.5 / 60, xend = 52.5 / 60, y = 4950, yend = 4950,\r\n    arrow = arrow(ends = \"both\", angle = 90, length = unit(.2, \"cm\"))\r\n  ) +\r\n  annotate(geom = \"text\", x = -2.8, y = 2000, label = \"No marked increase right\\nbefore/after sunset\", hjust = 0, family = \"Roboto Condensed\") +\r\n  annotate(geom = \"text\", x = 3.3, y = 3000, label = \"Gradual decline for several           \\nhours after sunset\", family = \"Roboto Condensed\") +\r\n  geom_curve(x = -1, xend = -.05, y = 2000, yend = 800, curvature = -.3, arrow = arrow(type = \"closed\", length = unit(.2, \"cm\"))) +\r\n  geom_curve(x = 1.5, xend = 4.5, y = 3900, yend = 1690, arrow = arrow(type = \"closed\", length = unit(.2, \"cm\")), curvature = .2)\r\n\r\n\r\n\r\nThis chart is stunning: The number of fatal pedestrian crashes is fairly flat for several hours before sunset.\r\nCrashes don’t start going up right before sunset or in the first few minutes after, when the sun is low and with news reports frequently mentioning glare as a factor contributing to crashes.\r\nBut within less than an hour of sunset, pedestrian fatalities spike.\r\nAnd they spike a lot.\r\nIn the 15 minutes right before/after sunset, 683 pedestrians were killed.\r\n45 minutes later, that number is 4802, more than 7 times as many!\r\nThe spike is so pronounced that it looks like an error.\r\nBut we checked and re-checked the data and could not find any errors in the analysis (see also below for some data checks).\r\nAfter the peak within the first hour over sunset, the number of crashes declines.\r\nThe decline is gradual, and the number of crashes remains much higher than before sunset.\r\nIs there a similar pattern around sunrise?\r\nWhat does the pattern look like for sunrise?\r\nIt is largely a less dramatic version of the sunset chart.\r\n\r\n\r\nShow code\r\n\r\npeds |>\r\n  filter(YEAR > 2000) |>\r\n  filter(abs(as.numeric(time_from_sunrise_min, units = \"hours\")) < 6.125) |>\r\n  ggplot(aes(as.numeric(time_from_sunrise_min, units = \"hours\"))) +\r\n  geom_histogram(binwidth = 1 / 4) +\r\n  geom_vline(xintercept = 0, color = \"red\", alpha = 0.3, linewidth = 1) +\r\n  ylab(\"Pedestrian fatalities\") +\r\n  xlab(\"Hours from sunset\") +\r\n  ylim(c(0, 5500)) +\r\n  labs(\r\n    title = \"Pedestrian fatalities have a less pronounced peak before sunrise\",\r\n    subtitle = \"Fatal pedestrian crashes in the US, 2001-2021; 15-minute bins\",\r\n    caption = \"Data: Fatality Analysis Reporting System (FARS)\\nVisualization: Harald Kliems\"\r\n  ) +\r\n  theme_ipsum(base_family = \"Roboto Condensed\") +\r\n  annotate(geom = \"text\", x = -.5, y = 2450, label = \"Peak about 20-50\\n minutes before sunrise\", family = \"Roboto Condensed\") +\r\n  annotate(\"segment\",\r\n    x = -22.5 / 60, xend = -52.5 / 60, y = 1980, yend = 1980,\r\n    arrow = arrow(ends = \"both\", angle = 90, length = unit(.2, \"cm\"))\r\n  ) +\r\n  annotate(geom = \"text\", x = 1.8, y = 2000, label = \"Local minimum\\naround sunrise\", family = \"Roboto Condensed\") +\r\n  annotate(geom = \"text\", x = -3.1, y = 1600, label = \"Gradual increase starting\\n ~2 hours before sunrise\", family = \"Roboto Condensed\") +\r\n  geom_curve(x = 1, xend = .05, y = 2000, yend = 800, curvature = .3, arrow = arrow(type = \"closed\", length = unit(.2, \"cm\"))) +\r\n  geom_curve(x = -2.5, xend = -1.1, y = 820, yend = 1700, arrow = arrow(type = \"closed\", length = unit(.2, \"cm\")), curvature = .2)\r\n\r\n\r\n\r\nPedestrian fatalities start increasing about 3 hours before sunset, peak 20-50 minutes before sunset, and rapidly decline right before and after sunset, and then stay relatively flat.\r\nThe peak 15-minutes has fewer than 1800 fatalities, and that 1800 is only 4 times as many as in the low period around sunrise.\r\nA different way to look at the data\r\nRather than using bar charts to show the crashes in relation to sunset and time, Austin put all the data together and added it to an animated map.\r\nAn animated chart showing a map of the US and pedestrian fatalities. The chart progresses through day/night cycles over the course of a year. The spike of fatalities after sunset is clearly visible.An animated chart showing a map of the US and pedestrian fatalities. The chart progresses through day/night cycles over the course of a year. The spike of fatalities after sunset is clearly visible.Some data checks, and searching for explanations\r\nAs we said, the charts are stunning.\r\nAustin and I couldn’t really believe what we were seeing.\r\nAnd when I posted an early version of the chart to Mastodon, many readers suspected an error.\r\nWe did many robustness checks (including independently cleaning and analyzing the data in Python and R), and at this point we’re pretty confident that the data are correct.\r\nAs to an explanation of why we’re seeing these patterns, we’re mostly stumped.\r\nSure, as the New York Times wrote, there are a lot of possible explanations of why more pedestrians in the US die at night.\r\nBut that those deaths are so heavily concentrated right after sunset (and, to a lesser extent: right before sunrise) is difficult to explain.\r\nBreaking down the data by additional variables may help support or rule out possible explanations.\r\nWhat about differentiating the data by state?\r\nHere’s a our sunrise chart from above, but faceted by state:\r\n\r\n\r\nShow code\r\n\r\nstate_fips <- tigris::fips_codes |>\r\n  distinct(state_code, .keep_all = T) |>\r\n  mutate(state_code = as.numeric(state_code)) |>\r\n  select(state_code, state)\r\n\r\npeds_state_facets <- peds |>\r\n  ungroup() |>\r\n  left_join(state_fips, by = join_by(STATE == state_code)) |>\r\n  filter(abs(as.numeric(time_from_sunset_min, units = \"hours\")) < 6.125) |>\r\n  mutate(states_binned = fct_lump(state, n = 11))\r\n\r\npeds_state_facets |>\r\n  ggplot(aes(as.numeric(time_from_sunset_min, units = \"hours\"))) +\r\n  geom_histogram(binwidth = 1 / 4) +\r\n  geom_vline(xintercept = 0, color = \"red\", alpha = 0.3, linewidth = 1) +\r\n  facet_wrap(~states_binned, scales = \"free_y\") +\r\n  ylab(\"Pedestrian fatalities\") +\r\n  xlab(\"Hours from sunset\") +\r\n  labs(\r\n    title = \"Pedestrian fatalities around sunset have a similar pattern across US States\",\r\n    subtitle = \"Fatal pedestrian crashes in the US, 2001-2021; 15-minute bins\",\r\n    caption = \"Data: Fatality Analysis Reporting System (FARS)\\nVisualization: Harald Kliems\"\r\n  ) +\r\n  theme_ipsum(base_family = \"Roboto Condensed\") +\r\n  theme(\r\n    panel.grid.major = element_blank(),\r\n    panel.grid.minor = element_blank()\r\n  )\r\n\r\n\r\n\r\nThe pattern does not differ much by state.\r\nNote that the y-axis for each state differs in order to better show the pattern; the overall number of pedestrian fatalities vary a lot by state.\r\nAnother test is to look at the pattern by month.\r\n\r\n\r\nShow code\r\n\r\npeds |>\r\n  mutate(label_month = month(MONTH, label = TRUE)) |>\r\n  filter(abs(as.numeric(time_from_sunset_min, units = \"hours\")) < 6.125) |>\r\n  ggplot(aes(as.numeric(time_from_sunset_min, units = \"hours\"))) +\r\n  geom_histogram(binwidth = 1 / 4) +\r\n  geom_vline(xintercept = 0, color = \"red\", alpha = 0.3, linewidth = .6) +\r\n  facet_wrap(~label_month) +\r\n  ylab(\"Pedestrian fatalities\") +\r\n  xlab(\"Hours from sunset\") +\r\n  labs(\r\n    title = \"Pedestrian fatalities around sunset have a similar pattern throughout the year\",\r\n    subtitle = \"Fatal pedestrian crashes in the US, 2001-2021; 15-minute bins\",\r\n    caption = \"Data: Fatality Analysis Reporting System (FARS)\\nVisualization: Harald Kliems\"\r\n  ) +\r\n  theme_ipsum(base_family = \"Roboto Condensed\") +\r\n  theme(\r\n    panel.grid.major = element_blank(),\r\n    panel.grid.minor = element_blank()\r\n  )\r\n\r\n\r\n\r\nAgain, the overall pattern stays intact.\r\nFrom about October to January, the post-sunset peaks appear to be more pronounced, as is the overall number of pedestrian fatalities within 6 hours of sunset.\r\nThis is easier to see on a bar plot, and it probably has something to do with pedestrian and vehicle volumes and when sunset happens: Earlier sunsets mean that the “after” period encompasses rush hour; whereas late sunsets in the summer occur after peak traffic.\r\n\r\n\r\nShow code\r\n\r\ntable_month <- peds |>\r\n  mutate(label_month = month(MONTH, label = TRUE)) |>\r\n  filter(abs(as.numeric(time_from_sunset_min, units = \"hours\")) < 6) |>\r\n  mutate(night = if_else(time_from_sunset_min <= 0, \"before\", \"after\")) |>\r\n  group_by(label_month, night) |>\r\n  count()\r\ntable_month |>\r\n  ggplot(aes(label_month, n, fill = fct_relevel(night, \"before\", \"after\"))) +\r\n  geom_col(position = position_dodge()) +\r\n  labs(\r\n    fill = \"Before/after sunset\", title = \"Fatal pedestrian crashes before and after sunset\",\r\n    subtitle = \"Crashes within 6 hours of sunset\",\r\n    caption = \"Data: Fatality Analysis Reporting System (FARS)\\nVisualization: Harald Kliems\"\r\n  ) +\r\n  xlab(\"Month\") +\r\n  ylab(\"Pedestrian fatalities\") +\r\n  scale_fill_ipsum() +\r\n  theme_ipsum_rc()\r\n\r\n\r\n\r\nFinally, we can check if this pattern is new, by separating the data into 5-year periods (which results in not including 2021 data):\r\n\r\n\r\nShow code\r\n\r\npeds |>\r\n  mutate(year_period = case_match(\r\n    YEAR,\r\n    c(2001:2005) ~ \"2001-2005\",\r\n    c(2006:2010) ~ \"2006-2010\",\r\n    c(2011:2015) ~ \"2011-2015\",\r\n    c(2016:2020) ~ \"2016-2020\"\r\n  )) |>\r\n  filter(!is.na(year_period)) |>\r\n  filter(abs(as.numeric(time_from_sunset_min, units = \"hours\")) < 6) |>\r\n  ggplot(aes(as.numeric(time_from_sunset_min, units = \"hours\"))) +\r\n  geom_histogram(binwidth = 1 / 4) +\r\n  geom_vline(xintercept = 0, color = \"red\", alpha = 0.3, linewidth = .6) +\r\n  facet_wrap(~year_period) +\r\n  ylab(\"Pedestrian fatalities\") +\r\n  xlab(\"Hours from sunset\") +\r\n  labs(\r\n    title = \"The pattern of pedestrian fatalities around sunset has not changed much from 2001 to 2020\",\r\n    subtitle = \"Fatal pedestrian crashes in the US, 2001-2020; 15-minute bins\",\r\n    caption = \"Data: Fatality Analysis Reporting System (FARS)\\nVisualization: Harald Kliems\"\r\n  ) +\r\n  theme_ipsum(base_family = \"Roboto Condensed\") +\r\n  theme(\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.minor.x = element_blank()\r\n  )\r\n\r\n\r\n\r\nOnce more, the pattern remains intact.\r\nBut we can see the increase in nighttime pedestrian fatalities in more recent years.\r\nWhat about other countries?\r\nIs what we’re seeing a uniquely US phenomenon?\r\nOne country for which we found reasonably good data is Germany.\r\nUnfortunately, the crash time is made available only down to the hour, making it impossible to do a 1:1 comparison in relation to the sunrise/sunset times.\r\nAdditionally, only data from 2016-2022 is available.\r\nHere’s the hourly distribution fatalities to see how German and US pedestrian fatalities compare.\r\n\r\n\r\nShow code\r\n\r\ncrashes_de_us <- readRDS(\"data/crashes_de_us.RDS\")\r\n\r\ncrashes_de_us |>\r\n  group_by(country) |>\r\n  tally() |>\r\n  mutate(population = case_when(\r\n    country == \"Germany\" ~ 83191000,\r\n    country == \"US\" ~ 331449281\r\n  )) |>\r\n  rowwise() |>\r\n  mutate(fatal_per_million = n / population * 1000000) |>\r\n  gt() |>\r\n  cols_label(country = \"\", n = \"Fatalities/fatal crashes\", population = \"Population (2020)\", fatal_per_million = \"Fatalities/fatal crashes per 1 million\") |>\r\n  fmt_number(fatal_per_million, decimals = 1) |>\r\n  fmt_number(c(n, population), use_seps = TRUE, decimals = 0) |> \r\n  tab_header(title = \"Pedestrian fatalities in the US and in Germany, 2016-2021\") |>\r\n  tab_source_note(source_note = md(\"Fatality Analysis Reporing System (FARS); Unfallatlas; Wikipedia <br> Data for Germany is the number of fatal _crashes_ involving a pedestrian; US data is number _fatalities_ for pedestrians)\"))\r\n\r\n\r\nPedestrian fatalities in the US and in Germany, 2016-2021\r\n    \r\n      Fatalities/fatal crashes\r\n      Population (2020)\r\n      Fatalities/fatal crashes per 1 million\r\n    Germany\r\n1,833\r\n83,191,000\r\n22.0US\r\n38,397\r\n331,449,281\r\n115.8Fatality Analysis Reporing System (FARS); Unfallatlas; Wikipedia  Data for Germany is the number of fatal crashes involving a pedestrian; US data is number fatalities for pedestrians)\r\n    \r\n\r\nThe overall number of fatal pedestrian crashes in the Germany is, unsurprisingly, a lot lower than in the US.\r\nThis is true both for absolute as population-adjusted numbers.\r\nThe US population in 2020 was 331 million; Germany’s was 83 million.\r\nFrom 2016 to 2021, Germany had 22 fatal pedestrian crashes per 1 million population; the US had 116 pedestrian fatalities per 1 million population.\r\nBut when do those fatal crashes happen, though?\r\nHere is the chart comparing the two countries.\r\n\r\n\r\nShow code\r\n\r\ncrashes_de_us |>\r\n  group_by(country, crash_hour) |>\r\n  tally() |>\r\n  ggplot(aes(x = crash_hour, n, fill = country)) +\r\n  geom_col() +\r\n  theme_minimal() +\r\n  facet_wrap(~country, scales = \"free_y\") +\r\n  theme_ipsum_rc() +\r\n  scale_fill_ipsum() +\r\n  theme(\r\n    legend.position = \"none\",\r\n    plot.caption = ggtext::element_textbox_simple(halign = 0)\r\n  ) +\r\n  labs(\r\n    title = \"The pattern of fatal pedestrian crashes by time of day differs between the US and Germany\",\r\n    fill = element_blank(),\r\n    subtitle = \"2016-2021 crash data\",\r\n    caption = \"Data: Fatality Analysis Reporing System (FARS); Unfallatlas <br> Data for Germany: number of fatal _crashes_ involving a pedestrian; US data: number of _fatalities_ for pedestrians<br>Visualization: Harald Kliems\",\r\n    x = \"Time of day\",\r\n    y = \"Number of crashes/fatalities\"\r\n  ) +\r\n  scale_x_continuous(breaks = c(0, 6, 12, 18))\r\n\r\n\r\n\r\nThe distribution across the day definitely looks different.\r\nWhile both charts have two peaks, the first peak is much later in Germany, around 10 AM; the second peak is both earlier and narrower in Germany, centered around evening rush hour.\r\nConclusion: What is going on??\r\nAustin and I split the data in many other ways to try and elucidate what is going on, and to confirm that we weren’t looking at weird artefacts or straight up errors in our code or in the data.\r\nAt this point were certain enough that the data are correct to publish this post.\r\nIf you find any errors in the code, please let us know!\r\nAs to why were seeing what we’re seeing: We’re still stumped.\r\nWhy such a rapid increase after (but not before!) sunset?\r\nWhy is the pattern different in the US than in Germany?\r\nWe can’t say.\r\nAdditional data would be needed to make progress on an explanation.\r\nFirst and foremost, one would want pedestrian and motor vehicle volume data by time of day, across the year, and for different locations in the US.\r\nSome of that data may exist in the proprietary databases of companies like Streetlight; some of it could be extrapolated from automated traffic counters.\r\nBut there are going to be many limitations and gaps, especially for pedestrian data and in non-urban areas.\r\nIt may also help to look at other crash types, such as fatal crashes not involving pedestrians; fatal bike crashes; or non-fatal injury crashes.\r\nThe FARS data only contains fatalities, and while some states make non-fatal crash data available, there are issues with the comparability of the data over time and between states.\r\nAnd a lot of data cleaning.\r\nFor now, we’ll leave it to the readers to come up with explanations, based on data or speculation.\r\nAnd maybe publish a follow-up post at a later point.\r\nMethods\r\nThe following contains and describes the code needed to prepare the data for the plots and tables above.\r\nWe want our data to be reproducible and open to checking by others.\r\nDownload FARS data\r\nComplete FARS is available as SAS and csv files for download.\r\nThe files are large and downloads regularly time out.\r\nThe structure of the folders and files has slight variations between years and therefore reading in the files requires some effort and multiple functions.\r\n\r\n\r\nShow code\r\n\r\nget_fars <- function(year) {\r\n  fars_url <- paste0(\r\n    \"https://static.nhtsa.gov/nhtsa/downloads/FARS/\",\r\n    year,\r\n    \"/National/FARS\",\r\n    year,\r\n    \"NationalSAS.zip\"\r\n  )\r\n  temp_file <- tempfile()\r\n  download.file(fars_url, destfile = temp_file)\r\n  unzip(temp_file, exdir = paste0(\"data/\", year), overwrite = TRUE)\r\n}\r\n\r\nwalk(2002:2007, safely(get_fars)) # some years will time out and you'll have to try again\r\n\r\n\r\nNow that we have the data downloaded, we read in two tables, the PERSON and the ACCIDENT files.\r\nUnfortunately the structure of those files varies over time and so joining them isn’t that straightforward.\r\nWe define two functions to read the files.\r\n\r\n\r\nShow code\r\n\r\n# folder structure is inconsistent between years and we need two functions\r\n# to read in the data\r\nread_FARS_subfolder <- function(year, table_type) {\r\n  read_sas(paste0(\"data/\", year, \"/FARS\", year, \"NationalSAS/\", table_type, \".sas7bdat\"))\r\n}\r\n\r\nread_FARS_no_subfolder <- function(year, table_type) {\r\n  read_sas(paste0(\"data/\", year, \"/\", table_type, \".sas7bdat\"))\r\n}\r\n\r\n\r\nThe variable in which the latitude and longitude are being saved and their format varies between years.\r\nThis requires cleanup.\r\n2000 crash data is the most different.\r\n\r\n\r\nShow code\r\n\r\naccident_00 <- read_FARS_no_subfolder(2000, \"accident\")\r\naccident_00 <- accident_00 |>\r\n  mutate(\r\n    LATITUDE = na_if(LATITUDE, \"88888888\"), # remove missing\r\n    LATITUDE = na_if(LATITUDE, \"99999999\"), # remove missing\r\n    LATITUDE = paste0(\r\n      \"-\",\r\n      str_sub(LATITUDE, start = 0, end = 2),\r\n      \".\",\r\n      str_sub(LATITUDE, start = 3)\r\n    ),\r\n    LATITUDE = na_if(LATITUDE, \"-NA.NA\")\r\n  ) |>\r\n  mutate(\r\n    LONGITUD = na_if(LONGITUD, \"888888888\"),\r\n    LONGITUD = na_if(LONGITUD, \"999999999\"),\r\n    LONGITUD = paste0(\r\n      str_sub(LONGITUD, start = 0, end = 3),\r\n      \".\",\r\n      str_sub(LONGITUD, start = 4)\r\n    ),\r\n    LONGITUD = na_if(LONGITUD, \"-NA.NA\")\r\n  ) |>\r\n  mutate(across(c(LATITUDE, LONGITUD), ~ as.numeric(.x)))\r\n\r\n\r\nFor 2001 to 2007, the coordinates are stored in a different variable.\r\n\r\n\r\nShow code\r\n\r\naccident_01_05 <- map2_dfr(\r\n  2001:2005,\r\n  \"accident\",\r\n  read_FARS_no_subfolder\r\n)\r\n\r\naccident_01_05 <- accident_01_05 |>\r\n  mutate(\r\n    LATITUDE = latitude,\r\n    LONGITUD = longitud\r\n  )\r\n\r\naccident_06_07 <- map2_dfr(\r\n  2006:2007,\r\n  \"accident\",\r\n  read_FARS_subfolder\r\n)\r\naccident_06_07 <- accident_06_07 |>\r\n  mutate(\r\n    LATITUDE = latitude,\r\n    LONGITUD = longitud\r\n  )\r\n\r\n\r\nStarting in 2008, the coordinate format is all the same, but there’s variability whether the data is in subfolders or not.\r\n\r\n\r\nShow code\r\n\r\naccident_08_15 <- map2_dfr(\r\n  2008:2015,\r\n  \"accident\",\r\n  read_FARS_subfolder\r\n)\r\n\r\n\r\naccident_16_19 <- map2_dfr(\r\n  2016:2019,\r\n  \"accident\",\r\n  read_FARS_no_subfolder\r\n)\r\n\r\n\r\naccident_20_21 <- map2_dfr(\r\n  2020:2021,\r\n  \"accident\",\r\n  read_FARS_subfolder\r\n)\r\n\r\n\r\nNow we combine all accident data frames.\r\n\r\n\r\nShow code\r\n\r\naccident <- bind_rows(\r\n  accident_00,\r\n  accident_01_05,\r\n  accident_06_07,\r\n  accident_08_15,\r\n  accident_16_19,\r\n  accident_20_21\r\n)\r\n\r\n\r\nFor the person data files, the only difference is whether they do or do not use subfolders.\r\n\r\n\r\nShow code\r\n\r\n# define the years for which data does/doesn't live in subfolders\r\nno_subfolder_years <- c(2000:2005, 2016:2019)\r\nsubfolder_years <- c(2006:2015, 2020:2021)\r\n\r\n\r\nperson_no_subfolder <- map2_dfr(\r\n  no_subfolder_years,\r\n  \"person\",\r\n  read_FARS_no_subfolder\r\n)\r\nperson_subfolder <- map2_dfr(\r\n  subfolder_years,\r\n  \"person\",\r\n  read_FARS_subfolder\r\n)\r\n\r\nperson <- bind_rows(person_subfolder, person_no_subfolder)\r\n\r\n\r\nAll these steps take time and so we save the person and accident data as rds files.\r\n\r\n\r\nShow code\r\n\r\nsaveRDS(person, file = \"data/person.RDS\")\r\nsaveRDS(accident, file = \"data/accident.RDS\")\r\n\r\nperson <- readRDS(\"data/person.RDS\")\r\naccident <- readRDS(\"data/accident.RDS\")\r\n\r\n\r\nFiltering and cleaning the data\r\nFARS data include all traffic fatalities.\r\nWe filter the person file to only include cases where the person is a pedestrian and the injury severity is a fatality.\r\n\r\n\r\nShow code\r\n\r\npeds <- person |>\r\n  filter(PER_TYP == 5 & INJ_SEV == 4)\r\n\r\n\r\nThe crash locations are in the ACCIDENT table.\r\nNot all crashes are geocoded:\r\n\r\n\r\nShow code\r\n\r\naccident <- accident |>\r\n  mutate(\r\n    LATITUDE = if_else(LATITUDE %in% c(99.9999, 88.8888, 77.7777), NA, LATITUDE),\r\n    LONGITUD = if_else(LONGITUD %in% c(999.9999, 888.8888, 777.7777), NA, LONGITUD)\r\n  ) |>\r\n  mutate(geocoded = if_else(is.na(LATITUDE), \"not geocoded\", \"geocoded\"))\r\n\r\naccident |>\r\n  group_by(geocoded) |>\r\n  tally()\r\n\r\n\r\nWe remove any non-geocoded crashes and the join the two tables, using the ST_CASE identifier.\r\nNote that ST_CASE is only unique within a year.\r\nAlso note that there are person entries that don’t have a corresponding accident.\r\nWe don’t fully understand what type of fatalities those are and will exclude them from analysis.\r\n\r\n\r\nShow code\r\n\r\naccident <- accident |>\r\n  filter(geocoded == \"geocoded\")\r\n\r\npeds <- peds |>\r\n  left_join(accident, by = join_by(\r\n    ST_CASE,\r\n    MONTH,\r\n    DAY,\r\n    HOUR,\r\n    MINUTE,\r\n    STATE,\r\n    VE_FORMS,\r\n    COUNTY,\r\n    RUR_URB,\r\n    FUNC_SYS,\r\n    HARM_EV,\r\n    MAN_COLL,\r\n    SCH_BUS\r\n  ))\r\n\r\n\r\nThe data contain a lot of variables and for now we’ll only keep the ones needed for the time-of-crash analysis.\r\n\r\n\r\nShow code\r\n\r\nvariables_for_daylight <- c(\r\n  \"STATE\",\r\n  \"ST_CASE\",\r\n  \"COUNTY\",\r\n  \"DAY\",\r\n  \"MONTH\",\r\n  \"HOUR\",\r\n  \"MINUTE\",\r\n  \"YEAR\",\r\n  \"AGE\",\r\n  \"SEX\",\r\n  \"HISPANIC\",\r\n  \"RACE\",\r\n  \"LATITUDE\",\r\n  \"LONGITUD\"\r\n)\r\n\r\npeds <- peds |>\r\n  select(all_of(variables_for_daylight))\r\n\r\n\r\nWe filter for crashes that don’t contain an exact time.\r\nWe also parse the date and time fields.\r\n\r\n\r\nShow code\r\n\r\npeds <- peds |>\r\n  filter(MINUTE != 99 & !is.na(YEAR)) |>\r\n  mutate(\r\n    crash_time = ymd_hm(paste(YEAR, MONTH, DAY, HOUR, MINUTE, sep = \"-\")),\r\n    crash_date = ymd(paste(YEAR, MONTH, DAY, sep = \"-\"))\r\n  )\r\n\r\n\r\nNext we need to deal with time and time zones.\r\nThe crash time and date in FARS appear to be stored as the local time, with no time zone information.\r\nWe will use the latitude and longitude to look up the time zone and then convert all times into UTC.\r\n\r\n\r\nShow code\r\n\r\n# create sf object\r\npeds <- st_as_sf(peds, coords = c(\"LONGITUD\", \"LATITUDE\"), remove = FALSE) # keep lat/lon columns\r\npeds$tz <- tz_lookup(peds, method = \"accurate\")\r\n\r\n# Function to convert times to UTC\r\nconvert_to_utc <- function(datetime, timezone) {\r\n  # Set the time zone of the datetime object\r\n  datetime <- force_tz(datetime, tzone = timezone) # assign time zone\r\n\r\n  # Convert the datetime to UTC\r\n  datetime_utc <- with_tz(datetime, tzone = \"UTC\") # convert to time zone\r\n\r\n  return(as_datetime(datetime_utc))\r\n}\r\n\r\npeds$crash_time_utc <- mapply(convert_to_utc, datetime = peds$crash_time, timezone = peds$tz)\r\n\r\npeds <- peds |>\r\n  mutate(\r\n    crash_time_utc = as_datetime(crash_time_utc),\r\n    crash_date_utc = date(crash_time_utc)\r\n  )\r\n\r\n\r\nBased on the new UTC times we can now calculate sunrise and sunsets for all crashes.\r\nThis was more complicated than I thought: Once we get to calculating the time to/from the nearest sunrise/sunset, sometimes that sunrise or sunset falls to a different date.\r\nSo we calculate the sunrise/sunset for the day before the crash, the day of the crash, and the day after, and then use the minimum difference.\r\n(I feel there must be a simpler way to do this…)\r\n\r\n\r\nShow code\r\n\r\npeds <- peds |>\r\n  rowwise() |>\r\n  mutate(\r\n    sun_0 = getSunlightTimes(date = crash_date, lat = LATITUDE, lon = LONGITUD),\r\n    sun_plus_1 = getSunlightTimes(date = crash_date + 1, lat = LATITUDE, lon = LONGITUD),\r\n    sun_minus_1 = getSunlightTimes(date = crash_date - 1, lat = LATITUDE, lon = LONGITUD),\r\n    sunrise_0 = sun_0$sunrise,\r\n    sunrise_plus_1 = sun_plus_1$sunrise,\r\n    sunrise_minus_1 = sun_minus_1$sunrise,\r\n    sunset_0 = sun_0$sunset,\r\n    sunset_plus_1 = sun_plus_1$sunset,\r\n    sunset_minus_1 = sun_minus_1$sunset\r\n  )\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\npeds <- peds |>\r\n  mutate(\r\n    time_from_sunrise_0 = crash_time_utc - sunrise_0,\r\n    time_from_sunset_0 = crash_time_utc - sunset_0,\r\n    time_from_sunrise_minus_1 = crash_time_utc - sunrise_minus_1,\r\n    time_from_sunset_minus_1 = crash_time_utc - sunset_minus_1,\r\n    time_from_sunrise_plus_1 = crash_time_utc - sunrise_plus_1,\r\n    time_from_sunset_plus_1 = crash_time_utc - sunset_plus_1,\r\n    time_from_sunrise_min = case_when(\r\n      min(abs(time_from_sunrise_0), abs(time_from_sunrise_minus_1), abs(time_from_sunrise_plus_1)) == abs(time_from_sunrise_0) ~ time_from_sunrise_0,\r\n      min(abs(time_from_sunrise_0), abs(time_from_sunrise_minus_1), abs(time_from_sunrise_plus_1)) == abs(time_from_sunrise_minus_1) ~ time_from_sunrise_minus_1,\r\n      min(abs(time_from_sunrise_0), abs(time_from_sunrise_minus_1), abs(time_from_sunrise_plus_1)) == abs(time_from_sunrise_plus_1) ~ time_from_sunrise_plus_1\r\n    ),\r\n    time_from_sunset_min = case_when(\r\n      min(abs(time_from_sunset_0), abs(time_from_sunset_minus_1), abs(time_from_sunset_plus_1)) == abs(time_from_sunset_0) ~ time_from_sunset_0,\r\n      min(abs(time_from_sunset_0), abs(time_from_sunset_minus_1), abs(time_from_sunset_plus_1)) == abs(time_from_sunset_minus_1) ~ time_from_sunset_minus_1,\r\n      min(abs(time_from_sunset_0), abs(time_from_sunset_minus_1), abs(time_from_sunset_plus_1)) == abs(time_from_sunset_plus_1) ~ time_from_sunset_plus_1\r\n    )\r\n  )\r\n\r\n# saveRDS(peds, file = \"data/peds_with_sunset.rds\")\r\n\r\n\r\nGerman crash data\r\nCrash data for 2016 to 2022 are available from https://unfallatlas.statistikportal.de/, and it’s documented (in English) here [https://www.opengeodata.nrw.de/produkte/transport_verkehr/unfallatlas/DSB_Unfallatlas_EN.pdf].\r\nUnfortunately, the minute of the crash is not included in the data.\r\nWe download the data to a folder, read it in, and save it for future use as an RDS file.\r\n\r\n\r\nShow code\r\n\r\nread_de_crashes <- function(crash_file) {\r\n  read_delim(crash_file, delim = \";\")\r\n}\r\n\r\ncrash_files <- list.files(\"data/de\", full.names = TRUE)\r\n\r\ncrashes_de <- map_dfr(crash_files, read_de_crashes)\r\n\r\nsaveRDS(crashes_de, \"data/crashes_de.RDS\")\r\n\r\n\r\nCrashes involving pedestrians are coded with IstFuss == 1 and fatal crashes are coded as UKATEGORIE == 1.\r\nWe filter accordingly and then combine with the US fatal pedestrian crash data.\r\nNote that the US data has one row for every pedestrian fatality, whereas the German data has one row per crash.\r\nThis may lead to minor difference, e.g regarding crashes that kill multiple pedestrians, or crashes that involve a pedestrian, but it’s not the pedestrian who is killed.\r\nOverall, these differences are probably fairly minor.\r\nTo make the time periods comparable, we limit the data to 2016-2021.\r\n\r\n\r\nShow code\r\n\r\nus_crashes <- peds |>\r\n  sf::st_drop_geometry() |>\r\n  mutate(\r\n    crash_hour = HOUR,\r\n    country = \"US\"\r\n  ) |>\r\n  filter(YEAR > 2015) |>\r\n  select(crash_hour, country)\r\n\r\nde_crashes_fatal <- crashes_de |>\r\n  filter(IstFuss == 1 & UJAHR < 2022) |>\r\n  mutate(\r\n    crash_hour = as.numeric(USTUNDE),\r\n    crash_type = case_when(\r\n      UKATEGORIE == 1 ~ \"fatal\",\r\n      UKATEGORIE == 2 ~ \"serious injury\",\r\n      UKATEGORIE == 3 ~ \"minor injury\"\r\n    ),\r\n    crash_type = factor(crash_type, levels = c(\"fatal\", \"serious injury\", \"minor injury\"), ordered = TRUE),\r\n    country = \"Germany\"\r\n  ) |>\r\n  filter(crash_type == \"fatal\") |>\r\n  select(crash_hour, country)\r\n\r\ncrashes_de_us <- rbind(us_crashes, de_crashes_fatal)\r\nsaveRDS(crashes_de_us, file = \"data/crashes_de_us.RDS\")\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ndevtools::session_info()\r\n\r\n─ Session info ─────────────────────────────────────────────────────\r\n setting  value\r\n version  R version 4.3.1 (2023-06-16 ucrt)\r\n os       Windows 10 x64 (build 19045)\r\n system   x86_64, mingw32\r\n ui       RTerm\r\n language (EN)\r\n collate  English_United States.utf8\r\n ctype    English_United States.utf8\r\n tz       America/Chicago\r\n date     2024-03-05\r\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\r\n\r\n─ Packages ─────────────────────────────────────────────────────────\r\n package           * version date (UTC) lib source\r\n bslib               0.5.0   2023-06-09 [1] CRAN (R 4.3.1)\r\n cachem              1.0.8   2023-05-01 [1] CRAN (R 4.3.1)\r\n callr               3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\r\n class               7.3-22  2023-05-03 [2] CRAN (R 4.3.1)\r\n classInt            0.4-9   2023-02-28 [1] CRAN (R 4.3.1)\r\n cli                 3.6.1   2023-03-23 [1] CRAN (R 4.3.1)\r\n colorspace          2.1-0   2023-01-23 [1] CRAN (R 4.3.1)\r\n commonmark          1.9.0   2023-03-17 [1] CRAN (R 4.3.1)\r\n crayon              1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\r\n crul                1.4.0   2023-05-17 [1] CRAN (R 4.3.1)\r\n curl                5.0.1   2023-06-07 [1] CRAN (R 4.3.1)\r\n data.table          1.14.8  2023-02-17 [1] CRAN (R 4.3.1)\r\n DBI                 1.1.3   2022-06-18 [1] CRAN (R 4.3.1)\r\n devtools            2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\r\n digest              0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\r\n distill             1.6     2023-10-06 [1] CRAN (R 4.3.1)\r\n downlit             0.4.3   2023-06-29 [1] CRAN (R 4.3.1)\r\n dplyr             * 1.1.2   2023-04-20 [1] CRAN (R 4.3.1)\r\n e1071               1.7-13  2023-02-01 [1] CRAN (R 4.3.1)\r\n ellipsis            0.3.2   2021-04-29 [1] CRAN (R 4.3.1)\r\n evaluate            0.22    2023-09-29 [1] CRAN (R 4.3.1)\r\n extrafont         * 0.19    2023-01-18 [1] CRAN (R 4.3.0)\r\n extrafontdb         1.0     2012-06-11 [1] CRAN (R 4.3.0)\r\n fansi               1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\r\n farver              2.1.1   2022-07-06 [1] CRAN (R 4.3.1)\r\n fastmap             1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\r\n fontBitstreamVera   0.1.1   2017-02-01 [1] CRAN (R 4.3.0)\r\n fontLiberation      0.1.0   2016-10-15 [1] CRAN (R 4.3.0)\r\n fontquiver          0.2.1   2017-02-01 [1] CRAN (R 4.3.1)\r\n forcats           * 1.0.0   2023-01-29 [1] CRAN (R 4.3.1)\r\n fs                  1.6.2   2023-04-25 [1] CRAN (R 4.3.1)\r\n gdtools             0.3.3   2023-03-27 [1] CRAN (R 4.3.1)\r\n generics            0.1.3   2022-07-05 [1] CRAN (R 4.3.1)\r\n gfonts              0.2.0   2023-01-08 [1] CRAN (R 4.3.1)\r\n ggplot2           * 3.4.4   2023-10-12 [1] CRAN (R 4.3.2)\r\n ggtext              0.1.2   2022-09-16 [1] CRAN (R 4.3.1)\r\n glue                1.6.2   2022-02-24 [1] CRAN (R 4.3.1)\r\n gridtext            0.1.5   2022-09-16 [1] CRAN (R 4.3.1)\r\n gt                * 0.9.0   2023-03-31 [1] CRAN (R 4.3.1)\r\n gtable              0.3.4   2023-08-21 [1] CRAN (R 4.3.1)\r\n haven             * 2.5.3   2023-06-30 [1] CRAN (R 4.3.1)\r\n highr               0.10    2022-12-22 [1] CRAN (R 4.3.1)\r\n hms                 1.1.3   2023-03-21 [1] CRAN (R 4.3.1)\r\n hrbrthemes        * 0.8.0   2020-03-06 [1] CRAN (R 4.3.1)\r\n htmltools           0.5.5   2023-03-23 [1] CRAN (R 4.3.1)\r\n htmlwidgets         1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\r\n httpcode            0.3.0   2020-04-10 [1] CRAN (R 4.3.1)\r\n httpuv              1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\r\n httr                1.4.7   2023-08-15 [1] CRAN (R 4.3.1)\r\n jquerylib           0.1.4   2021-04-26 [1] CRAN (R 4.3.1)\r\n jsonlite            1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\r\n KernSmooth          2.23-21 2023-05-03 [2] CRAN (R 4.3.1)\r\n knitr               1.43    2023-05-25 [1] CRAN (R 4.3.1)\r\n labeling            0.4.3   2023-08-29 [1] CRAN (R 4.3.1)\r\n later               1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\r\n lifecycle           1.0.3   2022-10-07 [1] CRAN (R 4.3.1)\r\n lubridate         * 1.9.2   2023-02-10 [1] CRAN (R 4.3.1)\r\n lutz              * 0.3.2   2023-10-17 [1] CRAN (R 4.3.2)\r\n magrittr            2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\r\n markdown            1.7     2023-05-16 [1] CRAN (R 4.3.1)\r\n memoise             2.0.1   2021-11-26 [1] CRAN (R 4.3.1)\r\n mime                0.12    2021-09-28 [1] CRAN (R 4.3.0)\r\n miniUI              0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\r\n munsell             0.5.0   2018-06-12 [1] CRAN (R 4.3.1)\r\n pillar              1.9.0   2023-03-22 [1] CRAN (R 4.3.1)\r\n pkgbuild            1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\r\n pkgconfig           2.0.3   2019-09-22 [1] CRAN (R 4.3.1)\r\n pkgload             1.3.3   2023-09-22 [1] CRAN (R 4.3.1)\r\n prettyunits         1.2.0   2023-09-24 [1] CRAN (R 4.3.1)\r\n processx            3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\r\n profvis             0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\r\n promises            1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\r\n proxy               0.4-27  2022-06-09 [1] CRAN (R 4.3.1)\r\n ps                  1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\r\n purrr             * 1.0.2   2023-08-10 [1] CRAN (R 4.3.2)\r\n R6                  2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\r\n rappdirs            0.3.3   2021-01-31 [1] CRAN (R 4.3.1)\r\n Rcpp                1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\r\n readr             * 2.1.4   2023-02-10 [1] CRAN (R 4.3.1)\r\n remotes             2.4.2   2021-11-30 [1] CRAN (R 4.3.1)\r\n rlang               1.1.1   2023-04-28 [1] CRAN (R 4.3.1)\r\n rmarkdown           2.23    2023-07-01 [1] CRAN (R 4.3.1)\r\n rstudioapi          0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\r\n Rttf2pt1            1.3.12  2023-01-22 [1] CRAN (R 4.3.0)\r\n sass                0.4.6   2023-05-03 [1] CRAN (R 4.3.1)\r\n scales              1.2.1   2022-08-20 [1] CRAN (R 4.3.1)\r\n sessioninfo         1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\r\n sf                * 1.0-14  2023-07-11 [1] CRAN (R 4.3.1)\r\n shiny               1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\r\n stringi             1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\r\n stringr           * 1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\r\n suncalc           * 0.5.1   2022-09-29 [1] CRAN (R 4.3.2)\r\n systemfonts         1.0.4   2022-02-11 [1] CRAN (R 4.3.1)\r\n tibble            * 3.2.1   2023-03-20 [1] CRAN (R 4.3.1)\r\n tidyr             * 1.3.0   2023-01-24 [1] CRAN (R 4.3.1)\r\n tidyselect          1.2.0   2022-10-10 [1] CRAN (R 4.3.1)\r\n tidyverse         * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\r\n tigris              2.0.3   2023-05-19 [1] CRAN (R 4.3.1)\r\n timechange          0.2.0   2023-01-11 [1] CRAN (R 4.3.1)\r\n tzdb                0.4.0   2023-05-12 [1] CRAN (R 4.3.1)\r\n units               0.8-2   2023-04-27 [1] CRAN (R 4.3.1)\r\n urlchecker          1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\r\n usethis             2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\r\n utf8                1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\r\n uuid                1.1-0   2022-04-19 [1] CRAN (R 4.3.0)\r\n vctrs               0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\r\n withr               2.5.1   2023-09-26 [1] CRAN (R 4.3.1)\r\n xfun                0.39    2023-04-20 [1] CRAN (R 4.3.1)\r\n xml2                1.3.5   2023-07-06 [1] CRAN (R 4.3.1)\r\n xtable              1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\r\n yaml                2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\r\n\r\n [1] C:/Users/user1/AppData/Local/R/win-library/4.3\r\n [2] C:/Program Files/R/R-4.3.1/library\r\n\r\n────────────────────────────────────────────────────────────────────\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/pedestrian-fatalities-soar-just-after-sunset/distill-preview.png",
    "last_modified": "2024-03-05T17:19:01-06:00",
    "input_file": {},
    "preview_width": 2112,
    "preview_height": 1190
  },
  {
    "path": "posts/2024-02-29-where-are-the-4-car-households/",
    "title": "Where are the 4+-car households?",
    "description": "A quick look at households with 4 cars or more.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2024-02-28",
    "categories": [
      "American Community Survey",
      "maps",
      "transportation"
    ],
    "contents": "\r\nSometimes you get sucked into a data rabbit hole. For reasons I don’t exactly remember, I got interested in households with a lot of cars. Here’s a quick blog post about households with 4 or more vehicles available, based on data from the American Community Survey. We limit the analysis to the 100 cities with the largest population (my preferred way of doing it because it includes the city I live in, Madison, Wisconsin).\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidycensus)\r\nlibrary(tigris)\r\noptions(tigris_use_cache = TRUE)\r\nlibrary(gt)\r\nlibrary(tmap)\r\nlibrary(hrbrthemes)\r\nextrafont::loadfonts()\r\n\r\n\r\nLet’s start with a map:\r\n\r\n\r\nShow code\r\n\r\nget_pop <- function(state) {\r\n  get_acs(\"place\", table = \"B01003\", state = state, cache_table = TRUE)\r\n}\r\n\r\npop <- map_dfr(datasets::state.abb, get_pop)\r\n\r\npop_100_states <- pop |>\r\n  arrange(desc(estimate)) |>\r\n  head(100) |>\r\n  mutate(state = str_sub(GEOID, start = 0, end = 2)) |>\r\n  distinct(state)\r\n\r\npop_100_cities <- pop |>\r\n  arrange(desc(estimate)) |>\r\n  head(100) |>\r\n  pull(NAME)\r\n\r\n\r\nget_cars <- function(state) {\r\n  get_acs(\"place\",\r\n    table = \"B08201\",\r\n    summary_var = \"B08201_001\",\r\n    state = state, cache_table = TRUE, geometry = T\r\n  )\r\n}\r\n\r\ncars <- map_dfr(pop_100_states, get_cars)\r\n\r\ntmap_mode(\"view\")\r\ncars |>\r\n  filter(NAME %in% pop_100_cities) |>\r\n  filter(variable == \"B08201_006\") |>\r\n  mutate(\r\n    cars_4_pct = estimate / summary_est,\r\n    cars_4_pct_label = scales::percent(cars_4_pct)\r\n  ) |>\r\n  arrange(desc(cars_4_pct)) |>\r\n  tm_shape() +\r\n  tm_dots(\r\n    title = \"4+-car households\",\r\n    col = \"cars_4_pct\",\r\n    id = \"NAME\",\r\n    popup.vars = c(\"Share of 4+-car households\" = \"cars_4_pct_label\"),\r\n    legend.format = list(fun = function(x) paste0(formatC(x * 100, digits = 0, format = \"f\"), \" %\")),\r\n    style = \"jenks\"\r\n  ) +\r\n  tm_layout(title = \"High-car ownership in US cities\") +\r\n  tm_credits(\"100 largest US cities. Data: American Community Survey\")\r\n\r\n\r\n\r\nSome patterns appear to emerge, but let’s look at the top and bottom ten of the list in a table:\r\n\r\n\r\nShow code\r\n\r\ntop_bottom_vector <- c(top_bottom = rep(\"Highest %\", 10), rep(\"Lowest %\", 10))\r\n\r\ntop_bottom <- cars |>\r\n  sf::st_drop_geometry() |>\r\n  filter(NAME %in% pop_100_cities) |>\r\n  filter(variable == \"B08201_006\") |>\r\n  mutate(\r\n    cars_4_pct = estimate / summary_est,\r\n    NAME = str_remove(NAME, \" city\")\r\n  ) |>\r\n  arrange(desc(cars_4_pct))\r\n\r\ntop <- head(top_bottom, 10) |> mutate(top_bottom = \"Highest %\")\r\nbottom <- tail(top_bottom, 10) |> mutate(top_bottom = \"Lowest %\")\r\n\r\ntop_bottom_table <- rbind(top, bottom)\r\n\r\ntop_bottom_table |>\r\n  group_by(top_bottom) |>\r\n  select(NAME, estimate, cars_4_pct, top_bottom) |>\r\n  gt() |>\r\n  tab_header(\r\n    title = \"US cities with the highest and lowest % of owning 4+ cars\",\r\n    subtitle = \"100 most populous cities\"\r\n  ) |>\r\n  fmt_percent(columns = cars_4_pct, decimals = 1) |>\r\n  fmt_number(estimate, use_seps = TRUE, decimals = 0) |>\r\n  tab_spanner(\"4+ car households\", columns = c(estimate, cars_4_pct)) |>\r\n  cols_label(NAME = \"City\", estimate = \"number\", cars_4_pct = \"percent\") |>\r\n  tab_source_note(\"American Community Survey, 5-year estimates, 2017-2021. Table B08201\") |>\r\n  opt_table_font(stack = \"industrial\") |>\r\n  tab_style(\r\n    style = cell_text(weight = \"bold\"),\r\n    locations = cells_row_groups()\r\n  )\r\n\r\n\r\nUS cities with the highest and lowest % of owning 4+ cars\r\n    100 most populous cities\r\n    City\r\n      \r\n        4+ car households\r\n      \r\n    number\r\n      percent\r\n    Highest %\r\n    Santa Ana, California\r\n12,754\r\n16.6%Riverside, California\r\n11,683\r\n13.2%Chula Vista, California\r\n10,656\r\n12.9%Anaheim, California\r\n12,730\r\n12.3%San Jose, California\r\n36,761\r\n11.3%Santa Clarita, California\r\n8,182\r\n11.2%Stockton, California\r\n9,785\r\n10.2%Garland, Texas\r\n7,881\r\n9.9%Laredo, Texas\r\n7,142\r\n9.8%Fremont, California\r\n6,791\r\n9.1%Lowest %\r\n    Atlanta, Georgia\r\n4,467\r\n2.0%St. Louis, Missouri\r\n2,621\r\n1.9%Baltimore, Maryland\r\n4,474\r\n1.8%Pittsburgh, Pennsylvania\r\n2,151\r\n1.6%Buffalo, New York\r\n1,789\r\n1.5%Philadelphia, Pennsylvania\r\n9,067\r\n1.4%Boston, Massachusetts\r\n3,431\r\n1.3%Newark, New Jersey\r\n1,370\r\n1.2%Jersey City, New Jersey\r\n1,026\r\n0.9%New York, New York\r\n26,323\r\n0.8%American Community Survey, 5-year estimates, 2017-2021. Table B08201\r\n    \r\n\r\nWhat can explain the difference? Is it something uniquely Californian and Texan that has people owning so many cars? One thing to look is median household income. Places with higher incomes probably afford more car ownership.\r\n\r\n\r\nShow code\r\n\r\nget_income <- function(state) {\r\n  get_acs(\"place\",\r\n    table = \"B19013\", survey = \"acs5\",\r\n    state = state, cache_table = TRUE, geometry = FALSE\r\n  )\r\n}\r\n\r\nincome <- map_dfr(pop_100_states, get_income)\r\n\r\nincome |>\r\n  filter(NAME %in% pop_100_cities) |>\r\n  left_join(top_bottom, by = \"GEOID\") |>\r\n  mutate(\r\n    short_name = str_extract(NAME.x, \"^([^,]*)\"),\r\n    short_name = str_remove(short_name, \" city| municipality\"),\r\n    median_income = estimate.x\r\n  ) |>\r\n  ggplot(aes(median_income, cars_4_pct, label = short_name)) +\r\n  ggrepel::geom_text_repel() +\r\n  geom_point() +\r\n  scale_x_continuous(name = \"Median household income\", labels = scales::label_dollar()) +\r\n  scale_y_continuous(name = \"Share of 4+-car households\", labels = scales::label_percent()) +\r\n  theme_ipsum_rc() +\r\n  labs(\r\n    title = \"High-car households and median household income\",\r\n    subtitle = \"100 largest US cities\",\r\n    caption = \"Data: American Community Survey 5-year estimates, 2017-2021\\nVisualization: Harald Kliems\"\r\n  )\r\n\r\n\r\n\r\nWe should also look at the average household size. Maybe cities with a high share of 4+-car households just have larger households!\r\n\r\n\r\nShow code\r\n\r\nget_hh_size <- function(state) {\r\n  get_acs(\"place\",\r\n    variable = \"B25010_001\", survey = \"acs5\",\r\n    state = state, cache_table = TRUE, geometry = FALSE\r\n  )\r\n}\r\n\r\n\r\nhh_size <- map_dfr(pop_100_states, get_hh_size)\r\n\r\nhh_size |>\r\n  filter(NAME %in% pop_100_cities) |>\r\n  left_join(top_bottom, by = \"GEOID\") |>\r\n  mutate(\r\n    short_name = str_extract(NAME.x, \"^([^,]*)\"),\r\n    short_name = str_remove(short_name, \" city| municipality\"),\r\n    hh_size = estimate.x\r\n  ) |>\r\n  ggplot(aes(hh_size, cars_4_pct, label = short_name)) +\r\n  ggrepel::geom_text_repel() +\r\n  geom_point() +\r\n  scale_x_continuous(name = \"Mean household size\") +\r\n  scale_y_continuous(name = \"Share of 4+-car households\", labels = scales::label_percent()) +\r\n  theme_ipsum_rc() +\r\n  labs(\r\n    title = \"High-car households and average household size\",\r\n    subtitle = \"100 largest US cities\",\r\n    caption = \"Data: American Community Survey 5-year estimates, 2017-2021\\nVisualization: Harald Kliems\"\r\n  )\r\n\r\n\r\n\r\nAnd yes, there’s a very strong relationship! Of course there are outliers such as New York and Boston—which probably tell us we should add an indicator for cities with a good public transit system. While it’s not perfect, we use transit commute mode share as our variable.\r\n\r\n\r\nShow code\r\n\r\nget_transit <- function(state) {\r\n  get_acs(\"place\",\r\n    table = \"S0801\", survey = \"acs5\",\r\n    state = state, cache_table = TRUE, geometry = FALSE\r\n  )\r\n}\r\n\r\n\r\ntransit <- map_dfr(pop_100_states, get_transit)\r\n\r\ntransit |>\r\n  filter(NAME %in% pop_100_cities) |>\r\n  filter(variable == \"S0801_C01_009\") |>\r\n  left_join(top_bottom, by = \"GEOID\") |>\r\n  mutate(\r\n    short_name = str_extract(NAME.x, \"^([^,]*)\"),\r\n    short_name = str_remove(short_name, \" city| municipality\"),\r\n    transit_share = estimate.x\r\n  ) |>\r\n  ggplot(aes(transit_share, cars_4_pct, label = short_name)) +\r\n  ggrepel::geom_text_repel() +\r\n  geom_point() +\r\n  # scale_x_continuous(name = \"Median household income\", labels = scales::label_dollar()) +\r\n  scale_y_continuous(name = \"Share of 4+-car households\", labels = scales::label_percent()) +\r\n  theme_ipsum_rc() +\r\n  labs(\r\n    title = \"High-car households and transit commuting mode share\",\r\n    subtitle = \"100 largest US cities\",\r\n    caption = \"Data: American Community Survey 5-year estimates, 2017-2021\\nVisualization: Harald Kliems\"\r\n  )\r\n\r\n\r\n\r\nHigh transit cities have few high-car households; but within low transit cities, there’s still a lot of variability.\r\nIf this weren’t just a quick post, I’d probably try building a model that takes income, household size, and transit mode share together. But I’ll leave that for another day or someone else.\r\nBonus table (revised: 2024-03-02)\r\nAfter publishing this article, I received feedback. Given the chart about household sizes, it would be interesting to look at the number of vehicles by household size. That makes sense: There’s a difference between a one-person household having 4+ cars and a four-person household doing so. The data is available, and we can calculate the percentage of households that have more cars than people in it. Here’s the new top/bottom ten table:\r\n\r\n\r\nShow code\r\n\r\nover_car_vars <- c(10, 11, 12, 17, 18, 24)\r\n\r\nfilter_vars <- map_chr(over_car_vars, function(x) {\r\n  paste0(\"B08201_0\", x)\r\n})\r\n\r\n\r\nover_cars <- cars |>\r\n  sf::st_drop_geometry() |>\r\n  filter(NAME %in% pop_100_cities) |>\r\n  pivot_wider(id_cols = c(GEOID, NAME), names_from = \"variable\", values_from = \"estimate\") |>\r\n  rowwise() |>\r\n  mutate(over_car = sum(!!!syms(filter_vars)) / B08201_001)\r\n\r\n\r\ntop_bottom <- over_cars |> arrange(desc(over_car))\r\n\r\ntop <- head(top_bottom, 10) |> mutate(top_bottom = \"Highest %\")\r\nbottom <- tail(top_bottom, 10) |> mutate(top_bottom = \"Lowest %\")\r\n\r\ntop_bottom_table <- rbind(top, bottom)\r\n\r\ntop_bottom_table |>\r\n  group_by(top_bottom) |>\r\n  select(NAME, over_car, top_bottom) |>\r\n  mutate(NAME = str_remove(NAME, \" city| municipality\")) |>\r\n  gt() |>\r\n  tab_header(\r\n    title = \"US cities with the highest and lowest % households with more cars than people\",\r\n    subtitle = \"100 most populous cities\"\r\n  ) |>\r\n  fmt_percent(columns = over_car, decimals = 1) |>\r\n  cols_label(NAME = \"City\", over_car = \"percentage of households\") |>\r\n  tab_source_note(\"American Community Survey, 5-year estimates, 2017-2021. Table B08201\") |>\r\n  opt_table_font(stack = \"industrial\") |>\r\n  tab_style(\r\n    style = cell_text(weight = \"bold\"),\r\n    locations = cells_row_groups()\r\n  )\r\n\r\n\r\nUS cities with the highest and lowest % households with more cars than people\r\n    100 most populous cities\r\n    City\r\n      percentage of households\r\n    Highest %\r\n    Chesapeake, Virginia\r\n15.6%Boise City, Idaho\r\n14.6%Virginia Beach, Virginia\r\n13.8%Anchorage, Alaska\r\n12.9%Wichita, Kansas\r\n12.6%Colorado Springs, Colorado\r\n12.6%Winston-Salem, North Carolina\r\n12.5%Spokane, Washington\r\n12.0%Oklahoma City, Oklahoma\r\n11.7%Albuquerque, New Mexico\r\n11.5%Lowest %\r\n    Baltimore, Maryland\r\n4.5%Buffalo, New York\r\n4.5%Miami, Florida\r\n3.3%San Francisco, California\r\n3.2%Chicago, Illinois\r\n3.1%Philadelphia, Pennsylvania\r\n3.0%Newark, New Jersey\r\n2.5%Boston, Massachusetts\r\n2.1%Jersey City, New Jersey\r\n1.1%New York, New York\r\n1.0%American Community Survey, 5-year estimates, 2017-2021. Table B08201\r\n    \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-29-where-are-the-4-car-households/where-are-the-4-car-households_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2024-03-02T06:46:01-06:00",
    "input_file": "where-are-the-4-car-households.knit.md",
    "preview_width": 1920,
    "preview_height": 1920
  },
  {
    "path": "posts/2024-02-28-madisons-largest-temperature-swings/",
    "title": "Madison's largest temperature swings",
    "description": "From 67 to 15 F overnight??",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2024-02-28",
    "categories": [
      "Madison (WI)",
      "climate"
    ],
    "contents": "\r\nThe weather forecast for February 27 and 28 promised a wild temperature swing: From a record-scratching 67 F (19° C) on the 27th to a more seasonally appropriate low of 15 F (-9° C) the next day. Would this constitute the largest temperature swing ever recorded in Madison_ I already had some code to get the daily GHCN weather data (shout-out to JD Johnson and his Milwaukee weather graphs). Calculating the numbers and producing a chart was straightforward. First we get data from the Madison Truax location, which is the station with the longest-running temperature records, going back to 1939.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n# station readme\r\n#   https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_station/readme-by_station.txt\r\n\r\n# # data readme\r\n# #   https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\r\n\r\n# download the zipped file\r\ntemp <- tempfile()\r\ndownload.file(\"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_station/USW00014837.csv.gz\", temp)\r\n\r\n# unzip and read\r\nghcn <- read_csv(temp,\r\n  col_names = c(\r\n    \"id\", \"yearmoda\", \"element\", \"value\",\r\n    \"mflag\", \"qflag\", \"sflag\", \"obs_time\"\r\n  ),\r\n  col_types = \"cccncccc\"\r\n)\r\n\r\n# delete the zipped file\r\nunlink(temp)\r\n\r\n# subset and format\r\nghcn.wide <- ghcn %>%\r\n  select(yearmoda, element, value) %>%\r\n  filter(element %in% c(\"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\")) %>%\r\n  separate(col = yearmoda, sep = c(4, 6), into = c(\"year\", \"month\", \"day\")) %>%\r\n  pivot_wider(names_from = element, values_from = value) %>%\r\n  # convert from tenths of mm to inches\r\n  mutate(\r\n    PRCP = PRCP * 0.00393701,\r\n    SNOW = SNOW * 0.00393701,\r\n    SNWD = SNWD * 0.00393701\r\n  ) %>%\r\n  # convert from tenths of degrees C to F\r\n  mutate(\r\n    TMAX = ((TMAX / 10) * (9 / 5)) + 32,\r\n    TMIN = ((TMIN / 10) * (9 / 5)) + 32\r\n  ) %>%\r\n  mutate(\r\n    date = as.Date(paste(year, month, day, sep = \"-\")),\r\n    day_of_year = case_when(\r\n      lubridate::leap_year(date) & lubridate::yday(date) == 60 ~ NA_real_,\r\n      lubridate::leap_year(date) & lubridate::yday(date) > 60 ~ lubridate::yday(date) - 1,\r\n      TRUE ~ lubridate::yday(date)\r\n    )\r\n  ) %>%\r\n  select(\r\n    year, month, day, date, day_of_year, PRCP, SNOW, SNWD,\r\n    TMAX, TMIN\r\n  )\r\n\r\n\r\nAnd now we calculate the relevant temperature difference and produce a chart:\r\n\r\n\r\nShow code\r\n\r\nghcn.wide |>\r\n  mutate(tmin_next_day = lead(TMIN)) |>\r\n  mutate(max_min_diff = abs(TMAX - tmin_next_day)) |>\r\n  arrange(desc(max_min_diff)) |>\r\n  head(10) |>\r\n  mutate(\r\n    rank_diff = row_number(max_min_diff),\r\n    mid_point = TMAX - ((TMAX - tmin_next_day) / 2),\r\n    date_label = fct_reorder(as.factor(format(date, \"%b %d, %Y\")), rank_diff)\r\n  ) |>\r\n  select(date_label, TMAX, tmin_next_day, max_min_diff, rank_diff, mid_point) |>\r\n  ggplot() +\r\n  geom_segment(aes(x = date_label, xend = date_label, y = TMAX, yend = tmin_next_day)) +\r\n  geom_point(aes(x = date_label, y = TMAX), color = \"red\") +\r\n  geom_point(aes(x = date_label, y = tmin_next_day), color = \"blue\") +\r\n  geom_text(aes(x = date_label, y = mid_point, label = round(max_min_diff, 0)), nudge_x = .4) +\r\n  geom_text(aes(x = date_label, y = tmin_next_day, label = round(tmin_next_day, 0)), color = \"blue\", alpha = .3, nudge_y = -3) +\r\n  geom_text(aes(x = date_label, y = TMAX, label = round(TMAX, 0)), color = \"red\", alpha = .3, nudge_y = 3) +\r\n  coord_flip() +\r\n  scale_y_continuous(name = \"Temperature (F)\") +\r\n  xlab(element_blank()) +\r\n  hrbrthemes::theme_ipsum_rc() +\r\n  theme(\r\n    panel.grid.major = element_blank(),\r\n    panel.grid.minor = element_blank(),\r\n    axis.text.x = element_blank()\r\n  ) +\r\n  labs(\r\n    title = \"Largest temperature swings in Madison, Wisconsin\",\r\n    subtitle = \"Maximum temperature compared with next day's minimum temperature\",\r\n    caption = \"Data: GHCN data for Madison Truax Airport (GHCND:USW00014837). Records begin on Jan 10, 1939.\\nVisualization: Harald Kliems\"\r\n  )\r\n\r\n\r\n\r\nIt’s February 28 now, and it’s very cold. And yesterday may have been warmer than initially predicted. GHCN records lag a few days behind, but I’ll certainly check back to see if we broke the top ten.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-02-28-madisons-largest-temperature-swings/madisons-largest-temperature-swings_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2024-02-28T08:34:49-06:00",
    "input_file": "madisons-largest-temperature-swings.knit.md",
    "preview_width": 1536,
    "preview_height": 844
  },
  {
    "path": "posts/2024-01-19-strava-vs-eco-counter/",
    "title": "Comparing counts from Strava Metro and loop counters",
    "description": "Can Strava Metro data in Madison be a good indicator of overall bike traffic?",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2024-01-19",
    "categories": [],
    "contents": "\r\nThis post is inspired by a line in this great paper about level of traffic stress and Strava counts.\r\n\r\nStrava counts were correlated with City of Toronto bicyclist counts at intersections for the period of July to September 2022 (City of Toronto 2022). The correlation was similar to results in other Canadian cities (R2 of 0.69, Figure S2). (Imrit et al. 2024)\r\n\r\nI am obsessed with bike counts and therefore was curious what the correlation would be for the two permanent counters in Madison. The counters, made by Eco Counter, use loop detectors in the bike path surface and are fairly reliable in counting all bikes. The latest Eco Counter full year of ridership I have data for is 2022. I visually matched the location of the two counters to two Strava segments (or “edges”) and downloaded hourly 2022 data for those segments.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(gt)\r\nlibrary(hrbrthemes)\r\nextrafont::loadfonts()\r\n\r\n\r\nData from the counters and from Strava Metro require a bit of cleaning.\r\n\r\n\r\nShow code\r\n\r\ncounts_2022 <- readxl::read_excel(\"data/EcoCounter_2022.xlsx\", skip = 3,\r\n                   col_names = c(\"time_count\", \"count_cap_city\", \"count_sw_path\")) |> \r\n  mutate(date_count = floor_date(time_count, unit = \"hours\")) |> \r\n  summarize(across(starts_with(\"count_\"), ~ sum(.x, na.rm = T)), .by = date_count) |> \r\n  pivot_longer(cols = starts_with(\"count_\"), names_to = \"location\", values_to = \"count_hourly\") |> \r\n  mutate(location = case_when(location == \"count_cap_city\" ~ \"Cap City at North Shore\",\r\n                              location == \"count_sw_path\" ~ \"SW Path at Randall\"),\r\n         dayofweek = wday(date_count),\r\n         weekendind = ifelse(dayofweek %in% c(1:5), \"weekday\", \"weekend\"),\r\n         month_count = month(date_count, label = T, abbr = T)) \r\n\r\nstrava_cap_city <- read_csv(\"data/cap_city_strava_hourly.csv\") %>% \r\n  mutate(location = \"Cap City at North Shore\")\r\nstrava_sw_path <- read_csv(\"data/sw_path_strava_hourly.csv\") %>% \r\n  mutate(location = \"SW Path at Randall\")\r\n\r\ncounts_cap_city <- strava_cap_city %>% \r\n  full_join(counts_2022 %>% filter(location == \"Cap City at North Shore\"), by = join_by(hour == date_count, location))\r\n\r\ncounts_sw_path <- strava_sw_path %>% \r\n  full_join(counts_2022 %>% filter(location == \"SW Path at Randall\"), by = join_by(hour == date_count, location))\r\n\r\n\r\nall_counts_2022 <- rbind(counts_cap_city, counts_sw_path) %>% \r\n  mutate(strava_count = replace_na(total_trip_count, 0))\r\n\r\n\r\nNow we can plot the counts. We’ll also add a line representing a simple linear regression.\r\n\r\n\r\nShow code\r\n\r\n all_counts_2022 %>% \r\n  ggplot(aes(count_hourly, strava_count)) +\r\n  geom_jitter(alpha = .1)+\r\n  facet_wrap(~ location) +\r\n  labs(x = \"Eco Counter hourly counts\",\r\n       y = \"Strava hourly counts\") +\r\n  hrbrthemes::theme_ipsum_rc() +\r\n  geom_smooth(method = \"lm\")\r\n\r\n\r\n\r\nThe Strava data is rounded into buckets of 5. The plot adds some jitter to prevent overplotting, but you can still see the horizontal bands along the 5-count increments. The overall correlation between the counts appears to be very high, and looking at the x and y axis labels, we see that Strava hourly counts are much lower than the Eco Counter ones. Here’s a summary table for the correlation between the counts and total numbers and percentages.\r\n\r\n\r\nShow code\r\n\r\nall_counts_2022 %>% \r\n  group_by(location) %>% \r\n  summarize(cor = (cor(count_hourly, strava_count)),\r\n            total_count_strava = sum(strava_count),\r\n            total_count_eco = sum(count_hourly),\r\n            pct_strava = total_count_strava/total_count_eco) %>% \r\n  gt() %>% \r\n  tab_header(title = md(\"Comparing _hourly_ counts between Strava and Eco Counter\"),\r\n             subtitle = \"Two locations in Madison, Wisconsin\") %>% \r\n  tab_spanner(columns = c(total_count_strava, total_count_eco),\r\n              label = \"Total count 2022\") %>% \r\n  cols_label(\r\n    location = \"Counter/segment location\",\r\n    cor = \"Correlation (r)\",\r\n    total_count_strava = \"Strava\",\r\n    total_count_eco = \"Eco Counter\",\r\n    pct_strava = \"Strava/Eco Counter counts\"\r\n  ) %>% \r\n  fmt_percent(columns = pct_strava, decimals = 0) %>% \r\n  fmt_number(columns = cor, decimals = 2) %>% \r\n  fmt_auto(columns = starts_with(\"total_count\"))\r\n\r\n\r\nComparing hourly counts between Strava and Eco Counter\r\n    Two locations in Madison, Wisconsin\r\n    Counter/segment location\r\n      Correlation (r)\r\n      \r\n        Total count 2022\r\n      \r\n      Strava/Eco Counter counts\r\n    Strava\r\n      Eco Counter\r\n    Cap City at North Shore\r\n0.90\r\n50,235 \r\n440,717 \r\n11%SW Path at Randall\r\n0.85\r\n26,710 \r\n298,947 \r\n9%\r\n\r\nIndeed, the correlation is very strong (as a rule of thumb, a correlation of over 0.75 is considered strong), and Strava counts represent about 10% of trips counted by the Eco Counters over the year.\r\nAll these numbers compare hourly counts. Hourly counts are important for some purposes (e.g. identifying peak use of a facility, or distinguising between commuting and recreational riding). Maybe more commonly used, however, are daily counts—in traffic engineering the “Average Daily Traffic” (ADT) is the main measure of vehicle traffic volumes. Daily counts are also what is being used in the (Imrit et al. 2024) article mentioned at the beginning of the article. So we’ll aggregate the data to the daily count and create graphs and correlations again.\r\n\r\n\r\nShow code\r\n\r\n all_counts_2022 %>% \r\n  mutate(day = floor_date(hour, unit = \"days\")) %>% \r\n  group_by(day, location) %>% \r\n  summarize(daily_eco = sum(count_hourly),\r\n            daily_strava = sum(strava_count)) %>% \r\n  ggplot(aes(daily_eco, daily_strava)) +\r\n  geom_point(alpha = .1)+\r\n  facet_wrap(~ location) +\r\n  labs(x = \"Eco Counter daily counts\",\r\n       y = \"Strava daily counts\") +\r\n  hrbrthemes::theme_ipsum_rc() +\r\n  geom_smooth(method = \"lm\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n all_counts_2022 %>% \r\n  mutate(day = floor_date(hour, unit = \"days\")) %>% \r\n  group_by(day, location) %>% \r\n  summarize(daily_eco = sum(count_hourly),\r\n            daily_strava = sum(strava_count)) %>% \r\n  ungroup() %>% \r\n  group_by(location) %>% \r\n  summarize(cor = (cor(daily_eco, daily_strava))) %>% \r\n  gt() %>% \r\n    tab_header(title = md(\"Comparing _daily_ counts between Strava and Eco Counter\"),\r\n             subtitle = \"Two locations in Madison, Wisconsin\") %>% \r\n    # tab_spanner(columns = c(total_count_strava, total_count_eco),\r\n    #           label = \"Total count 2022\") %>% \r\n  cols_label(\r\n    location = \"Counter/segment location\",\r\n    cor = \"Correlation (r)\") %>% \r\n  fmt_number(columns = cor, decimals = 2)\r\n\r\n\r\nComparing daily counts between Strava and Eco Counter\r\n    Two locations in Madison, Wisconsin\r\n    Counter/segment location\r\n      Correlation (r)\r\n    Cap City at North Shore\r\n0.96SW Path at Randall\r\n0.93\r\n\r\nThe correlation between the two types of counts is now even higher, in both locations. The 0.69 number mentioned by Imrit et al. is actually not the correlation but the R2 value of a linear model. Without going into too much detail, they both are a measure of the direction and strength of relationship between two values. For the sake of completeness, let’s calculate the R2 values for daily counts in a simple regression model for each of the locations:\r\n\r\n\r\nShow code\r\n\r\nall_counts_2022 %>% \r\n   mutate(day = floor_date(hour, unit = \"days\")) %>% \r\n  group_by(day, location) %>% \r\n  summarize(daily_eco = sum(count_hourly),\r\n            daily_strava = sum(strava_count)) %>% \r\n  ungroup() %>% \r\n  filter(location == \"SW Path at Randall\") %>% \r\n  with(lm(daily_eco ~ daily_strava)) %>% \r\n  broom::glance() %>% \r\n  select(r.squared)\r\n\r\n# A tibble: 1 × 1\r\n  r.squared\r\n      <dbl>\r\n1     0.860\r\n\r\nShow code\r\n\r\nall_counts_2022 %>% \r\n   mutate(day = floor_date(hour, unit = \"days\")) %>% \r\n  group_by(day, location) %>% \r\n  summarize(daily_eco = sum(count_hourly),\r\n            daily_strava = sum(strava_count)) %>% \r\n  ungroup() %>% \r\n  filter(location == \"Cap City at North Shore\") %>% \r\n  with(lm(daily_eco ~ daily_strava)) %>% \r\n  broom::glance() %>% \r\n  select(r.squared)\r\n\r\n# A tibble: 1 × 1\r\n  r.squared\r\n      <dbl>\r\n1     0.923\r\n\r\nFor the SW Path at Randall, the R2 is 0.86; and for the Cap City at North Shore location it is 0.92! That is, much higher than the 0.69 found in Toronto.\r\nWe cannot know if these numbers would look different in other places in the city, but at least on these two major bike paths, the Strava numbers are a great indicator for overall bike usage. Often when you mention Strava data, people will point that Strava users and usage are not representative of the population at large. Which in many cases certainly is the case. But as we have seen in these two particular locations, Strava data appears to be very well aligned with loop counter data.\r\nAcknowledgments\r\nThis report includes aggregated and de-identified data from Strava Metro. Data for the Eco Counters was provided to me by the City of Madison.\r\n\r\n\r\n\r\nImrit, Amreen A., Jaimy Fischer, Timothy C. Y. Chan, Shoshanna Saxe, and Madeleine Bonsma-Fisher. 2024. “A Street-Specific Analysis of Level of Traffic Stress Trends in Strava Bicycle Ridership and Its Implications for Low-Stress Bicycling Routes in Toronto.” Findings, January. https://doi.org/10.32866/001c.92109.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-01-19-strava-vs-eco-counter/strava-vs-eco-counter_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2024-02-02T19:05:55-06:00",
    "input_file": "strava-vs-eco-counter.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-11-30-population-growth-in-wisconsin/",
    "title": "Population growth in Wisconsin",
    "description": "Mapping population growth since the 2020 census",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-11-30",
    "categories": [
      "Wisconsin",
      "population",
      "map"
    ],
    "contents": "\r\nThe Journal Sentinel recently ran an article on population growth (or the lack thereof) yesterday. Conveniently, that day’s #30DayMapChallenge also happened to be “population,” and the JS article didn’t include any maps. So let’s create a map of the population change figures.\r\nThe data\r\nPopulation estimates are produced by the Wisconsin Department of Administration, and they compare current estimates with the 2020 decennial census numbers. The WSJ article uses the “Municipality Final Population Estimates, Alphabetical List” file, and so will we.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(tigris)\r\noptions(tigris_use_cache = TRUE)\r\nlibrary(leaflet)\r\nlibrary(tmap)\r\nlibrary(readxl)\r\nlibrary(sf)\r\n\r\ngrowth <- read_excel(\"data/Final_Ests_Muni_2023.xlsx\", skip = 2) %>% \r\n  janitor::clean_names()\r\nglimpse(growth)\r\n\r\nRows: 1,861\r\nColumns: 11\r\n$ doa_code                 <chr> \"10201\", \"43002\", \"34002\", \"01201\",…\r\n$ fips_5                   <chr> \"00100\", \"00175\", \"00225\", \"00275\",…\r\n$ muni_type                <chr> \"C\", \"T\", \"T\", \"C\", \"T\", \"T\", \"T\", …\r\n$ municipality_name        <chr> \"Abbotsford\", \"Abrams\", \"Ackley\", \"…\r\n$ county                   <chr> \"In Multiple Counties\", \"Oconto\", \"…\r\n$ final_estimate_2023      <dbl> 2375, 2000, 465, 1736, 1379, 543, 1…\r\n$ census_2020              <dbl> 2275, 1960, 467, 1761, 1378, 540, 1…\r\n$ numeric_change           <dbl> 100, 40, -2, -25, 1, 3, 19, -42, -6…\r\n$ percent_change           <dbl> 0.0440, 0.0204, -0.0043, -0.0142, 0…\r\n$ voting_age_estimate_2023 <dbl> 1746, 1579, 385, 1380, 1154, 433, 1…\r\n$ voting_age_census_2020   <dbl> 1665, 1541, 385, 1394, 1148, 429, 1…\r\n\r\nThe article excludes cities with fewer than 30,000 residents. Setting thresholds is always a balancing act, but a threshold of 30,000 residents is quite high. For example, Verona or Middleton, both sizable suburbs of Madison, continue to see large population growth, but their total populations are only about 15,000 and 23,000, respectively. So let’s set a threshold of 15,000 for now and see how it goes.\r\n\r\n\r\nShow code\r\n\r\ngrowth <- growth %>% \r\n  filter(final_estimate_2023 >= 15000) # could also filter on census_2020, but doesn't make a difference\r\n\r\n\r\nNote that the municipality_name variable has some strange numbers after some names. If we look back at the spreadsheet, those are footnotes. This will cause problems when joining the growth data with geographic datasets, and so we’ll either have to clean those names or use the fips_5 variable instead. We will do the latter, as that may also help avoid issues caused by duplicate names or differences in spelling.\r\nNext we need geographic data. As per usual, the tigris package should be easiest.\r\n\r\n\r\nShow code\r\n\r\nwi_places <- places(state = \"WI\", cb = TRUE)\r\nglimpse(wi_places)\r\n\r\nRows: 806\r\nColumns: 13\r\n$ STATEFP    <chr> \"55\", \"55\", \"55\", \"55\", \"55\", \"55\", \"55\", \"55\", \"…\r\n$ PLACEFP    <chr> \"01000\", \"33275\", \"34825\", \"37225\", \"71975\", \"480…\r\n$ PLACENS    <chr> \"01582672\", \"01583366\", \"01583393\", \"01583435\", \"…\r\n$ AFFGEOID   <chr> \"1600000US5501000\", \"1600000US5533275\", \"1600000U…\r\n$ GEOID      <chr> \"5501000\", \"5533275\", \"5534825\", \"5537225\", \"5571…\r\n$ NAME       <chr> \"Algoma\", \"Hawkins\", \"Hillsboro\", \"Ironton\", \"Sca…\r\n$ NAMELSAD   <chr> \"Algoma city\", \"Hawkins village\", \"Hillsboro city…\r\n$ STUSPS     <chr> \"WI\", \"WI\", \"WI\", \"WI\", \"WI\", \"WI\", \"WI\", \"WI\", \"…\r\n$ STATE_NAME <chr> \"Wisconsin\", \"Wisconsin\", \"Wisconsin\", \"Wisconsin…\r\n$ LSAD       <chr> \"25\", \"47\", \"25\", \"47\", \"47\", \"25\", \"25\", \"25\", \"…\r\n$ ALAND      <dbl> 6362282, 5712401, 3638454, 862597, 2262888, 20609…\r\n$ AWATER     <dbl> 114522, 48938, 98551, 3866, 347671, 55411136, 542…\r\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((-87.46487 4..., MULT…\r\n\r\nLet’s do a join.\r\n\r\n\r\nShow code\r\n\r\ngrowth_sf <- wi_places %>% \r\n  inner_join(growth, by = join_by(PLACEFP == fips_5))\r\n\r\n\r\nLooks good! Now we can start mapping, starting with something simple\r\n\r\n\r\nShow code\r\n\r\n# get Wisconsin county map\r\nwi_counties <- counties(state = \"WI\", cb = TRUE)\r\n\r\ntm_shape(wi_counties) +\r\n  tm_polygons() +\r\n  tm_shape(growth_sf) +\r\n  tm_bubbles(col = \"percent_change\", size = \"percent_change\", style = \"jenks\") +\r\n  tm_text(\"NAME\", auto.placement = TRUE, size = .5) +\r\n  tmap::tm_layout(legend.outside = TRUE)\r\n\r\n\r\n\r\nThat’s a good start, but there are too many cities, the breaks don’t quite work and the labeling needs work. I spent a good amount of time trying out different options, but I’ll rest-of-the-f’ing-owl that process. Here’s a decent looking panel of two maps, one for the fastest growing and one for shrinking Wisconsin municipalities.\r\n\r\n\r\nShow code\r\n\r\nshrunk_lg_growth <- growth_sf |> \r\n  filter(percent_change >= 0.04 | percent_change < -0.01) |> \r\n  mutate(abs_percent_change = abs(percent_change),\r\n         shrink_grow = if_else(percent_change < 0, \"shrink\", \"grow\"),\r\n         name_label = paste0(NAME, \" \", scales::label_percent(accuracy = 0.1)(percent_change)))\r\n\r\nbbox_new <- st_bbox(wi_counties) # current bounding box\r\n\r\nxrange <- bbox_new$xmax - bbox_new$xmin # range of x values\r\nyrange <- bbox_new$ymax - bbox_new$ymin # range of y values\r\n\r\n  bbox_new[1] <- bbox_new[1] - (0.1 * xrange) # xmin - left\r\n bbox_new[3] <- bbox_new[3] + (0.1 * xrange) # xmax - right\r\n bbox_new[2] <- bbox_new[2] - (0.02 * yrange) # ymin - bottom\r\n # bbox_new[4] <- bbox_new[4] + (0.5 * yrange) # ymax - top\r\n\r\nbbox_new <- bbox_new %>%  # take the bounding box ...\r\n  st_as_sfc() # ... and make it a sf polygon\r\n\r\n\r\n\r\ntm_shape(wi_counties, bbox = bbox_new ) +\r\n    tm_polygons()+\r\n  tm_shape(shrunk_lg_growth) +\r\n  tm_bubbles(col = \"shrink_grow\", size = \"abs_percent_change\") +\r\n  tm_text(\"name_label\", \r\n          size = .7,\r\n          auto.placement = TRUE) +\r\n  tmap::tm_layout(legend.show = FALSE,\r\n    legend.outside = TRUE, \r\n                  legend.outside.position = \"bottom\",\r\n                  panel.labels = c(\"Municipalities with > 4% growth\", \"Municipalities that lost > 1% population\"))  +\r\n    tm_facets(by = \"shrink_grow\", ncol = 1)\r\n\r\n\r\n\r\nAs you can see, the included municipalities differ from the ones highlighted in the article. This is because of the different population and growth/shrinkage cut-offs. And yet, the overall conclusion of the article can be seen in these maps as well: “A shrinking population in Milwaukee, and other large cities, has been counterbalanced by a growing population in Madison, which is one of the few areas in the state to experience such an increase since 2020.”\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-30-population-growth-in-wisconsin/population-growth-in-wisconsin_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2023-12-02T14:24:03-06:00",
    "input_file": "population-growth-in-wisconsin.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-11-09-winners-and-loser-in-bike-mode-share/",
    "title": "Winners and losers in bike mode share",
    "description": "Where did bike commuting grow, where did it shrink.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-11-09",
    "categories": [
      "American Community Survey",
      "transportation",
      "biking"
    ],
    "contents": "\r\nMiami Beach Life style (Credit: prayitnophotography CC-BY)Bike commuting in Madison isn’t doing so well.\r\nIn my previous post, using American Community Survey (ACS) one-year estimates, we saw that commuting by bike and other cycling metrics have declined in Madison over the last 10 or so years.\r\nThe post also presented some comparisons with national trends and a few other US cities.\r\nI picked those particular cities based on my personal curiosity rather than on a particular metric.\r\nLately I was wondering, though: Are there places where bike commuting actually has grown, and is there something we can learn from those places?\r\nSo this post is a first step in this work.\r\nRather than using one-year estimates, we’ll use five-year estimates because they have smaller margins of errors.\r\nThese estimates aren’t available yet for the 2022 ACS data (they’ll be released in December), and so we’ll compare the 2017-2021 period with 2012-2016.\r\nThe geographic level of analysis will be at the city (or in ACS terminology: place) level.\r\nPlaces can be small, and so there are a lot of them and downloading the data takes a bit of time.\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe’ll exclude small cities and towns, defined by having fewer than 30,000 workers, from the analysis.\r\n\r\n\r\n\r\nNext we focus only on the bike commuting variable and do some data manipulations.\r\nImportantly, we make sure to only include cities where the change in bike commuting is statistically significant at the 90% level.\r\n\r\n\r\n\r\nNow we pick the top and bottom 15 from the remaining list:\r\n\r\n\r\n\r\nWell, actually it turns out that there aren’t 15 cities where the bike mode share has grown over this time period, and so we’ll include the bottom 15 and the cities where there has been growth.\r\nLet’s map the winners and losers:\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe size of the dots represents the bike mode share in the 2012-2016 period; the color represents the change in mode share.1\r\nA lot of the losers are on the west coast, and there are several ones that started out with a large mode share. For example, Davis dropped from about 21 to about 14%. Madison also makes an appearance on the map, with its decrease of 1.6 percentage points. The few winners, on the other hand, are mostly on the east coast, didn’t make huge gains, and most of them both started and ended low.\r\nHere’s a table of the data:\r\n\r\n\r\n\r\n      \r\n        Bike commute mode share\r\n      \r\n      Change (absolute)\r\n    2012-2016\r\n      2017-2021\r\n      Change (%pt.)\r\n    Miami Beach, Florida\r\n4.1%\r\n6.2%\r\n+2.1%\r\n+51%Jersey City, New Jersey\r\n0.4%\r\n0.8%\r\n+0.4%\r\n+100%Gaithersburg, Maryland\r\n0.1%\r\n0.4%\r\n+0.3%\r\n+300%Plantation, Florida\r\n0.1%\r\n0.4%\r\n+0.3%\r\n+300%West Covina, California\r\n0.0%\r\n0.3%\r\n+0.3%\r\n+Inf%Longview, Texas\r\n0.0%\r\n0.3%\r\n+0.3%\r\n+Inf%New York, New York\r\n1.1%\r\n1.4%\r\n+0.3%\r\n+27%Toledo, Ohio\r\n0.2%\r\n0.4%\r\n+0.2%\r\n+100%Boulder, Colorado\r\n10.3%\r\n8.8%\r\n−1.5%\r\n−15%Bloomington, Indiana\r\n3.9%\r\n2.4%\r\n−1.5%\r\n−38%Bozeman, Montana\r\n5.6%\r\n4.0%\r\n−1.6%\r\n−29%Medford, Oregon\r\n2.2%\r\n0.6%\r\n−1.6%\r\n−73%Madison, Wisconsin\r\n5.2%\r\n3.6%\r\n−1.6%\r\n−31%Portland, Oregon\r\n6.5%\r\n4.7%\r\n−1.8%\r\n−28%Santa Barbara, California\r\n4.8%\r\n2.9%\r\n−1.9%\r\n−40%Flagstaff, Arizona\r\n4.4%\r\n2.4%\r\n−2.0%\r\n−45%Chico, California\r\n5.2%\r\n3.1%\r\n−2.1%\r\n−40%Fort Collins, Colorado\r\n6.6%\r\n4.4%\r\n−2.2%\r\n−33%Berkeley, California\r\n8.2%\r\n5.9%\r\n−2.3%\r\n−28%Eugene, Oregon\r\n7.4%\r\n4.8%\r\n−2.6%\r\n−35%Missoula, Montana\r\n7.2%\r\n4.4%\r\n−2.8%\r\n−39%Santa Cruz, California\r\n9.5%\r\n6.2%\r\n−3.3%\r\n−35%Davis, California\r\n21.1%\r\n13.8%\r\n−7.3%\r\n−35%Data: American Community Survey 5-year estimates, Table S0801\r\n    \r\n\r\nTo come back to my initial question: Is there something to be learned from the communities that managed to grow their bike mode share?\r\nBased on what we’re seeing here, I think the only city worth taking a closer look at is Miami Beach.\r\nThe city’s bike commute mode share grew from in 2012-2016 to % in the 2017-2021 period.\r\nAll other cities that saw growth in bike commute share had an increase below 1 percentage point, and the 2021 mode share is below 1.5% for all of them.\r\nA quick web search about Miami Beach shows that they adopted a Bicycle Pedestrian Master Plan and Street Design Guide and, at least according to the city website, “the city has been aggressively implementing lanes ever since. In 2017 Street Plans started working closely with the City of Miami Beach to further design and implement priority ‘quick build’ projects.” The Places for Bikes rating has Miami Beach at 46 points, which is a decent but not outstanding score.\r\nMadison, for example, scores a 58.\r\nOther factors may include Miami Beach’s unique geography on a narrow strip of land, and its character as a resort town dominated by the tourism sector.\r\nWe will see what the most current estimates will bring—based on what I’ve seen with the 2022 one-year estimates, I wouldn’t be surprised if the five-year estimates including 2022 will further reduce the number of cities that have seen growth.\r\nStay tuned: the data will be out soon.\r\n\r\nUnfortunately tmap can’t display two legends in interactive maps.↩︎\r\n",
    "preview": "posts/2023-11-09-winners-and-loser-in-bike-mode-share/img/Miami Beach.jpg",
    "last_modified": "2023-11-18T13:22:36-06:00",
    "input_file": "winners-and-loser-in-bike-mode-share.knit.md"
  },
  {
    "path": "posts/2023-11-07-quick-post-commute-time-in-madison/",
    "title": "Quick post: Commute time in Madison",
    "description": "How long does it take Madisonians to get to work?",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-11-07",
    "categories": [
      "Madison (WI)",
      "American Community Survey",
      "transportation",
      "quick post"
    ],
    "contents": "\r\nThe New York Times had an interesting article on how commutes have changed during the COVID-19 pandemic and after: “Most Americans still have to commute every day. Here’s how that experience has changed.”\r\nThe article includes data only on 20 metro areas in the US. The following chart looks at the average commute length in Madison and compares it with the US trend. Whereas nationally, commutes had gotten longer before the pandemic and then saw a drop and a rebound, in Madison average commute times stay roughly the same before then experiencing a similar drop and rebound. Overall, Madison commuters have a much shorter average commute, with 19.3 minutes compared to the national average of 26.4 minutes in 2022.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidycensus)\r\nlibrary(tidyverse)\r\nlibrary(hrbrthemes)\r\nlibrary(extrafont)\r\n\r\nget_msn_travel_time <- function(year) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"S0801\", geography = \"place\", state = 55, cache_table = T)\r\n  acs_data |>\r\n    filter(NAME == \"Madison city, Wisconsin\") |>\r\n    mutate(year = year)\r\n}\r\n\r\nget_national_travel_time <- function(year) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"S0801\", geography = \"us\", cache_table = T)\r\n  acs_data |>\r\n    mutate(year = year)\r\n}\r\n\r\nvariables <- load_variables(2022, dataset = \"acs1/subject\")\r\nmsn_travel_time <- map_dfr(c(2010:2019, 2021:2022), get_msn_travel_time)\r\nus_travel_time <- map_dfr(c(2010:2019, 2021:2022), get_national_travel_time)\r\n\r\ntravel_time <- rbind(us_travel_time |> mutate(location = \"national\"), msn_travel_time |> mutate(location = \"Madison\"))\r\n\r\n\r\ntravel_time |>\r\n  filter(variable == \"S0801_C01_046\") |>\r\n  mutate(\r\n    estimate_min = estimate - moe,\r\n    estimate_max = estimate + moe,\r\n    plot_label = if_else(year == max(year), location, NA_character_)\r\n  ) |>\r\n  ggplot(aes(year, estimate, color = location)) +\r\n  geom_line() +\r\n  geom_point() +\r\n  geom_ribbon(aes(ymin = estimate_min, ymax = estimate_max),\r\n    alpha = .2, linewidth = 0\r\n  ) +\r\n  geom_text(aes(label = plot_label), nudge_y = 1.5) +\r\n  scale_color_ipsum() +\r\n  scale_x_continuous(breaks = seq(2010, 2022, 2), limits = c(2010, 2022.5)) +\r\n  xlab(\"year\") +\r\n  ylab(\"Commute time (minutes)\") +\r\n  labs(\r\n    title = \"Average commute time for Madison (WI)\\nresidents compared to national average\",\r\n    subtitle = \"Grey band shows margin of error\",\r\n    caption = \"Data: American Community Survey 1-year estimates, Table S0801\\nNo estimate for 2020 available\\nVisualization: Harald Kliems\"\r\n  ) +\r\n  hrbrthemes::theme_ipsum_rc() +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-07-quick-post-commute-time-in-madison/quick-post-commute-time-in-madison_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-11-07T08:43:22-06:00",
    "input_file": "quick-post-commute-time-in-madison.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-11-04-30daymapchallenge-a-bad-map-of-the-us/",
    "title": "#30DayMapChallenge: A \"bad\" map of the US",
    "description": "Places that start with \"bad\" (but not \"badger\"!)",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-11-04",
    "categories": [
      "map"
    ],
    "contents": "\r\nThe theme for today’s #30DayMapChallenge: Bad maps. Inspired by this map of German town’s starting with “Bad” (designating a spa town), I thought I’d do something similar for the US.\r\nWe get places names from the USGS Gazetteer files. Be warned that these are large files that will take a while to download and read.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\nlibrary(tmap)\r\nurl <- \"https://prd-tnm.s3.amazonaws.com/StagedProducts/GeographicNames/FullModel/Gazetteer_National_GDB.zip\"\r\nzip_file <- tempfile(fileext = \".zip\")\r\noptions(timeout=180) #file is large and times out with default timeout\r\ndownload.file(url, zip_file, mode = \"wb\")\r\ntemp_dir <- tempdir()\r\nplaces_dir <- unzip(zipfile = zip_file, exdir = temp_dir)\r\n\r\ngazetteer_places <- st_read(paste0(temp_dir, \"\\\\Gazetteer_National_GDB.gdb\"), layer = \"Gaz_Names\") #layer with names\r\n\r\nReading layer `Gaz_Names' from data source \r\n  `C:\\Users\\user1\\AppData\\Local\\Temp\\RtmpkZQ702\\Gazetteer_National_GDB.gdb' \r\n  using driver `OpenFileGDB'\r\n\r\nShow code\r\n\r\ngazetteer_geo <- st_read(paste0(temp_dir, \"\\\\Gazetteer_National_GDB.gdb\"), layer = \"Gaz_Features\")\r\n\r\nReading layer `Gaz_Features' from data source \r\n  `C:\\Users\\user1\\AppData\\Local\\Temp\\RtmpkZQ702\\Gazetteer_National_GDB.gdb' \r\n  using driver `OpenFileGDB'\r\nSimple feature collection with 980622 features and 19 fields\r\nGeometry type: MULTIPOINT\r\nDimension:     XYZ\r\nBounding box:  xmin: -179.4681 ymin: -85.37423 xmax: 179.9833 ymax: 71.41745\r\nz_range:       zmin: 0 zmax: 0\r\nGeodetic CRS:  NAD83\r\n\r\nShow code\r\n\r\nall_places <- gazetteer_geo |> right_join(gazetteer_places, by = \"feature_id\")\r\n\r\n\r\nNow we start filtering for places starting with “Bad”:\r\n\r\n\r\nShow code\r\n\r\nbad_places <- all_places |> \r\n  filter(str_detect(feature_name, \"^[Bb]ad\"))\r\n\r\n\r\nThis gives us 640 places, but just looking at the names of the first few shows that they’re not all “bad” places:\r\n\r\n\r\nShow code\r\n\r\nhead(bad_places$feature_name)\r\n\r\n[1] \"Bad River\"                    \"Badger Creek\"                \r\n[3] \"Badger Mountain Division\"     \"Bad River Indian Reservation\"\r\n[5] \"Bad River Reservation\"        \"Badwater Creek\"              \r\n\r\nTwo out of the 6 are not “bad” but “Badger.” After browsing through the full list of included names, I came with this heuristic to eliminate most “non-bad” names, such as “Baden,” “Badoff,” or “Badito.”\r\n\r\n\r\nShow code\r\n\r\nbad_places <- bad_places |> \r\n  filter(!str_detect(feature_name, \"^[Bb]ad[degiou]\"))\r\nbad_places |> \r\n  st_drop_geometry() |> \r\n  select(feature_name, feature_class) |> \r\n  DT::datatable()\r\n\r\n\r\n\r\nWhat type of places are most commonly bad?\r\n\r\n\r\nShow code\r\n\r\nbad_places |> \r\n  st_drop_geometry() |> \r\n  group_by(feature_class) |> \r\n  tally(sort = T) |> \r\n  DT::datatable()\r\n\r\n\r\n\r\nStreams!\r\nLet’s map all the 209 bad places:\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(bad_places) +\r\n  tm_dots(\"feature_class\", id = \"feature_name\",\r\n          popup.vars = c(\"Type of feature\" = \"feature_class\")) +\r\n  tm_layout(title = \"Bad Places in the United States\")\r\n\r\n\r\n\r\nYou can click on the points to display the name. We could add the names right on the map, but as bad places cluster together (and in the case of rivers and streams, the same feature shows up multiple times), this makes for a messy map:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(bad_places) +\r\n  tm_dots(\"feature_class\", id = \"feature_name\",\r\n          popup.vars = c(\"Type of feature\" = \"feature_class\")) +\r\n  tm_text(\"feature_name\")\r\n\r\n\r\n\r\nThat’s all for today.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-11-04-30daymapchallenge-a-bad-map-of-the-us/map_screenshot.png",
    "last_modified": "2023-11-04T14:37:25-05:00",
    "input_file": "30daymapchallenge-a-bad-map-of-the-us.knit.md",
    "preview_width": 500,
    "preview_height": 314
  },
  {
    "path": "posts/2023-10-29-quick-post-job-growth-in-dane-county/",
    "title": "Quick post: Job growth in Dane County",
    "description": "Some maps and charts of 2015 and 2019 LEHD data",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-10-29",
    "categories": [
      "Madison (WI)",
      "LEHD",
      "jobs",
      "maps",
      "quick post"
    ],
    "contents": "\r\nI’m trying to write more short blog posts. This is one of them. In the context of my recent post on the state of biking in Madison, I was curious about where Madison and Dane County have seen job growth. One explanation for a declining bike mode share could be that jobs are created at the periphery, where people have longer commutes and those commute routes may lack good bike facilities.\r\nJob data is available from the Longitudinal Employer-Household Dynamics (LEHD) data set via the ledhr R package. Let’s download 2015 and 2019 data (I wanted a pre-pandemic end year), calculate the change in jobs of all kind.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(tmap)\r\nlibrary(tidycensus)\r\nlibrary(lehdr)\r\nlibrary(sf)\r\n\r\n# get dane county geography\r\ndane_county <- tigris::block_groups(55, 25, cb = T)\r\n# job data at the block group level\r\njobs_2019 <- grab_lodes(\r\n  state = \"WI\",\r\n  year = 2019,\r\n  lodes_type = \"wac\", # for workplace geography\r\n  agg_geo = \"bg\",\r\n  use_cache = TRUE\r\n)\r\njobs_2015 <- grab_lodes(\r\n  state = \"WI\",\r\n  year = 2015,\r\n  agg_geo = \"bg\",\r\n  lodes_type = \"wac\",\r\n  use_cache = TRUE\r\n)\r\n\r\n\r\njobs_change <- rbind(jobs_2015, jobs_2019) |>\r\n  select(year, C000, w_bg) |>\r\n  pivot_wider(\r\n    names_from = year,\r\n    values_from = C000, # total number of jobs variable\r\n    names_prefix = \"jobs_\"\r\n  ) |>\r\n  mutate(job_growth = jobs_2019 - jobs_2015)\r\njobs_change <- dane_county |>\r\n  left_join(jobs_change, by = join_by(GEOID == w_bg)) |>\r\n  filter(ALAND != 0) # filter out the lakes\r\n\r\n\r\nNow we can create a choropleth map of those changes:\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\n\r\njobs_change |>\r\n  filter(!is.na(job_growth)) |>\r\n  tm_shape() +\r\n  tm_polygons(\r\n    col = \"job_growth\",\r\n    title = \"Job growth 2015-2019\", style = \"jenks\",\r\n    alpha = .7\r\n  )\r\n\r\n\r\n\r\nThere doesn’t seem to be a clearly discernible patter: Some areas on and near the isthmus have seen job growth, but so do some more suburban areas. Rather than only looking at a map, we can also create a scatterplot of the numbers of new jobs and the distance of the block group from the Wisconsin State Capitol:\r\n\r\n\r\nShow code\r\n\r\n# coordinates for the Capitol\r\npoint_data <- data.frame(\r\n  x = 43.074722,\r\n  y = -89.384167\r\n)\r\n\r\n# Create an sf object with a single point\r\ncapitol_sf <- st_as_sf(point_data, coords = c(\"y\", \"x\"))\r\n\r\n# calculate block group centroids\r\njobs_centroids <- jobs_change |>\r\n  st_centroid()\r\n\r\nst_crs(capitol_sf) <- st_crs(jobs_centroids)\r\n\r\n# st_distance doesn't work well with the tidyverse and we use `map` instead to calculate the distances to the Capitol\r\nx <- map(1:nrow(jobs_centroids), function(x) st_distance(jobs_centroids[x, ], capitol_sf))\r\n\r\njobs_centroids$dist_to_capitol <- as.numeric(x)\r\n\r\njobs_distances <- jobs_centroids |>\r\n  filter(!is.na(job_growth)) |>\r\n  st_drop_geometry() |>\r\n  mutate(dist_to_capitol_mi = dist_to_capitol / 1609.344)\r\n\r\njobs_distances |>\r\n  ggplot(aes(dist_to_capitol_mi, job_growth)) +\r\n  geom_point() +\r\n  theme_minimal() +\r\n  xlab(\"Distance to Capitol (miles)\") +\r\n  ylab(\"Job gain/loss 2015-2019\") +\r\n  labs(title = \"Job growth by distance from Wisconsin State Capitol\")\r\n\r\n\r\n\r\nIt looks like the block groups with the largest changes in either direction are less than five miles from the Capitol, and past 10 miles there aren’t really any block groups that saw large job growth or decline over that time period.\r\nAnother way to look at the data is to create distance bins and tabulate the job changes within each category:\r\n\r\n\r\nShow code\r\n\r\nlibrary(gt)\r\njobs_distances |>\r\n  mutate(dist_bins = cut(dist_to_capitol_mi, breaks = c(0, 5, 10, 15, 20, Inf), labels = c(\"<5 miles\", \"5 to <10 miles\", \"10 to <15 miles\", \"15 to <20 miles\", \">20 miles\"))) |>\r\n  summarise(job_growth_agg = sum(job_growth), .by = dist_bins) |>\r\n  gt() |>\r\n  tab_header(\r\n    title = \"Job growth by distance from Wisconsin State Capitol\",\r\n    subtitle = \"2015 to 2019\"\r\n  ) |>\r\n  cols_label(\r\n    dist_bins = \"Distance\",\r\n    job_growth_agg = \"Growth in jobs\"\r\n  ) |>\r\n  tab_source_note(\"Data: LEHD at block group level aggregation\")\r\n\r\n\r\nJob growth by distance from Wisconsin State Capitol\r\n    2015 to 2019\r\n    Distance\r\n      Growth in jobs\r\n    <5 miles\r\n31685 to <10 miles\r\n1255810 to <15 miles\r\n400415 to <20 miles\r\n466>20 miles\r\n501Data: LEHD at block group level aggregation\r\n    \r\n\r\nThat’s all for now—I think the next step will be to do the same type of analysis for housing growth.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-10-29-quick-post-job-growth-in-dane-county/quick-post-job-growth-in-dane-county_files/figure-html5/distance-scatterplot-1.png",
    "last_modified": "2023-10-29T18:15:02-05:00",
    "input_file": "quick-post-job-growth-in-dane-county.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/the-state-of-biking-in-madison-2022/",
    "title": "The State of Biking in Madison 2022",
    "description": "A look at several indicators of how many people biked (and took other means of transportation).",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-10-28",
    "categories": [
      "Madison (WI)",
      "transportation",
      "American Community Survey",
      "Eco Counter"
    ],
    "contents": "\r\nAnother year of commuting data from the American Community Survey (ACS) was released. This is an opportunity to look at the state of biking in 2022. Last year I did a deep dive into the ACS commuting data, as it was the first year that the data was released after the onset of the pandemic. And indeed: Huge changes in commuting, especially an explosion in working from home, could be seen. However, commuting is only one part of our transportation landscape, and this year my review will also look at several other indicators of the state of biking in Madison, as well as making comparisons with some other US cities and national trends.\r\nCommuting data from ACS: Fewer people work from home; driving and transit usage are up\r\nAfter an unprecedented rise in working from home between 2019 and 2021, some people returned to the office in 2022. The share of working from home declined from over 26% in 2021 to about 20% in 2021. The two modes with the largest increases were driving (58% to 62%; increase within margin of error) and transit (3% to 5%). All other modes didn’t change much at all, and any changes were well within the margins of error. Biking ended up at a miserable 2.6% (+/- 0.7%), the lowest percentage in the data reaching back to 2010.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tmap)\r\nlibrary(sf)\r\nlibrary(tidycensus)\r\nlibrary(tidyverse)\r\nlibrary(gt)\r\nlibrary(ggtext)\r\nlibrary(extrafont)\r\nlibrary(hrbrthemes)\r\nloadfonts(device = \"win\", quiet = TRUE) \r\n\r\n\r\n\r\n# variables <- load_variables(dataset = \"acs1/subject\", year = \"2021\")\r\n\r\nget_msn_mode_share <- function(year) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"S0801\", geography = \"place\", state = 55, cache_table=T)\r\n  acs_data |> \r\n    filter(NAME == \"Madison city, Wisconsin\") |> \r\n    mutate(year = year)\r\n}\r\n\r\n\r\n# variable_readable = case_when(\r\n#   variable == \"S0801_C01_002\" ~ \"Drive\",\r\n#   variable == \"S0801_C01_009\" ~ \"Transit\",\r\n#   variable == \"S0801_C01_010\" ~ \"Walk\",\r\n#   variable == \"S0801_C01_011\" ~ \"Bike\",\r\n#   variable == \"S0801_C01_013\" ~ \"Work from home\",\r\n#   variable == \"S0801_C02_002\" ~ \"Drive, male\",\r\n#   variable == \"S0801_C02_009\" ~ \"Transit, male\",\r\n#   variable == \"S0801_C02_010\" ~ \"Walk, male\",\r\n#   variable == \"S0801_C02_011\" ~ \"Bike, male\",\r\n#   variable == \"S0801_C02_013\" ~ \"Work from home, male\",\r\n#   variable == \"S0801_C03_002\" ~ \"Drive, female\",\r\n#   variable == \"S0801_C03_009\" ~ \"Transit, female\",\r\n#   variable == \"S0801_C03_010\" ~ \"Walk, female\",\r\n#   variable == \"S0801_C03_011\" ~ \"Bike, female\",\r\n#   variable == \"S0801_C03_013\" ~ \"Work from home, female\",\r\n#   \r\n#   \r\n# )\r\n\r\n\r\n\r\nmsn_mode_share <- map_dfr(c(2010:2019, 2021:2022), get_msn_mode_share)\r\n\r\nmsn_mode_share <- msn_mode_share |> \r\n  mutate(gender = case_when(str_detect(variable, \"^S0801_C01\") ~ \"total\",\r\n                            str_detect(variable, \"^S0801_C02\") ~ \"male\",\r\n                            str_detect(variable, \"^S0801_C03\") ~ \"female\"),\r\n         mode_readable = case_when(\r\n           str_detect(variable, \"S0801_C0[1-3]_002\") ~ \"Drive\",\r\n           str_detect(variable, \"S0801_C0[1-3]_009\") ~ \"Transit\",\r\n           str_detect(variable, \"S0801_C0[1-3]_010\") ~ \"Walk\",\r\n           str_detect(variable, \"S0801_C0[1-3]_011\") ~ \"Bike\",\r\n           str_detect(variable, \"S0801_C0[1-3]_013\") ~ \"Work from home\"))\r\n        \r\n# data frame for the ggrepel labels on the right of the plot\r\nmsn_mode_share_2022 <-  msn_mode_share |> \r\n  filter(year == 2022 & !is.na(mode_readable))\r\n\r\nmsn_mode_share |> \r\n  filter(!is.na(mode_readable) & gender == \"total\") |> \r\n  group_by(mode_readable, year) |> \r\n  ggplot(aes(year, estimate, color = mode_readable)) +\r\n  geom_line(linewidth = 1.2) +\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  geom_crossbar(aes(ymin = estimate - moe, ymax = estimate + moe), alpha = .9,\r\n                  fatten = 1) +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_x_continuous(breaks = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2021, 2022), minor_breaks = NULL, limits = c(2010, 2023.5)) +\r\n  ylab(\"estimate (%)\") +\r\n  labs(title = \"Some Madisonians returned to the office in 2022,\\nand they did so by car or bus\",\r\n       subtitle =\"City of Madison commute mode share and margin of error\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: Harald Kliems\") +\r\n    ggrepel::geom_text_repel(data = msn_mode_share_2022 |> filter(gender == \"total\"), aes(label = paste0(mode_readable, \" \", estimate, \"%\")), nudge_x = 1) +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nThe following chart zooms in on the the changes between 2021 and 2022, which makes it easier to see which changes are beyond the margin of error:\r\n\r\n\r\nShow code\r\n\r\nmsn_mode_share |> \r\n  filter(!is.na(mode_readable) & gender == \"total\" & year >= 2019) |> \r\n  group_by(mode_readable, year) |> \r\n  pivot_wider(names_from = year, values_from = c(estimate, moe)) |> \r\n  mutate(diff_22_21 = estimate_2022-estimate_2021,\r\n         diff_22_21_formatted = paste0(round(diff_22_21,digits = 1), \"%\")) |>\r\n  ggplot() +\r\n    geom_segment(aes(x=mode_readable, xend=mode_readable, y=estimate_2021, yend=estimate_2022), color=\"grey\", arrow = arrow(length = unit(2, \"mm\"))) +\r\n  geom_point(aes(x = mode_readable, y = estimate_2021), color = \"#d18975\") +\r\n    geom_crossbar(aes(mode_readable, estimate_2021, ymin = estimate_2021 - moe_2021, ymax = estimate_2021 + moe_2021), alpha = .6,\r\n                  fatten = 1, color = \"#d18975\")+\r\n    geom_point(aes(x = mode_readable, y = estimate_2022), color = \"#8fd175\") +\r\n      geom_crossbar(aes(mode_readable, estimate_2022, ymin = estimate_2022 - moe_2022, ymax = estimate_2022 + moe_2022), alpha = .6,\r\n                  fatten = 1, color = \"#8fd175\")+\r\n  geom_label(aes(x = mode_readable, \r\n                y = (estimate_2022 + estimate_2021)/2, \r\n                label = diff_22_21_formatted),\r\n            nudge_x = .4, label.padding = unit(.1, \"lines\"))+\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  hrbrthemes::theme_ipsum() +\r\n  coord_flip() +\r\n  theme(panel.grid.major.y = element_blank()) +\r\n  ylab(\"estimate (%)\") +\r\n  xlab(element_blank()) +\r\n  labs(title = \"Working from home is down; transit commutes are up\",\r\n       subtitle =\"City of Madison, 2021-2022. Boxes show margin of error\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: Harald Kliems\") \r\n\r\n\r\n\r\nAs always, when talking about ACS data, keep in mind that commuting is defined as how the survey respondent “usually” got to work in the past week. Especially now that many workers are in hybrid work arrangements, there is additional uncertainty about the interpretation of the numbers.\r\nHow does Madison compare to US-wide trends?\r\nIf we look at change over time, it seems like a good idea to ask whether Madison is following a unique path. The following chart compares commuting in Madison with the US as a whole:\r\n\r\n\r\nShow code\r\n\r\nget_national_mode_share <- function(year) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"S0801\", geography = \"us\", cache_table=T)\r\n  acs_data |> \r\n    mutate(year = year)\r\n}\r\n\r\n\r\n# variable_readable = case_when(\r\n#   variable == \"S0801_C01_002\" ~ \"Drive\",\r\n#   variable == \"S0801_C01_009\" ~ \"Transit\",\r\n#   variable == \"S0801_C01_010\" ~ \"Walk\",\r\n#   variable == \"S0801_C01_011\" ~ \"Bike\",\r\n#   variable == \"S0801_C01_013\" ~ \"Work from home\",\r\n#   variable == \"S0801_C02_002\" ~ \"Drive, male\",\r\n#   variable == \"S0801_C02_009\" ~ \"Transit, male\",\r\n#   variable == \"S0801_C02_010\" ~ \"Walk, male\",\r\n#   variable == \"S0801_C02_011\" ~ \"Bike, male\",\r\n#   variable == \"S0801_C02_013\" ~ \"Work from home, male\",\r\n#   variable == \"S0801_C03_002\" ~ \"Drive, female\",\r\n#   variable == \"S0801_C03_009\" ~ \"Transit, female\",\r\n#   variable == \"S0801_C03_010\" ~ \"Walk, female\",\r\n#   variable == \"S0801_C03_011\" ~ \"Bike, female\",\r\n#   variable == \"S0801_C03_013\" ~ \"Work from home, female\",\r\n#   \r\n#   \r\n# )\r\n\r\n\r\n\r\nnational_mode_share <- map_dfr(c(2010:2019, 2021:2022), get_national_mode_share)\r\n\r\nnational_mode_share <- national_mode_share |> \r\n  mutate(gender = case_when(str_detect(variable, \"^S0801_C01\") ~ \"total\",\r\n                            str_detect(variable, \"^S0801_C02\") ~ \"male\",\r\n                            str_detect(variable, \"^S0801_C03\") ~ \"female\"),\r\n         mode_readable = case_when(\r\n           str_detect(variable, \"S0801_C0[1-3]_002\") ~ \"Drive\",\r\n           str_detect(variable, \"S0801_C0[1-3]_009\") ~ \"Transit\",\r\n           str_detect(variable, \"S0801_C0[1-3]_010\") ~ \"Walk\",\r\n           str_detect(variable, \"S0801_C0[1-3]_011\") ~ \"Bike\",\r\n           str_detect(variable, \"S0801_C0[1-3]_013\") ~ \"Work from home\"))\r\n        \r\n# data frame for the ggrepel labels on the right of the plot\r\nnational_mode_share_2022 <-  national_mode_share |> \r\n  filter(year == 2022 & !is.na(mode_readable))\r\n\r\ncombined <- rbind(national_mode_share |> mutate(location = \"national\"), msn_mode_share |> mutate(location = \"Madison\"))\r\ncombined_2022 <- rbind(national_mode_share_2022 |> mutate(location = \"national\"), msn_mode_share_2022 |> mutate(location = \"Madison\"))\r\n\r\ncombined |> \r\n  filter(!is.na(mode_readable) & gender == \"total\") |> \r\n  group_by(mode_readable, year, location) |> \r\n  ggplot(aes(year, estimate, color = mode_readable)) +\r\n  geom_line(size = 1.2) +\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  # geom_crossbar(aes(ymin = estimate - moe, ymax = estimate + moe), alpha = .9, fatten = 1) +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_x_continuous(breaks = c(2010, 2015, 2022), limits = c(2010, 2023.5)) +\r\n  ylab(\"estimate (%)\") +\r\n  labs(title = \"Mode share trends in Madison follow\\na similar pattern as the national numbers\",\r\n       subtitle =\"City of Madison and US commute mode share, 2010-2022\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: Harald Kliems\") +\r\n    ggrepel::geom_text_repel(data = combined_2022 |> filter(gender == \"total\"), aes(label = paste0(mode_readable, \" \", estimate, \"%\")), nudge_x = 1) +\r\n  # theme(legend.position = \"none\", plot.title = element_text(lineheight = .5, margin = margin(b = 0.5)), plot.subtitle = element_text(margin = margin(t = 0))) +\r\n  facet_wrap(~location) +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nA similar pattern emerges. While Madison commuters drive less, bike, walk, and bus more than the average US commuter, the trend over time looks similar. And that’s true for the change from 2021 to 2022 as well: Just like in Madison, nationwide working from home went down, and driving and transit rebounded slightly.\r\nHow are other cities doing?\r\nRather than comparing Madison with the US as whole, we can also compare our 2022 numbers with other US cities. Here we see a lot of variability, but when it comes to biking, other places aren’t doing much better. San Francisco and Portland have marginally higher bike commuting rates, and Minneapolis is a little lower than Madison, but they all are within the margin error. New York City is below 2%, and in Milwaukee not even 1 out of 100 workers bike to work.\r\n\r\n\r\nShow code\r\n\r\nget_place_mode_share <- function(year, places) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"S0801\", geography = \"place\", cache_table=T)\r\n  acs_data |> \r\n    filter(NAME %in% places) |> \r\n    mutate(year = year)\r\n}\r\n\r\nnational_mode_share_2022_total <- national_mode_share_2022 %>% \r\n  filter(gender == \"total\") %>% \r\n  mutate(\r\n    label = case_when(\r\n           mode_readable == \"Drive\" ~ \"<span style='font-family:fa-solid;'>&#xf1b9;<\/span>\",\r\n           mode_readable == \"Transit\" ~ \"<span style='font-family:fa-solid;'>&#xf207;<\/span>\",\r\n           mode_readable == \"Walk\" ~ \"<span style='font-family:fa-solid;'>&#xf554;<\/span>\",\r\n           mode_readable == \"Bike\" ~ \"<span style='font-family:fa-solid;'>&#xf206;<\/span>\",\r\n           mode_readable == \"Work from home\" ~ \"<span style='font-family:fa-solid;'>&#xe066;<\/span>\"\r\n  ),\r\n  NAME = \"US\")\r\n\r\nmode_share_comparison <- get_place_mode_share(2022, c(\"Madison city, Wisconsin\", \"Portland city, Oregon\", \"Minneapolis city, Minnesota\", \"San Francisco city, California\", \"Milwaukee city, Wisconsin\", \"New York city, New York\"))\r\n\r\nmode_share_comparison <- mode_share_comparison |> \r\n  mutate(gender = case_when(str_detect(variable, \"^S0801_C01\") ~ \"total\",\r\n                            str_detect(variable, \"^S0801_C02\") ~ \"male\",\r\n                            str_detect(variable, \"^S0801_C03\") ~ \"female\"),\r\n         mode_readable = case_when(\r\n           str_detect(variable, \"S0801_C0[1-3]_002\") ~ \"Drive\",\r\n           str_detect(variable, \"S0801_C0[1-3]_009\") ~ \"Transit\",\r\n           str_detect(variable, \"S0801_C0[1-3]_010\") ~ \"Walk\",\r\n           str_detect(variable, \"S0801_C0[1-3]_011\") ~ \"Bike\",\r\n           str_detect(variable, \"S0801_C0[1-3]_013\") ~ \"Work from home\"),\r\n         label = case_when(\r\n           mode_readable == \"Drive\" ~ \"<span style='font-family:fa-solid;'>&#xf1b9;<\/span>\",\r\n           mode_readable == \"Transit\" ~ \"<span style='font-family:fa-solid;'>&#xf207;<\/span>\",\r\n           mode_readable == \"Walk\" ~ \"<span style='font-family:fa-solid;'>&#xf554;<\/span>\",\r\n           mode_readable == \"Bike\" ~ \"<span style='font-family:fa-solid;'>&#xf206;<\/span>\",\r\n           mode_readable == \"Work from home\" ~ \"<span style='font-family:fa-solid;'>&#xe066;<\/span>\"\r\n         ),\r\n         NAME = str_remove(NAME, \" city\")) |> \r\n    filter(!is.na(mode_readable) & gender == \"total\")\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(showtext)\r\n\r\nshowtext_auto()\r\n# First argument = name in R\r\n# Second argument = path to .otf-file\r\nfont_add('fa-reg', 'fonts/Font_Awesome_6_Free-Regular-400.otf')\r\nfont_add('fa-brands', 'fonts/Font_Awesome_6_Brands-Regular-400.otf')\r\nfont_add('fa-solid', 'fonts/Font_Awesome_6_Free-Solid-900.otf')\r\nfont_add(\"Arial Narrow\", \"C:/Windows/Fonts/ARIALN.TTF\")\r\nfont_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\r\n\r\np <- rbind(mode_share_comparison, national_mode_share_2022_total) %>% \r\n  mutate(name_by_bike_share = fct_relevel(NAME, c(\"San Francisco, California\", \r\n                        \"Portland, Oregon\",\r\n                        \"Madison, Wisconsin\", \r\n                        \"Minneapolis, Minnesota\", \r\n                        \"New York, New York\",\r\n                        \"Milwaukee, Wisconsin\",\r\n                        \"US\"))) %>% \r\n  group_by(mode_readable, name_by_bike_share) |> \r\n  ggplot(aes(name_by_bike_share, estimate, color = mode_readable, label = label)) +\r\n    geom_richtext(size = 8, label.colour = NA, fill = NA) +\r\n    geom_point(size = 2, color = \"red\") +\r\n  hrbrthemes::scale_color_ipsum() +\r\n  scale_fill_discrete() +\r\n  hrbrthemes::theme_ipsum() +\r\n  # scale_x_continuous(breaks = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2021, 2022), minor_breaks = NULL, limits = c(2010, 2023.5)) +\r\n  ylab(\"estimate (%)\") +\r\n  xlab(element_blank()) +\r\n  labs(title = \"2022 Commute mode share\",\r\n       subtitle = \"Madison and select US cities\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: Harald Kliems\") +\r\n    # ggrepel::geom_text_repel(data = combined_2022 |> filter(gender == \"total\"), aes(label = paste0(mode_readable, \" \", estimate, \"%\")), nudge_x = 1) +\r\n  theme(legend.position = \"none\",\r\n        panel.grid.major.y = element_blank()) +\r\n  coord_flip() +\r\n  theme(plot.title = element_text(lineheight = .5, margin = margin(b = .5)), plot.subtitle = element_text(margin = margin (t = 0)), plot.caption = element_text(lineheight = .5))\r\n  # facet_wrap(~NAME)\r\np\r\n\r\n\r\n\r\nCounter data 2022\r\nCommuting data from the American Community Survey is only part of the picture. Another indicator of the state of biking comes from the city’s two permanent visual bike counters. These are located on the Southwest Path near Camp Randall and on the Cap City Trail at the intersection of John Nolen and North Shore. Rather than only counting commutes, these counters capture all bike trips.\r\n\r\n\r\nShow code\r\n\r\nshowtext.auto(enable = FALSE)\r\ndetach(\"package:showtext\", unload = TRUE)\r\ncounts_2022 <- readxl::read_excel(\"data/EcoCounter_2022.xlsx\", skip = 3,\r\n                   col_names = c(\"time_count\", \"count_cap_city\", \"count_sw_path\")) |> \r\n  mutate(date_count = floor_date(time_count, unit = \"hours\")) |> \r\n  summarize(across(starts_with(\"count_\"), ~ sum(.x, na.rm = T)), .by = date_count) |> \r\n  pivot_longer(cols = starts_with(\"count_\"), names_to = \"location\", values_to = \"count_hourly\") |> \r\n  mutate(location = case_when(location == \"count_cap_city\" ~ \"Cap City at North Shore\",\r\n                              location == \"count_sw_path\" ~ \"SW Path at Randall\"),\r\n         dayofweek = wday(date_count),\r\n         weekendind = ifelse(dayofweek %in% c(1:5), \"weekday\", \"weekend\"),\r\n         month_count = month(date_count, label = T, abbr = T)) \r\n\r\n\r\n# code reused from https://github.com/vgXhc/madison_bike_counts/blob/master/analysis.R\r\n\r\n##get bike counter data\r\ncc_counts <- read_csv(\"data/Eco-Totem_Capital_City_Trail_Bike_Counts(3).csv\", col_types = \"ci-\") %>% mutate(location = \"Cap City at North Shore\")\r\nsw_counts <- read_csv(\"data/Eco-Totem_Southwest_Path_Bike_Counts(2).csv\", col_types = \"ci-\") %>% mutate(location = \"SW Path at Randall\")\r\n#combine two counter locations\r\ncounts <- bind_rows(cc_counts, sw_counts)\r\n#some data prep for counts\r\ncounts2 <- counts %>% \r\n  drop_na %>% \r\n  mutate(date_count = mdy_hm(Count_Date), #fix date and time\r\n         location = as.factor(location),\r\n         # Count = ifelse(Count == 0, 1, Count), #convert 0 counts to 1 to allow log transform\r\n         # log_count = log(Count), #create value for log of count\r\n         dayofweek = wday(date_count),\r\n         weekendind = ifelse(dayofweek %in% c(1:5), \"weekday\", \"weekend\"),\r\n         month_count = month(date_count, label = T, abbr = T)\r\n         ) |> \r\n  select(-Count_Date) |> \r\n  rename(count_hourly = Count) |> \r\n  filter(date_count < ymd_hms(\"2022-01-01 00:00:00\"))\r\n\r\n\r\nrbind(counts2, counts_2022) |> \r\n  mutate(year_count = year(date_count)) |> \r\n  filter(year_count >= 2016) |> \r\n  summarize(count_annual_by_location = sum(count_hourly), .by = c(location, year_count)) |>\r\n  reframe(location, year_count, count_annual_by_location, count_annual = sum(count_annual_by_location), .by = year_count) |> \r\n  ggplot(aes(year_count, count_annual_by_location, fill = location)) +\r\n  geom_col(position = \"stack\") +\r\n  geom_text(\r\n    aes(label = after_stat(y), group = year_count), \r\n    stat = 'summary', fun = sum, vjust = -.3, family = \"Roboto Condensed\"\r\n  ) +\r\n  ylim(c(0, 1050000)) +\r\n  scale_fill_ipsum(name = \"Location\") +\r\n  hrbrthemes::theme_ipsum() +\r\n  ylab(\"Number of cyclists\") +\r\n  xlab(\"Year\") +\r\n  labs(title = \"Bike counts declined by 24% between 2016 and 2022\",\r\n       subtitle = \"Counts at permanent Eco Counter locations\",\r\n       caption = \"Data: City of Madison\\nVisualization: Harald Kliems\") +\r\n  theme(legend.position = \"right\")\r\n\r\n\r\n\r\nThese counts don’t paint a rosy picture for biking either. Since the counters were installed, they have counted fewer and fewer people on bikes. While in 2016 we had almost one million counts, by 2022 that number has declined to less than 750,000.\r\nStreetlight data\r\nThe American Community Survey only counts commutes; the Eco Counters only count at two locations. Streetlight, a private company, promises to provide comprehensive counts for the whole metro area. They do this by using cell phone and other location data and applying various machine learning mechanisms to estimate bike trips. The data and algorithms are proprietary, but research published by Streetlight claims a high correlation between their data and validation counts from permanent bike counters.\r\nIn a recently released report, Streetlight compares 100 US metro areas1 and how cycling activity has changed there compared to 2019, and then between 2020 and 2022. The idea is that during the COVID pandemic many US cities saw a surge in cycling, and now we have an opportunity to see if that surge was sustainable. In terms of growth from 2019 to 2020, the Madison metro area did not see a surge: We ranked 86th out of 100 metro areas in terms of growth. And from 2020 to 2022, Madison fell even further, to 98th place, ahead only of Portland, OR, and Fresno, CA.\r\nHere’s the map showing the change in cycling activity between 2019 and 2022. You can see how Madison (unlabeled on the map) is one of the few metro areas that actually saw a decline during this time period.\r\nBicycle trip growth in the top 100 metro areas, 2019 to 2022A single sign of optimism? BCycle ridership\r\nSo far all we have seen is doom and gloom. Are there any signs for optimism? Yes: The one indicator that has seen growth in the past few years is use of Madison’s bike share system BCycle. I don’t have complete ridership stats over the years, by Madison BCycle shared this graph of March to September ridership over the past 5 years.2\r\nMadison BCycle March to September ridershipAdding up the numbers shows a 3.6% growth from 222,000 trips in 2021 to 230,000 trips in 2022 during the March to September time period. Of course the more exciting part of this graph is the growth in 2023, but hey, this post is about 2022.\r\nWhere does this leave us?\r\nI don’t like it, but it appears that the conclusion for 2022 has to be that it was another lost year for biking in Madison. Yes, compared to most other US cities, there is still a lot of biking, but for many years we have lacked any growth and likely have seen a decline in how many people ride their bikes and how often they do it. This post doesn’t go into possible explanations, but that work needs to be done in order to figure out how we can turn this trend around.\r\n\r\nStreelight appears to use metropolitan statistical areas. It’s important to note that these are much larger than what most people imagine a metro area to be. In the case of Madison, the area encompasses Dane, Columbia, Green and Iowa counties.↩︎\r\nThe BCycle system is open from mid-March to mid-December.↩︎\r\n",
    "preview": "posts/the-state-of-biking-in-madison-2022/state_biking_five_charts_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-10-29T10:01:52-05:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 830
  },
  {
    "path": "posts/2023-05-31-ntdr-an-r-package-for-transit-data/",
    "title": "`ntdr 0.3.0`: Now available on CRAN",
    "description": "An R package for National Transit Database data",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-05-31",
    "categories": [
      "transit",
      "National Transit Database",
      "ntdr"
    ],
    "contents": "\r\nFor the past few months I have been working on my first R package. Like with many packages, my motivation came from repeatedly doing the same data cleaning operations, getting annoyed by it, and then writing code to make it less annoying. Now the package is available on CRAN and it’s time for a brief post about the package.\r\nWhat the package does\r\nntdr currently only has one purpose: Retrieve the latest monthly National Transit Database ridership data and provide it in a tidy data format. Rather than downloading an unwieldy Excel file with hundreds of columns, just use the get_ntd() command to get data on ridership and related metrics.\r\nHow do I use the package\r\nThe best place to start is the package’s website, especially the “Get Started” article. The package itself is simple, but the article provides some useful information about the NTD data and its quirks and limitations. Here’s a very simple example to install the package, retrieve bus ridership in Madison, and plot the data:\r\n\r\n\r\n\r\nFeedback appreciated\r\nThis is my very first package, and I appreciate feedback, feature requests and bug reports!\r\nAcknowledgments\r\nThis package would not exist or be on CRAN without the wonderful “R Packages” book by Hadley Wickham and Jennifer Bryan. Writing a package was intimidating for me, but the book systematically walks you through every step of the process and the tools that make the process quite straightforward in the end. Even the submission to CRAN – about which you can read many frustrating stories – turned out to be easy. Just follow the process outlined in the book!\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-31-ntdr-an-r-package-for-transit-data/ntdr-an-r-package-for-transit-data_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-05-31T07:47:16-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-05-23-rent-and-rent-burden-in-madison/",
    "title": "Rent and rent burden in Madison, Wisconsin",
    "description": "Some quick charts for Madison and how it compares to other US cities",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-05-23",
    "categories": [
      "housing",
      "Madison (WI)",
      "American Community Survey"
    ],
    "contents": "\r\nLast week Allison Garfield published an article in the Cap Times, asking why Madison rents are rising so fast and won’t slow down. It’s a great article about the strain that rising rents have put on residents and what the city can and cannot do about it. For rent data, the article relies on a report from Apartment List, a commercial apartment listing platform:\r\n\r\nA national study from Apartment List in March found that rent prices in Madison have jumped 14.1% over the past year and 30.4% since March 2020. It was the fastest-rising rent of any major city in the United States, the study reported. The average one-bedroom apartment in January 2017 rented for $873. In April 2023, that number clocks in at $1,322.\r\n\r\nYou can read more on how Apartment List calculates their numbers here. I was curious how the Apartment List numbers compare to data from the American Community Survey. The American Community Survey provides a lot of data about housing, and we can look at the data going back almost 20 years.\r\nWhat follows are a number of charts about rents and renting in Madison and in other large cities in the US.\r\nRenters\r\nFirst, how many people in Madison actually are renters?\r\n\r\n\r\nShow code\r\n\r\nget_renters <- function(acs_year){\r\n  get_acs(\"place\", \r\n          table = \"B25003\", \r\n          year = acs_year, \r\n          survey = \"acs1\",\r\n          summary_var = \"B25003_001\",\r\n          cache_table = TRUE)\r\n}\r\n\r\nyears <- c(2005:2019,2021) # 2020 has no 1y estimates\r\nnames(years) <- years\r\n\r\nrenters <- map_dfr(years, get_renters, .id = \"year\")\r\n\r\nrenters |> \r\n  filter(NAME == \"Madison city, Wisconsin\") |> \r\n  filter(variable == \"B25003_003\") |> \r\n  mutate(renters_pct = estimate / summary_est) |> \r\n  ggplot(aes(as.numeric(year), renters_pct)) +\r\n  geom_line() +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_y_continuous(labels = label_percent()) +\r\n  labs(title = \"About half of households in Madison are renters\",\r\n       x = \"year\",\r\n       y = \"renter households\",\r\n       caption = \"Data: American Community Survey 1-year estimates B25003 \\nVisualization: Harald Kliems\")\r\n\r\n\r\n\r\nMadison is pretty much split in half: 51% of households rented in 2021.\r\nWe can compare this to the 50 most populous cities in the US:\r\n\r\n\r\nShow code\r\n\r\n# get population\r\npop <- get_acs(\r\n  \"place\",\r\n  table = \"B01003\",\r\n  survey = \"acs1\",\r\n  cache_table = TRUE\r\n)\r\n\r\nlargest_50 <- pop |> \r\n  arrange(desc(estimate)) |> \r\n  head(50) |> \r\n  pull(GEOID)\r\n\r\n\r\n\r\nrenters_50 <- renters |> \r\n  filter(GEOID %in% largest_50 | NAME == \"Madison city, Wisconsin\") |> \r\n  mutate(city = str_replace(NAME, \" city\", \"\")) |> \r\n  filter(variable == \"B25003_003\") |> \r\n  mutate(renters_pct = estimate / summary_est)\r\n\r\nmin_max_cities <- renters_50 |> \r\n  filter(year == 2021) |> \r\n  filter(renters_pct == min(renters_pct) | renters_pct == max(renters_pct)) |> \r\n  pull(city)\r\n  \r\nrenters_50 |> \r\n  ggplot(aes(as.numeric(year), renters_pct, group = city)) +\r\n  geom_line() +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_y_continuous(labels = label_percent()) +\r\n  labs(title = \"Percentage of renter households in Madison and 50 most populous US cities\",\r\n       x = \"year\",\r\n       y = \"renter households\",\r\n       caption = \"Data: American Community Survey 1-year estimates B25003 \\nVisualization: Harald Kliems\") +\r\n  xlim(c(2005, 2024)) +\r\n  gghighlight(city %in% c(min_max_cities, \"Madison, Wisconsin\"))\r\n\r\n\r\n\r\nMadison is somewhere in the middle. Miami had the highest percentage of renters (70%) in 2021, whereas Virginia Beach had the lowest (35%).\r\nRent\r\nHow much rent do these households pay? The American Community Survey reports on “gross median rent,” which is “the sum of contract rent and the average cost of the utilities (electricity, gas, and water and sewer) and fuel (oil, coal, kerosene, wood, etc).”\r\n\r\n\r\nShow code\r\n\r\nget_rents <- function(acs_year){\r\n  get_acs(\"place\", \r\n          table = \"B25064\", \r\n          year = acs_year, \r\n          survey = \"acs1\",\r\n          cache_table = TRUE) |> \r\n    rename(rent = estimate)\r\n}\r\n\r\n\r\nrents <- map_dfr(years, get_rents, .id = \"year\")\r\n\r\nmadison_rent <- rents |> \r\n  filter(NAME == \"Madison city, Wisconsin\") \r\n\r\nmadison_rent |> \r\n  ggplot(aes(x = as.numeric(year), y = rent)) +\r\n  geom_line() +\r\n  scale_y_continuous(labels = label_dollar()) +\r\n  hrbrthemes::scale_color_ipsum() +\r\n  labs(title = paste0(\"Median rent in Madison increased \\nfrom $\",\r\n                      madison_rent$rent[[1]],\r\n                      \" in 2005 to $\",\r\n                      madison_rent$rent[[nrow(madison_rent)]],\r\n                      \" in 2021\"),\r\n       x = \"Year\",\r\n       y = \"median gross rent\",\r\n       caption = \"Data: American Community Survey 1-year estimates B25064\\nVisualization: Harald Kliems\") +\r\n  hrbrthemes::theme_ipsum() \r\n\r\n\r\nShow code\r\n\r\n  # xlim(c(2005, 2023)) +\r\n  # theme(legend.position = \"none\",\r\n  #       plot.margin = unit(c(1,1,1,1), \"cm\")) +\r\n  # geom_text(aes(x = 2021.2, y = 3532, label = \"income\"), hjust = \"left\") +\r\n  # geom_text(aes(x = 2021.2, y = 1237, label = \"rent\"), hjust = \"left\")\r\n\r\n\r\nUnsurprisingly, rent has gone up a lot since 2005. The median renter now pays 63% more than they did in 2005. Again, we can compare Madison with the 50 most populous US cities:\r\n\r\n\r\nShow code\r\n\r\nrents_50 <- rents |> \r\n  filter(GEOID %in% largest_50 | NAME == \"Madison city, Wisconsin\") |> \r\n  mutate(city = str_replace(NAME, \" city\", \"\"))\r\n\r\nmin_max_rent_cities <- rents_50 |> \r\n  filter(year == 2021) |> \r\n  filter(rent == min(rent) | rent == max(rent)) |> \r\n  pull(city)\r\n  \r\nrents_50 |> \r\n  ggplot(aes(as.numeric(year), rent, group = city)) +\r\n  geom_line() +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_y_continuous(labels = label_dollar()) +\r\n  labs(title = \"Rent is lowest in Wichita, highest in San Jose\",\r\n       x = \"year\",\r\n       y = \"median gross rent\",\r\n       caption = \"Data: American Community Survey 1-year estimates B25064 \\nVisualization: Harald Kliems\") +\r\n  xlim(c(2005, 2025)) +\r\n  gghighlight(city %in% c(min_max_rent_cities, \"Madison, Wisconsin\"))\r\n\r\n\r\n\r\nThere’s a huge spread in the median rent: In 2021, the median rent in Wichita was about $850, whereas in San Jose it was $2300!\r\nIf we’re interested in change over time, it makes sense to look at an indexed version of this data: We set the rent in the year 2005 to a value of 100 in each of the cities and then look at how that index changes over time.\r\n\r\n\r\nShow code\r\n\r\n# function for indexing: \r\nindex_function <- function(x){\r\n  # input: x, a vector\r\n  # output: x_indexed, a vector \r\n  initial <- x[1]\r\n  map_dbl(x, function(x){x/initial * 100})\r\n}\r\n\r\n\r\n\r\nrents_50 |> \r\n  group_by(NAME) |> \r\n  mutate(city = str_replace(NAME, \" city\", \"\")) |> \r\n  mutate(rent_index = index_function(rent)) |> \r\n  ggplot(aes(as.numeric(year), rent_index, group = city)) +\r\n  geom_line() +\r\n  scale_y_continuous() +\r\n  xlim(c(2005, 2023))+\r\n  gghighlight::gghighlight(city %in% c(\"Madison, Wisconsin\",\r\n                                       \"Seattle, Washington\",\r\n                                       \"Detroit, Michigan\"), \r\n                           label_params = list(linewidth = 10)) +\r\n  hrbrthemes::theme_ipsum() +\r\n  xlab(\"year\") +\r\n  ylab(\"indexed rent\") +\r\n  labs(title = \"Median rent in Madison compared to 50 largest US cities\",\r\n       subtitle = \"Indexed to 2005 rent\",\r\n       caption = \"Data: American Community Survey 1-year estimates\\nVisualization: Harald Kliems\")\r\n\r\n\r\n\r\nAs we see above, all cities are on an upward trajectory, but that trajectory is much steeper for some cities than for others: In Seattle, the indexed rent is at 220 in 2021, i.e. people now pay 2.2 times as much as they did in 2005. At the bottom of the chart is Detroit, where rent now is about 1.4 times what is was in 2005. As we already saw in the previous chart, Madison’s increases are at about 1.6 times.\r\nRent and income\r\nLooking at rent alone is not enough. We should also look at income: Rising rents are less of an issue when incomes rise accordingly. (Of course it’s more complicated than that. For example, an influx of new high-income residents plus stagnating incomes for current residents can still result in displacement)\r\n\r\n\r\nShow code\r\n\r\nget_renter_income <- function(acs_year){\r\n  get_acs(\"place\",\r\n        variable = \"B25119_003\",\r\n        year = acs_year,\r\n        survey = \"acs1\",\r\n        cache_table = TRUE) |> \r\n    rename(income = estimate) |> \r\n    mutate(income = income/12)\r\n}\r\n\r\nrenter_income <- map_dfr(years, get_renter_income, .id = \"year\")\r\n\r\nrent_and_income <- rents |> \r\n  left_join(renter_income, by = c(\"NAME\", \"year\")) |> \r\n  select(year, rent, income, NAME) |> \r\n  pivot_longer(c(\"rent\", \"income\"), names_to = \"name\", values_to = \"value\")\r\n\r\nrent_and_income |> \r\nfilter(NAME == \"Madison city, Wisconsin\") |> \r\nggplot(aes(x = as.numeric(year), y = value, color = name)) +\r\n  geom_line() +\r\n  hrbrthemes::scale_color_ft() +\r\n  scale_y_continuous(labels = label_dollar(), limits = c(500, 4000)) +\r\n  labs(title = \"Median rent and income in Madison\\nboth have gone up between 2005 and 2021\",\r\n       x = \"Year\",\r\n       y = element_blank(),\r\n       caption = \"Data: American Community Survey 1-year estimates\\nVisualization: Harald Kliems\") +\r\n  hrbrthemes::theme_ipsum() +\r\n  xlim(c(2005, 2023)) +\r\n  theme(legend.position = \"none\",\r\n        plot.margin = unit(c(1,1,1,1), \"cm\")) +\r\n  geom_text(aes(x = 2021.2, y = 3532, label = \"income\"), hjust = \"left\") +\r\n  geom_text(aes(x = 2021.2, y = 1237, label = \"rent\"), hjust = \"left\")\r\n\r\n\r\n\r\nA common way to define housing affordability is to look at the proportion of housing cost to household income. So for a theoretical household making the median income and paying the median rent, what proportion of that income goes to rent?\r\n\r\n\r\nShow code\r\n\r\nrent_and_income |> \r\n  filter(NAME == \"Madison city, Wisconsin\") |> \r\n  pivot_wider(names_from = \"name\", values_from = \"value\") |> \r\n  mutate(rent_pct = rent  /income) |> \r\n  ggplot(aes(as.numeric(year), rent_pct)) +\r\n  geom_line() +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_y_continuous(labels = label_percent(),\r\n                     limits = c(.25, .4)) +\r\n  xlim(c(2005, 2024)) +\r\n  labs(title = \"Rent as percentage of household income in Madison, WI\",\r\n       x = \"Year\",\r\n       y = element_blank(),\r\n       caption = \"Data: American Community Survey 1-year estimates\\nVisualization: Harald Kliems\") +\r\n  annotate(\"segment\", x = 2005, xend = 2021, yend = .3, y = .3,\r\n           linetype = 2) +\r\n  annotate(\"text\", x = 2021.5, y = .29, label = \"Rent-burdened threshold\",\r\n           vjust = \"inward\")\r\n\r\n\r\n\r\nGenerally, 30 per cent is considered the threshold at which someone is considered “rent burdened.” And our theoretical median household is above that threshold in all years since 2005. There’s a spike right after the financial crisis, followed by a slow decline, and then the percentage goes up again with the pandemic.\r\nRather than looking at a median household, the American Community Survey also provides the number of households who actually are rent burdened.\r\n\r\n\r\nShow code\r\n\r\n# rent burdened\r\nget_rent_burden <- function(acs_year){\r\n  get_acs(\"place\", \r\n          table = \"B25106\", \r\n          year = acs_year, \r\n          survey = \"acs1\",\r\n          summary_var = \"B25106_024\", #Estimate!!Total!!Renter-occupied housing units\r\n          cache_table = TRUE) |> \r\n    filter(GEOID %in% largest_50 | NAME == \"Madison city, Wisconsin\")\r\n}\r\n\r\nyears <- c(2005:2019,2021)\r\nnames(years) <- years\r\n\r\nrent_burden <- map_dfr(years, get_rent_burden, .id = \"year\")\r\n\r\nrent_burden |> \r\n  filter(variable %in% c(\"B25106_044\",\r\n                         \"B25106_040\",\r\n                         \"B25106_036\",\r\n                         \"B25106_032\",\r\n                         \"B25106_028\")) |> \r\n  group_by(NAME, year) |> \r\n  summarize(all_burdened = sum(estimate)) |> \r\n  filter(NAME == \"Madison city, Wisconsin\") |> \r\n  ggplot(aes(as.numeric(year), all_burdened)) +\r\n  geom_line()+\r\n  ylim(c(0, 33000)) +\r\n  labs(title = \"Madison renter households that spend \\n 30+% of income on housing\",\r\n       y = \"number of households\",\r\n       x = \"year\",\r\n       caption = \"Data: American Community Survey 1-year estimates B25106\\nVisualization: Harald Kliems\") + \r\n  hrbrthemes::theme_ipsum()\r\n\r\n\r\n\r\nThe total number is fairly flat over time in Madison, which is not bad for a city that has grown a lot in overall population over that time period. We can chart the percentage of rent-burdened household as the share of all renter households and compare Madison with other cities.\r\n\r\n\r\nShow code\r\n\r\nrent_burden |> \r\n  filter(variable %in% c(\"B25106_044\",\r\n                         \"B25106_040\",\r\n                         \"B25106_036\",\r\n                         \"B25106_032\",\r\n                         \"B25106_028\")) |> \r\n  mutate(city = str_replace(NAME, \" city\", \"\")) |>\r\n  group_by(city, year) |> \r\n  reframe(all_burdened = sum(estimate), burdened_pct = all_burdened/summary_est) |> \r\n  distinct() |> \r\n  # filter(NAME == \"Madison city, Wisconsin\") |> \r\n  ggplot(aes(as.numeric(year), burdened_pct, group = city)) +\r\n  geom_line()+\r\n  scale_y_continuous(labels = label_percent()) +\r\n  xlim(c(2005, 2023)) +\r\n  labs(title = \"Rent-burdened households in 50 most populous US cities and Madison\",\r\n       subtitle = \"Proportion of renter-households that spend 30+% of household income on housing\",\r\n       y = \"rent burdened households\",\r\n       x = \"year\",\r\n       caption = \"Data: American Community Survey 1-year estimates B25106\\nVisualization: Harald Kliems\") + \r\n  hrbrthemes::theme_ipsum() +\r\n  gghighlight::gghighlight(city %in% c(\r\n    \"Madison, Wisconsin\",\r\n    \"Miami, Florida\",\r\n    \"San Francisco, California\"), label_params = list(hjust = \"right\"))\r\n\r\n\r\n\r\nThis chart is probably surprising to many (it certainly was to me!). San Francisco is often considered the prime example of the housing crisis. And Miami (which, as we saw above, has the highest proportion of renters to owners) rarely comes up in national debates about the burden of rent. This just goes to show that the relationship between rents and incomes is complicated: Yes, rising rent is less of an income when incomes rise accordingly. But rising incomes can also take on the form of an influx of higher-income people and the displacement of folks with lower incomes.\r\nAre you interested in any other rent-related statistics or have questions? Shoot me an email!\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-05-23-rent-and-rent-burden-in-madison/rent-and-rent-burden-in-madison_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-05-26T07:33:41-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/before-and-after-the-metro-network-redesign/",
    "title": "Before and after the Metro Network Redesign",
    "description": "Comparing transit accessibility with the r5r package",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-04-24",
    "categories": [
      "Madison (WI)",
      "transit",
      "GTFS",
      "accessibility",
      "r5r"
    ],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\nlibrary(r5r)\r\nlibrary(sf)\r\nlibrary(tidyverse)\r\nlibrary(tmap)\r\nlibrary(gt)\r\nlibrary(reactable)\r\n\r\n\r\nOur transit system here in Madison (WI) will see a big change in June: On June 12, the flip will be switched and the Transit Network Redesign goes into effect. Last week, Metro released GTFS data for the new network. Not only can riders now use Transit or Google Maps to explore how they get from A to B in the new system; it also allows accessibility analyses that compare the network before and after the redesign.\r\nAccessibility analysis is a big topic in planning, and in addition to proprietary tools, there is an open source ecosystem of software products around transit analysis. For this article I use r5r, “an R package for rapid realistic routing on multimodal transport networks (walk, bike, public transport and car). It provides a simple and friendly interface to R5, the Rapid Realistic Routing on Real-world and Reimagined networks, the routing engine developed independently by Conveyal.”\r\nPrerequisites and data\r\nThe minimum requirements for using r5r are a file with OpenStreetMap road network data and a GTFS file for the transit data. The former I downloaded from Protomaps, and the latter is available directly from Metro or via OpenMobilityData.\r\nComparing trip times\r\nOne key promise of the redesign was that it would decrease travel times. r5r allows us to investigate this. I created a list of points-of-interest (POIs) in different parts of town and saved their coordinated in a Google Spreadsheet. Obviously one could choose other POIs—I picked a mix of locations that are important to me, locations that are on the various edges of town, and some that have come up in public debates during the redesign process. It would be easy to add additional POIs and re-run the analysis.\r\n\r\n\r\nShow code\r\n\r\n#read poi coordinates\r\npoi <- googlesheets4::read_sheet(\"https://docs.google.com/spreadsheets/d/1TLN77Pks1W5dl-kx55XCi55-bK--EhJ_DeHhrexGR5M/edit?usp=sharing\")\r\n\r\npoi |> gt()\r\n\r\n\r\nid\r\n      lat\r\n      lon\r\n    State Capitol\r\n43.07475\r\n-89.38416UW Hospital\r\n43.07645\r\n-89.43103Sequoya Library\r\n43.05378\r\n-89.45037Henry Vilas Zoo\r\n43.05808\r\n-89.41039Mickey’s Tavern\r\n43.08790\r\n-89.35847Willy St North\r\n43.12771\r\n-89.36300Walmart South Madison\r\n43.04289\r\n-89.35068East Towne Mall\r\n43.12427\r\n-89.30629West High\r\n43.06860\r\n-89.42617DMV West\r\n43.07894\r\n-89.52885Catholic Multicultural Center\r\n43.04605\r\n-89.39332\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\npoi |> \r\n  sf::st_as_sf(coords = c(\"lon\", \"lat\")) |> \r\n  tm_shape() +\r\n  tm_dots() +\r\n  tm_text(\"id\", auto.placement = TRUE,\r\n          ymod = 1.5)\r\n\r\n\r\n\r\nAnother choice is about the time of the comparison: Transit frequencies vary a lot by time of day, with more trips during rush hour, fewer trips during the rest of the day, and no service at all between approximately midnight and 6 am. And less service on weekends. For this analysis I randomly picked a Wednesday afternoon, at 2 pm, right after when the redesign goes into effect and one week before. Not peak hour, but also not low-frequency late night or weekend service. One great feature of r5r is that it is possible to analyze departures within time windows. This addresses a limitation of analyzing only a single time point: If we were to choose only 2:00 pm for our analysis, the particular schedules would have a huge impact on the result. For example, if the stop closest to the origin has a bus that comes every half hour, it matters a lot for the total trip time if the bus comes at 1:59 or at 2:01 pm. With a 1:59 departure, we’d just have missed the bus and would only get to start our trip at 2:29 pm. Whereas with a 2:01 departure, there is basically no waiting time, cutting 28 minutes from the trip. r5r allows analysis of departure times for every minute within a given time window. So it will return trip times for 2:00, 2:01, 2:02, …, 2:30 departures and then we can average those times to reduce the bias of choosing only a single time. These averages, then, represent a scenario of spontaneous, unplanned trips: I know where I want to go, I don’t know anything about bus schedules, and I want to leave right now. Again, other choices for analysis are possible.\r\nTrip time results\r\nHere are the results from the analysis in a table. You can sort and filter the table and explore the results. Overall trip times have indeed decreased for many origin/destination pairs. Walk times are more mixed: There are many origin/destination pairs where walk time has increased. This was one of the criticisms about the redesign: For people with limited mobility, these increased walk times could be a problem. But then there are also routes where walk time has decreased. The bottom line: It’s difficult to draw conclusions from the analysis of a set of 11 locations. r5r provides additional analysis capabilities, and in a future post I may return to this topic.\r\n\r\n\r\nShow code\r\n\r\nSys.setenv(JAVA_HOME=\"c:/Program Files/OpenJDK/jdk-11/\")\r\n\r\noptions(java.parameters = \"-Xmx2G\")\r\nrJava::.jinit()\r\n#rJava::.jcall(\"java.lang.System\", \"S\", \"getProperty\", \"java.version\")\r\n\r\n\r\n# Indicate the path where OSM and GTFS data are stored\r\nr5r_core <- setup_r5(data_path = \"data/\")\r\n\r\nmode <- c(\"WALK\", \"TRANSIT\")\r\nmax_walk_time <- 30 # minutes\r\nmax_trip_duration <- 150 # minutes\r\ndeparture_datetime <- as.POSIXct(\"12-06-2023 14:00:00\",\r\n                                 format = \"%d-%m-%Y %H:%M:%S\")\r\n\r\n# # extract OSM network\r\n# street_net <- street_network_to_sf(r5r_core)\r\n# \r\n# # extract public transport network\r\n# transit_net <- r5r::transit_network_to_sf(r5r_core)\r\n\r\n\r\n#compare trips before and after network redesign\r\ndeparture_datetime_before <- as.POSIXct(\"05-06-2023 14:00:00\",\r\n                                 format = \"%d-%m-%Y %H:%M:%S\")\r\ndeparture_datetime_after <- as.POSIXct(\"12-06-2023 14:00:00\",\r\n                                        format = \"%d-%m-%Y %H:%M:%S\")\r\n\r\n# calculate a travel time matrix\r\nttm_before <-  expanded_travel_time_matrix(r5r_core = r5r_core,\r\n                                    origins = poi,\r\n                                    destinations = poi,\r\n                                    mode = mode,\r\n                                    departure_datetime = departure_datetime_before,\r\n                                    max_walk_time = max_walk_time,\r\n                                    max_trip_duration = max_trip_duration,\r\n                                    time_window = 30,\r\n                                    breakdown = TRUE)\r\n\r\nttm_before <- ttm_before |> \r\n  filter(from_id != to_id) |> \r\n  mutate(origin_destination = paste0(from_id, \" to \", to_id)) |> \r\n  summarize(mean_traveltime_before = mean(total_time),\r\n            mean_walk_time_before = mean(access_time + egress_time),\r\n            mean_transfer_time_before = mean(transfer_time),\r\n            .by = origin_destination)\r\n\r\nttm_after <-  expanded_travel_time_matrix(r5r_core = r5r_core,\r\n                                           origins = poi,\r\n                                           destinations = poi,\r\n                                           mode = mode,\r\n                                           departure_datetime = departure_datetime_after,\r\n                                           max_walk_time = max_walk_time,\r\n                                           max_trip_duration = max_trip_duration,\r\n                                           time_window = 30,\r\n                                          breakdown = TRUE)\r\n\r\nttm_after <- ttm_after |> \r\n  filter(from_id != to_id) |> \r\n  mutate(origin_destination = paste0(from_id, \" to \", to_id)) |> \r\n  group_by(origin_destination) |> \r\n  summarize(from_id,\r\n            to_id,\r\n            mean_traveltime_after = mean(total_time),\r\n            mean_walk_time_after = mean(access_time + egress_time),\r\n            mean_transfer_time_after = mean(transfer_time)) |> \r\n  distinct(origin_destination, .keep_all = TRUE)\r\n\r\nbefore_after <- ttm_before |> \r\n  left_join(ttm_after, by = c(\"origin_destination\")) |> \r\n  mutate(change_total = (mean_traveltime_after-mean_traveltime_before)/abs(mean_traveltime_before),\r\n         change_walk = (mean_walk_time_after-mean_walk_time_before)/abs(mean_walk_time_before),\r\n         change_transfer = (mean_transfer_time_after - mean_transfer_time_before) / abs(mean_transfer_time_before)) |> \r\n  select(-change_transfer)\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nbefore_after |> \r\n  select(from_id,\r\n         to_id,\r\n         mean_traveltime_before,\r\n         mean_traveltime_after,\r\n         change_total,\r\n         mean_walk_time_before,\r\n         mean_walk_time_after,\r\n         change_walk) |> \r\n  reactable(\r\n    searchable = FALSE,\r\n    defaultSorted = \"from_id\",\r\n    columns = list(\r\n      from_id = colDef(name = \"From\",\r\n                       filterable = TRUE),\r\n      to_id = colDef(name = \"To\", \r\n                     filterable = TRUE),\r\n      change_total = colDef(format = colFormat(percent = TRUE, digits = 1),\r\n                            name = \"change total time\"),\r\n      change_walk = colDef(format = colFormat(percent = TRUE, digits = 1),\r\n                           name = \"change walk time\"),\r\n      mean_traveltime_after = colDef(format = colFormat(digits = 0),\r\n                                     name = \"total time after\"),\r\n      mean_traveltime_before = colDef(format = colFormat(digits = 0),\r\n                                      name = \"total time before\"),\r\n      mean_walk_time_after = colDef(format = colFormat(digits = 0),\r\n                                    name = \"walk time after\"),\r\n      mean_walk_time_before = colDef(format = colFormat(digits = 0),\r\n                                     name = \"walk time before\")\r\n  )\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/before-and-after-the-metro-network-redesign/img/metro_image.png",
    "last_modified": "2023-04-24T16:51:38-05:00",
    "input_file": {},
    "preview_width": 819,
    "preview_height": 482
  },
  {
    "path": "posts/2023-03-26-understanding-adjusted-data-in-the-national-transit-database/",
    "title": "Understanding Raw and Adjusted Data in the National Transit Database",
    "description": "A comparison of Madison Metro ridership data released in the National Transit Database at different points in time.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-04-06",
    "categories": [
      "Madison (WI)",
      "transit",
      "National Transit Database"
    ],
    "contents": "\r\nI regularly post about transit data, here and on social media–often about ridership for our local transit agency, Madison Metro.\r\nThose posts have always been based on data from National Transit Database (NTD).\r\nBut I kept encountering discrepancies in the ridership data: I saw one number in the NTD spreadsheets and another number mentioned in, say, a local press conference.\r\nWhen I asked Metro about this, they mentioned that they were aware of possible data issues, but it wasn’t clear whether those were related to NTD.\r\nIn this post, I dig deeper in what we can learn from looking at NTD reporting over time.\r\nThere are two types of monthly time series data available from NTD: “adjusted” and “raw” data.\r\nI spent a good amount of time trying to find out what exactly NTD mean by adjusted data.\r\nOn their download page they say this (emphasis mine):\r\n\r\n“Contains monthly-updated service information reported by urban Full Reporters, released after adjustments.\r\n\r\n\r\nThis file will be updated periodically by FTA to include data for subsequent months, and to incorporate revisions to prior months in the calendar year made by the transit properties.\r\nIn some cases, this may include revisions to data from previous years.\r\n\r\n\r\nAny data value that appears in this file that does not appear in the Raw Database Time Series is based on a growth estimate developed by FTA. The strength of each estimate improves when the population of agencies reporting in the prior period is larger.\r\n”\r\n\r\nA PowerPoint presentation on using NTD data, states that “Adjusted Monthly Module Data Release Is Adjusted to Support Time-Series Study.” And cautions, “Data from the most-recent Month may be incomplete due to late reporters. Data is subject to change for up to 18 months (or more in rare cases).”\r\nNone of this is very helpful in understanding how the data are being adjusted.\r\nFor an agency like Madison Metro, which as a full reporter is required to report data monthly, it would seem to make sense to just use the raw data.\r\nLet’s compare raw and adjusted:\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(ntdr) #install with remotes::install_github(\"https://github.com/vgXhc/ntdr\")\r\nggplot2::theme_set(theme_minimal())\r\nmetro_raw <- get_ntd(agency = \"City of Madison\", data_type = \"raw\", modes = \"MB\", cache = TRUE)\r\nmetro_adjusted <- get_ntd(agency = \"City of Madison\", data_type = \"adjusted\", modes = \"MB\", cache = TRUE)\r\n\r\nmetro_raw <- metro_raw |> \r\n  mutate(data_type = \"raw\")\r\nmetro_adjusted <- metro_adjusted |> \r\n  mutate(data_type = \"adjusted\")\r\n\r\nmetro_all <- rbind(metro_raw, metro_adjusted)\r\n\r\nggplot(metro_all, aes(month, value, color = data_type)) +\r\n  geom_line(alpha = .5) +\r\n  labs(title = \"Madison Metro monthly ridership\") +\r\n  ylab(\"Unlinked passenger trips\")\r\n\r\n\r\n\r\nWe see that the raw data is not available for the most recent year.\r\nHere’s what the ReadMe tab of the raw data release says:\r\n\r\nThe Monthly module includes a limited set of key indicators reported by transit properties.\r\nData is reported on a monthly basis, by mode and type of service, for a fiscal year.\r\n\r\nAnd further down: “Unlinked Passenger Trips – For the most recent closed-out annual report year.” So the raw data only seem to be made available after a full year has been closed out.\r\nThis is really confusing.\r\nBack to the adjusted data.\r\nBecause I’m really bad at cleaning out my “Downloads” folder, I have several monthly adjusted data files released in different months in 2022.\r\nThis gives us the ability to compare the files and see how large the adjustments being made actually are.\r\nLet’s compare data released for1 March, October, and December of 2022.\r\n\r\n\r\nShow code\r\n\r\nlibrary(readxl)\r\nbus_dec_22 <- read_excel(\"c:/Users/user1/Downloads/December 2022 Adjusted Database(1).xlsx\", sheet = 3)\r\n\r\nbus_oct_22 <- read_excel(\"C:/Users/user1/Downloads/October 2022 Adjusted Database(1).xlsx\", sheet = 3)\r\n\r\nbus_mar_22 <- read_excel(\"c:/Users/user1/Downloads/March 2022 Adjusted Database.xlsx\", sheet = 3)\r\n\r\nbus_long_dec_22 <- bus_dec_22 %>% \r\n  filter(Agency == \"City of Madison\" & Modes == \"MB\") |> \r\n  pivot_longer(cols = 10:ncol(bus_dec_22), names_to = \"month\", values_to = \"value\") %>% \r\n  mutate(date = my(month),\r\n         month = month(date),\r\n         report_month = \"dec_22\")\r\n\r\n\r\nbus_long_mar_22 <- bus_mar_22 %>% \r\n  filter(Agency == \"City of Madison\" & Modes == \"MB\") |> \r\n  pivot_longer(cols = 10:ncol(bus_mar_22), names_to = \"month\", values_to = \"value\") %>% \r\n  mutate(date = my(month),\r\n         month = month(date),\r\n         report_month = \"mar_22\")\r\n\r\nbus_long_oct_22 <- bus_oct_22 %>% \r\n  filter(Agency == \"City of Madison\" & Modes == \"MB\") |> \r\n  pivot_longer(cols = 10:ncol(bus_oct_22), names_to = \"month\", values_to = \"value\") %>% \r\n  mutate(date = my(month),\r\n         month = month(date),\r\n         report_month = \"oct_22\")\r\n\r\nmetro_comp <- rbind(bus_long_dec_22, bus_long_oct_22, bus_long_mar_22)\r\n\r\nmetro_comp|> \r\n  ggplot(aes(date, value, color = report_month)) +\r\n  geom_line() +\r\n  scale_color_discrete(\"Data release\") +\r\n  ylab(\"Unlinked passenger trips\") +\r\n  labs(title = \"Comparing NTD monthly adjusted data releases\")\r\n\r\n\r\n\r\nThe three lines overlap nicely for any data prior to about 2020.\r\nLet’s zoom in on the more recent data:\r\n\r\n\r\nShow code\r\n\r\nmetro_comp |> \r\n  filter(date > ymd(\"2020-01-01\")) |> \r\n  ggplot(aes(date, value, color = report_month)) +\r\n  geom_line() +\r\n  ylab(\"Unlinked passenger trips\")+\r\n  labs(title = \"Comparing NTD monthly adjusted data releases\")\r\n\r\n\r\n\r\nThe most obvious discrepancy is between the data released for March 2022 and the October and December releases: The March release starts diverging in January 2021 and is much lower, especially in the second half of 2021.\r\nThe October and December releases appear quite similar, but if you squint, you can see small differences.\r\nWe can calculate and plot these differences:\r\n\r\n\r\nShow code\r\n\r\nmetro_comp |> \r\n  pivot_wider(names_from = \"report_month\", values_from = \"value\") |> \r\n  mutate(diff_mar_oct = mar_22 - oct_22,\r\n         diff_mar_dec = mar_22 - dec_22,\r\n         diff_oct_dec = oct_22- dec_22) |> \r\n  pivot_longer(cols = starts_with(\"diff_\"), names_to = \"report_date\", values_to = \"difference\") |> \r\n  filter(report_date == \"diff_oct_dec\" & date > ymd(\"2020-01-01\")) |> \r\n  ggplot(aes(date, difference)) +\r\n  geom_col() +\r\n  labs(title = \"Ridership difference December and October 2022 NTD releases\") +\r\n  ylab(\"Unlinked passenger trip difference\")\r\n\r\n\r\n\r\nYes, there are differences for 2022, but the differences are fairly small compared to overall ridership.\r\nConclusion\r\nAdjusted monthly NTD data is indeed adjusted from one month to another.\r\nWhether that adjustment is because of data revisions from the reporting agency or something that FTA does (or both) is unclear.\r\nThe adjustments affect primarily the most recent data, and they can be large or small.\r\nIt is possible that the COVID shock to ridership had an outsize influence on the adjustments being made.\r\nThese conclusions are based on my looking at the data for one specific agency; I cannot say if they hold true for other transit agencies.\r\nI also only look at ridership data.\r\nThe monthly NTD releases also include vehicle revenue miles, vehicle revenue hours, and some other metrics.\r\nPersonally, I will be much more cautious with current-year data and either not report on it all or at least add more cautionary language about data quality.\r\nIt’d be great if our local transit agency made their raw data reported to FTA available on the City’s own open data portal.\r\nI welcome feedback from others who know more about the NTD reporting and adjustment process.\r\n\r\nThere is a lag of about three months in publishing the data in the website.\r\nFor example right now (April 6, 2023), the most recent data available is for January 2023.↩︎\r\n",
    "preview": "posts/2023-03-26-understanding-adjusted-data-in-the-national-transit-database/understanding-adjusted-data-in-the-national-transit-database_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-04-06T09:35:15-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-10-28-map-fun-with-city-of-madison-tax-parcel-data/",
    "title": "Map fun with City of Madison tax parcel data",
    "description": "Building age, property use, and architectural styles in Madison",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-02-21",
    "categories": [
      "Madison (WI)",
      "housing",
      "map"
    ],
    "contents": "\r\nThe City of Madison’s open data portal has a tax parcel dataset. Tax information, however, is not the only thing included in the dataset. There are over 140 variables and for this post we will look at three of them: The year the structure on the parcel was built, the use of the property, and the architectural style.\r\n\r\nCorrection 2023-02-21: The originally published version of this post used the BuiltYear variable for the map of the age of buildings. Jim Kreft pointed out that there is another variable in the dataset, Maxconstru, which has more complete data. The code and map have been updated accordingly.\r\n\r\n\r\nShow code\r\n\r\nlibrary(sf)\r\nlibrary(tidyverse)\r\n\r\n\r\nbuildings <- read_sf(\"/Users/user1/Downloads/Tax_Parcels_(Assessor_Property_Information)/Tax_Parcels_(Assessor_Property_Information).shp\")\r\nbuildings <- buildings |> \r\n  st_make_valid()\r\n\r\n\r\nBuilding age\r\nLet’s start with the YearBuilt variable. This allows us a glimpse into the development of Madison over time. First, a static map:\r\n\r\n\r\nShow code\r\n\r\nbuildings2 <- buildings |> \r\n  mutate(MaxConstru = na_if(MaxConstru, 0),\r\n         MaxConstru_labels = cut(MaxConstru,\r\n                                breaks = c(-Inf, 1900, 1945, 1980, 2000, 2010, Inf),\r\n                                labels = c(\"pre-1900\", \"1900 to 1944\", \"post-WWII to 1979\", \"1980 to 1999\", \"2000 to 2009\", \"2010 and newer\"),\r\n                                ordered_result = T))\r\n\r\nlibrary(tmap)\r\ntmap_mode(\"plot\")\r\ntmap_options(bg.color = \"black\", legend.text.color = \"white\")\r\np <- buildings2 |> \r\n  select(MaxConstru_labels) |> \r\n  tm_shape() +\r\n  tm_polygons(title = \"\", \"MaxConstru_labels\", border.alpha = 0, style = \"cont\",\r\n              colorNA = \"darkgrey\") +\r\n  tm_credits(\"Map: @HaraldKliems. Data: City of Madison Tax Parcels\", col = \"white\") +\r\n  tm_layout(title = \"How old are Madison’s buildings?\",\r\n            scale = 1,\r\n            bg.color = \"black\",\r\n            legend.text.color = \"white\",\r\n            title.color = \"white\",\r\n            legend.title.color = \"white\")\r\n\r\n# tmap_save(p, filename = \"c:/Users/user1/Desktop/map.png\", width = 12, height = 9, dpi = 600)\r\np\r\n\r\n\r\n\r\nIt’d be nice to have an interactive map, but the dataset is too large to not crash most browsers (stay tuned; I’m working on alternatives).\r\nIn the static map it’s not easy to see areas where parcels have redeveloped, such as along the East Washington Ave corridor, but we can make out some glimpses. We also see that a lot of parcels have missing data, and that missingness doesn’t seem to be random. It’d be interesting to know why the city has better data on some properties/areas/eras than on others!\r\nProperty use\r\nThe age of a structure only tells us part of a story. How is a building use, and how do these uses cluster? The property use data has many different categories, and to keep the map legible we need to lump a lot of them together. This is what we get for the 10 most common uses:\r\n\r\n\r\nShow code\r\n\r\np2 <- buildings2 |>\r\n  select(PropertyUs) |>\r\n  mutate(PropertyUs = case_when(\r\n    str_detect(PropertyUs, \"Apartment$\") ~ \"Apartment\",\r\n    str_detect(PropertyUs, \"^Condominium\") ~ \"Condominium\",\r\n    TRUE ~ PropertyUs\r\n  )) |>\r\n  mutate(PropertyUs_collapsed = fct_lump_n(PropertyUs, 10)) |>\r\n  tm_shape() +\r\n  tm_polygons(\r\n    title = \"\",\r\n    \"PropertyUs_collapsed\",\r\n    border.alpha = 0,\r\n    colorNA = \"ghostwhite\"\r\n  ) +\r\n  tm_credits(\"Map: @HaraldKliems. Data: City of Madison Tax Parcels\") +\r\n  tm_layout(\r\n    title = \"Property use for tax parcels in Madison\",\r\n    scale = .6,\r\n    bg.color = \"black\",\r\n    legend.text.color = \"white\",\r\n    title.color = \"white\",\r\n    legend.title.color = \"white\"\r\n  )\r\n\r\ntmap_mode(\"plot\")\r\np2\r\n\r\n\r\n\r\nHome style\r\nA third and final map looks at the “home style.”\r\n\r\n\r\nShow code\r\n\r\np3 <- buildings2 |>\r\n  select(HomeStyle) |>\r\n  mutate(\r\n    HomeStyle = case_when(\r\n      str_detect(HomeStyle, \"^Ranch\") ~ \"Ranch\",\r\n      str_detect(HomeStyle, \"^Townhouse\") ~ \"Townhouse\",\r\n      str_detect(HomeStyle, \"^Garden\") ~ \"Garden\",\r\n      TRUE ~ HomeStyle\r\n    ),\r\n    HomeStyle_collapsed = fct_lump_n(HomeStyle, 10)\r\n  ) |>\r\n  tm_shape() +\r\n  tm_polygons(\"HomeStyle_collapsed\",\r\n              border.alpha = 0,\r\n              colorNA = \"ghostwhite\") +\r\n  tm_credits(\"Map: @HaraldKliems. Data: City of Madison Tax Parcels\") +\r\n  tm_layout(\r\n    title = \"Architectural style of Madison buildings\",\r\n    scale = .6,\r\n    bg.color = \"black\",\r\n    legend.text.color = \"white\",\r\n    title.color = \"white\",\r\n    legend.title.color = \"white\"\r\n  )\r\n\r\ntmap_mode(\"plot\")\r\np3\r\n\r\n\r\n\r\nLumping together the less common styles under “Other” is a little unfortunate for this variable. It’ll make use miss out on less common but interesting architectural styles such as “Spanish mediterranean” or “New style modern international”. So let’s at least have a table with all the styles and how common they are.\r\n\r\n\r\nShow code\r\n\r\nbuildings2 |> st_drop_geometry() |>\r\n  select(HomeStyle) |>\r\n  mutate(HomeStyle = case_when(str_detect(HomeStyle, \"^Ranch\") ~ \"Ranch\",\r\n                               str_detect(HomeStyle, \"^Townhouse\") ~ \"Townhouse\",\r\n                               str_detect(HomeStyle, \"^Garden\") ~ \"Garden\",\r\n                               TRUE ~ HomeStyle)) |>\r\n           group_by(HomeStyle) |>\r\n           tally(sort = T) |>\r\n  DT::datatable()\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-10-28-map-fun-with-city-of-madison-tax-parcel-data/map-fun-with-city-of-madison-tax-parcel-data_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-02-21T07:00:43-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-02-18-using-knitr-and-rmarkdown-to-format-google-forms-responses/",
    "title": "Candidate questionnaires for local elections",
    "description": "Using `knitr` and `RMarkdown` for formatting form responses",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2023-02-18",
    "categories": [
      "Madison (WI)",
      "Google Forms",
      "Madison Bikes",
      "RMarkdown",
      "knitr"
    ],
    "contents": "\r\n\r\nThe non-profit I am involved with, Madison Bikes, has been doing candidate questionnaires for local elections for several election cycles now.\r\nWith each iteration, we learn some things about the best process to make this happen.\r\nHow do you keep track of candidates?\r\nHow do you best collect responses?\r\nAnd then how do you present those responses to your community?\r\nFor the 2023 spring primary and general elections, we used a new process that takes some of those lessons learned and saved us a lot of work. In this post, I will describe the process with the goal of documenting for ourselves and also allowing others to adapt it to their own purposes. The first sections are about the logistics and of interest to anyone who wants to do candidate questionnaires. The second part gets into the technical process of using R programming to take the responses and format them.\r\nIf you want to see what the end result looks like before reading the post, here’s the link.\r\nIdentifying candidates and their contact information\r\nThe first step to a candidate questionnaire is to keep track of who is running.\r\nThe timeline for filing for candidacy is short:\r\nSource: City of Madison Clerk’s Office\r\nDecember 1, 2022\r\nFirst day nomination papers may be circulated.\r\nDecember 23, 2022\r\nDeadline for incumbents not seeking re-election to file Notice of Non-Candidacy.\r\nJanuary 3, 2023\r\nAll papers and forms due in City Clerk’s Office at 5 p.m.\r\nJanuary 6, 2023\r\nDeadline to challenge nomination papers.\r\nI set up a Google Sheet early on and manually kept it up to date as I saw new candidates announcing (or incumbents announcing their non-candidacy), mostly relying on the Clerk’s Office’s candidate filings but also media coverage.\r\nAs we got closer to the deadline, I also started collecting candidates’ websites, social media links, and email addresses.\r\nThis involved a lot of internet searching and was very time consuming.\r\nI later found out that candidates provide an email address to the Clerk’s Office on their official paperwork.\r\nWhen we do this again for the next election cycle, I would no longer manually search and just wait until after the deadline.\r\nDeveloping questions\r\nIn previous years Madison Bikes did their own questionnaire.\r\nWhen we learned that Madison is for People, a housing advocacy organization, was also interested in doing a questionnaire, we connected with them to coordinate efforts.\r\nWe also got Madison Area Bus Advocates on board and developed a joint questionnaire.\r\nPartnering with other organization has the advantage of being able to share some of the logistical work, but it also creates additional coordination work.\r\nOverall, this went very smoothly.\r\nWe needed to make sure that the questionnaire didn’t become too burdensome for the candidates, while still providing meaningful content for our organizations’ respective audiences.\r\nIn the end, each organization provided three to four questions, plus three short joint questions about how candidates get around the city.\r\nWe had slightly different questions for mayoral candidates and candidate for the common council.\r\nDistributing the questions, collecting answers\r\nWe split up the work of emailing candidates between three people (Thanks, Connor, Will, and Jonathan!).\r\nEmails were sent individually, and I think this is something that in the future we could consider automating, e.g. by using Mailchimp or Qualtrics.1\r\nWe tracked the send-out in the same Google Sheet where we kept the contact information.\r\nWe discussed several options for receiving candidate responses.\r\nShould they just sent their responses in an email?\r\nPut them into a Google Doc?\r\nBased on our prior experience we decided to go with a Google Form.\r\nWith over 40 candidates total, anything else would likely have turned into a nightmare.\r\nThe Google Form had a text field for the candidate’s name, a dropdown menu for their district (don’t use a text field for this!), and text fields for each candidate question.\r\nWe carefully tested the survey, for example making sure that the respondents didn’t need a Google account.\r\nThen a link to the survey was included in the emails.\r\nTurning a form into HTML\r\nThe deadline for candidates to respond was only a week before the primary election date.\r\nTurning the form responses into nicely formatted content for our website quickly was therefore very important.\r\nI ended up using RMarkdown and knitr as the main tools for this, relying heavily on the wonderful “R Markdown: The Definitive Guide” and the “R Markdown Cookbook”.\r\nIt’s very much possible that there are better/different tools for this out there, but RMarkdown is what I was familiar with and the final product turned out well.\r\nYou can find the repository with all code on Github: https://github.com/vgXhc/sheets-2-wp\r\nThe Madison Bikes website runs on WordPress, which limited our options of what we could do.\r\nIn the last round of candidate questionnaires, Ben had created a script to create html tags around the responses read in from a csv file.\r\nWe then copied that html into a WordPress custom html block.\r\nThis seemed to work well enough and so I also aimed to create an html document to paste into WordPress, but instead of bare html it would be an html document generated by knitr.\r\nLet’s walk through this step by step.\r\nThere are three main files:\r\ncombined-report.Rmd: This is the file that eventually gets knitted and produces the html and pdf output\r\noutput-council-child.Rmd: The template for generating content blocks within combined-report.Rmd for council candidates.\r\noutout-mayor-child.Rmd: Same, but for mayoral candidates\r\ncombined-report.Rmd has a full yaml header, both for html and pdf output.2\r\n---\r\ntitle: \"Candidate Q&A\"\r\noutput: \r\n  pdf_document:\r\n    toc: TRUE\r\n    toc_depth: 1\r\n  html_document:\r\n    toc: TRUE\r\n    toc_depth: 1\r\n    toc_float: true\r\ndocumentclass: scrreport\r\n---\r\nOne thing we didn’t have last time was a table of contents (TOC), which made it difficult to navigate through the Q&A.\r\nThe toc: true option easily generates the TOC.\r\nA floating TOC would have been even better (it really helps to keep track of where in the document you are) , but since that relies on Bootstrap and jQuery, it doesn’t work in a WordPress html block.\r\nSince Madison is for People and Madison Area Bus Advocates use different content management systems, I rendered a version with and without the floating TOC.\r\nFor the pdf version, I specified the LaTeX document class scrreport from the KOMA-Script bundle.\r\nAfter the yaml header there is a bunch of code to prep the data.\r\nThe form responses are read in with the googlesheets4 package.\r\nThey come in as wide data, with one row for each candidate and one column for each question.\r\nThe code then transforms the data into a long format, with one row for each question and answer.\r\nTo help with formatting, the question is split into a “topic” (everything before the colon) and the actual question.3\r\nThere’s some duplication in the code that could be cleaned up with a function.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(googlesheets4)\r\nlibrary(here)\r\nlibrary(knitr)\r\nresponses_council <-\r\n  read_sheet(\r\n    \"https://docs.google.com/spreadsheets/d/1zMTl2BQzQ231SFjN8RRKvZm3PidXSoS2tVDFXSn5AwU/\",\r\n    col_types = \"cccccccccccccccc\"\r\n  ) |>\r\n  mutate(\r\n    district = factor(\r\n      `Which district are you running for?`,\r\n      c(\r\n        \"District 1\",\r\n        \"District 2\",\r\n        \"District 3\",\r\n        \"District 4\",\r\n        \"District 5\",\r\n        \"District 6\",\r\n        \"District 7\",\r\n        \"District 8\",\r\n        \"District 9\",\r\n        \"District 10\",\r\n        \"District 11\",\r\n        \"District 12\",\r\n        \"District 13\",\r\n        \"District 14\",\r\n        \"District 15\",\r\n        \"District 16\",\r\n        \"District 17\",\r\n        \"District 18\",\r\n        \"District 19\",\r\n        \"District 20\"\r\n      )\r\n    ),\r\n    district_short = str_remove(district, \"istrict \")\r\n  ) |>\r\n  arrange(district, `Your name`)\r\nresponses_mayor <-\r\n  read_sheet(\r\n    \"https://docs.google.com/spreadsheets/d/1T3hQ20IDetkjQ71qj5nZqRiZ8fZ8XHoKDCiUKaIPs9Y/\"\r\n  )\r\nresponses_council_long <- responses_council |>\r\n  pivot_longer(\r\n    cols = 6:ncol(responses_council) - 2,\r\n    names_to = \"question\",\r\n    values_to = \"answer\"\r\n  ) |>\r\n  rename(name = `Your name`) |>\r\n  mutate(\r\n    topic = str_extract(question, \"^(.*?)\\\\:\"),\r\n    # everything before the colon\r\n    question = if_else(is.na(topic), question, str_remove(question, topic))\r\n  )\r\nresponses_mayor_long <- responses_mayor |>\r\n  mutate(district = \"Mayoral candidate\",\r\n         district_short = \"mayoral cand.\") |>\r\n  pivot_longer(\r\n    cols = 3:ncol(responses_mayor),\r\n    names_to = \"question\",\r\n    values_to = \"answer\"\r\n  ) |>\r\n  rename(name = `Your name`) |>\r\n  mutate(\r\n    topic = str_extract(question, \"^(.*?)\\\\:\"),\r\n    # everything before the colon\r\n    question = if_else(is.na(topic), question, str_remove(question, topic))\r\n  )\r\n\r\n\r\n\r\n\r\nstr(responses_council)\r\n\r\ntibble [19 × 18] (S3: tbl_df/tbl/data.frame)\r\n $ Timestamp                                                                                                                                                                                                                                                                                                                                                                                                             : chr [1:19] \"2/14/2023 22:30:06\" \"2/14/2023 19:08:25\" \"2/5/2023 22:06:43\" \"2/11/2023 8:42:33\" ...\r\n $ Your name                                                                                                                                                                                                                                                                                                                                                                                                             : chr [1:19] \"John W. Duncan\" \"Colin Barushok\" \"Juliana Bennett\" \"Derek Field\" ...\r\n $ Which district are you running for?                                                                                                                                                                                                                                                                                                                                                                                   : chr [1:19] \"District 1\" \"District 2\" \"District 2\" \"District 3\" ...\r\n $ When was the last time you took the bus?                                                                                                                                                                                                                                                                                                                                                                              : chr [1:19] \"The last time I rode a public transit bus was when I lived in Boston, where commuter routes were convenient for\"| __truncated__ \"September 2022\" \"Early this week, 1/31/2023\" \"Thursday, two days ago\" ...\r\n $ When was the last time you rode your bike to work, to school, or for an errand?                                                                                                                                                                                                                                                                                                                                       : chr [1:19] \"A lack of designated bike lanes along busy roads does not allow for safe commuting to work or to run errands in this area.\" \"During summer 2022\" \"Today, 2/2/2023\" \"Three years ago when I lived closer to downtown\" ...\r\n $ What is the primary way you move around the city?                                                                                                                                                                                                                                                                                                                                                                     : chr [1:19] \"The safest and best option for commuting for those of us on the far-west side is usually by car.\" \"Walking\" \"walk or carpool\" \"Bus for work, walking around the neighborhood, and car for errands\" ...\r\n $ General Vision:  2023 brings many significant changes for Metro including the beginning of Bus Rapid Transit implementation, a complete redesign of the transit network, and policy changes such as Transportation Demand Management and Transit Oriented Development. Do you support the current direction of Madison’s transit plans, and what is your vision for Madison’s transit system in the mid and long term?: chr [1:19] \"I support the decision to increase the frequency and capacity of bus service along high-demand routes in the ci\"| __truncated__ \"I support Bus Rapid Transit, efforts toward Transportation Demand Management, and Transit Oriented Development.\"| __truncated__ \"Over the past two years, I have been supportive of BRT, Metro Network Redesign, Vision Zero, TOD and more that \"| __truncated__ \"I'm excited for the areas of the city whose transit service is improving with higher frequency, including along\"| __truncated__ ...\r\n $ Sustainable Funding: In the Metro redesign, funding constraints limiting the budget to 2019 levels required compromises in network coverage, hours of service, and frequency outside a handful of core routes. What would you do to establish more sustainable funding to improve Metro’s quality of service?                                                                                                         : chr [1:19] \"The unfortunate reality facing growing cities throughout the country, including Madison, is that budget dollars\"| __truncated__ \"We should make sure BRT is efficient and convenient to encourage more riders for a wider variety of trips. This\"| __truncated__ \"We are in a budget deficit, yes. We are also at a point where we need to get our priorities in order of what we\"| __truncated__ \"Transit service must be a budget priority going into some tough upcoming budget years and I'll remain an advoca\"| __truncated__ ...\r\n $ Accessibility: The Metro network redesign has increased the distance to the nearest bus stop for some residents. This is a major concern for bus riders with limited mobility. What measures would you take to ensure riders with mobility limitations are well-served by our transit system? What do you think the role of paratransit is in this regard?                                                            : chr [1:19] \"The goal of our transit system should be to provide equitable access to fixed-route buses. In the long term, I \"| __truncated__ \"Madison is required to provide transportation for everybody regardless of their ability. The city should invest\"| __truncated__ \"From my time on Council, I have seen how the city is not always good at including community voices, especially \"| __truncated__ \"This is a concern in my district as well - one that I've discussed with a few residents at their doors while ca\"| __truncated__ ...\r\n $ Historic Preservation: There have been conflicts between the priorities of promoting new housing development and preserving historically significant buildings and neighborhoods in recent years. What specific factors would you consider when balancing new development against preservation, and how much weight would you give to the different factors?                                                          : chr [1:19] \"Historic preservation, while desirable, has often been used as a tool for excluding historically marginalized c\"| __truncated__ \"District 2 is unique because it is the most dense district in the city and the district with the highest concen\"| __truncated__ \"Oftentimes, we view historic preservation as binary subjects. Supporting historic preservation and the city’s n\"| __truncated__ \"Madison is experiencing a housing crisis. We need to balance preservation of historically significant sites wit\"| __truncated__ ...\r\n $ Housing Affordability: The City's 2022 Housing Snapshot indicated that more housing was needed at all income levels, including both affordable housing and market rate housing. What is your plan to ensure housing is built that is available at all income levels?                                                                                                                                                  : chr [1:19] \"Meeting our city's housing needs will require a multipronged approach that involves collaborating with develope\"| __truncated__ \"I would support policies that encourage density in more parts of the city, along transit routes, and in unused \"| __truncated__ \"Increasing affordable housing is one of my top priorities for the upcoming term. My plan to increase housing is\"| __truncated__ \"Housing is one of my core campaign issues. My partner and I experienced Madison's housing crisis for ourselves \"| __truncated__ ...\r\n $ Zoning Reform: Municipalities across the country, including Portland, Minneapolis, and Charlotte have taken steps to reform zoning by eliminating parking minimums and allowing for small multi-family buildings by-right throughout the city. Would you support similar reforms in Madison? Why or why not?                                                                                                          : chr [1:19] \"Yes, I support these types of initiatives. I believe that zoning reform is necessary to meet future housing dem\"| __truncated__ \"Yes I support these policies. We are trying to encourage less driving for environmental reasons, and we are als\"| __truncated__ \"Yes of course! I have been supportive of such reforms, including zoning reforms for missing middle housing. In \"| __truncated__ \"I believe that zoning is a policy tool, and that it was implemented by the community many decades ago in an att\"| __truncated__ ...\r\n $ Complete Green Streets: Madison recently adopted a Complete Green Streets policy that prioritizes walking, biking, transit, and green infrastructure over driving and car parking when it comes to allocating our public right of way. Are you committed to implementing this policy, especially when a project requires the removal of car parking or inconveniencing drivers?                                       : chr [1:19] \"I support the Complete Green Streets policy that the city has pursued, and I am committed to implementing it. W\"| __truncated__ \"I'm committed to implementing complete green streets, encouraging less single occupancy modes of travel, improv\"| __truncated__ \"Yes definitely! I was happy to support Complete Green Streets. Madison should be a 15 minute city, meaning anyo\"| __truncated__ \"Yes. I believe in the objectives of this policy.\" ...\r\n $ Vision Zero: Madison committed to eliminating all fatalities and serious injuries from traffic crashes by 2035. Yet in 2022, 14 people were killed, including 3 people riding bikes, and 74 were seriously injured. Which roadways and intersections in your district should be prioritized for safety improvements, and what strategies would you use to ensure improvements are implemented?                        : chr [1:19] \"Rapid development in my district continues to prioritize the needs of drivers. Long stretches of increasingly b\"| __truncated__ \"The most concerning intersection is W Gorham Street becomes University Avenue at Basset Street. Cars routinely \"| __truncated__ \"Vision Zero is an excellent program, but it is still in its pilot stage. In the next two years, I would like to\"| __truncated__ \"Traffic safety is one of the top issues residents raise with me as I canvass neighborhoods in my district. I ap\"| __truncated__ ...\r\n $ Bike Network: Madison Bikes wants all residents to have access to a low-stress bike network that makes biking safe and convenient for people of all ages and abilities, no matter where they live in the city. Where in your district do you see major gaps in this network and how would you propose to fix these gaps?                                                                                              : chr [1:19] \"Significant gaps currently exist in the bike network on most major roads in my district. As the city plans the \"| __truncated__ \"Westbound University Avenue has a non protected bike lane that is between auto lanes and bus lanes. This has lo\"| __truncated__ \"My district downtown is fairly bike and pedestrian friendly. I believe that we could add protected bike lanes a\"| __truncated__ \"I see major gaps in the network right in my district. Neighborhoods on the far east side are rigidly segmented \"| __truncated__ ...\r\n $ Transportation Climate Impact: In Madison, about 40% of greenhouse gas emissions come from transportation. How do you think the city should go about reducing emissions from that sector over the next 5 years?                                                                                                                                                                                                       : chr [1:19] \"In addition to making investments in expanding our public transit system and bike network, I believe the city s\"| __truncated__ \"We must remain steadfast in our efforts to provide the best possible infrastructure for bicycles, safer streets\"| __truncated__ \"I like that Bus Rapid uses electric buses. It is a great start to ensure that our entire city fleet uses electr\"| __truncated__ \"Within transportation, I think Madison should be reducing its greenhouse gas emissions by offering clear, conve\"| __truncated__ ...\r\n $ district                                                                                                                                                                                                                                                                                                                                                                                                              : Factor w/ 20 levels \"District 1\",\"District 2\",..: 1 2 2 3 3 4 5 6 6 9 ...\r\n $ district_short                                                                                                                                                                                                                                                                                                                                                                                                        : chr [1:19] \"D1\" \"D2\" \"D2\" \"D3\" ...\r\n\r\nstr(responses_council_long)\r\n\r\ntibble [247 × 8] (S3: tbl_df/tbl/data.frame)\r\n $ Timestamp                          : chr [1:247] \"2/14/2023 22:30:06\" \"2/14/2023 22:30:06\" \"2/14/2023 22:30:06\" \"2/14/2023 22:30:06\" ...\r\n $ name                               : chr [1:247] \"John W. Duncan\" \"John W. Duncan\" \"John W. Duncan\" \"John W. Duncan\" ...\r\n $ Which district are you running for?: chr [1:247] \"District 1\" \"District 1\" \"District 1\" \"District 1\" ...\r\n $ district                           : Factor w/ 20 levels \"District 1\",\"District 2\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n $ district_short                     : chr [1:247] \"D1\" \"D1\" \"D1\" \"D1\" ...\r\n $ question                           : chr [1:247] \"When was the last time you took the bus?\" \"When was the last time you rode your bike to work, to school, or for an errand?\" \"What is the primary way you move around the city?\" \"  2023 brings many significant changes for Metro including the beginning of Bus Rapid Transit implementation, a\"| __truncated__ ...\r\n $ answer                             : chr [1:247] \"The last time I rode a public transit bus was when I lived in Boston, where commuter routes were convenient for\"| __truncated__ \"A lack of designated bike lanes along busy roads does not allow for safe commuting to work or to run errands in this area.\" \"The safest and best option for commuting for those of us on the far-west side is usually by car.\" \"I support the decision to increase the frequency and capacity of bus service along high-demand routes in the ci\"| __truncated__ ...\r\n $ topic                              : chr [1:247] NA NA NA \"General Vision:\" ...\r\n\r\nThe next step took me a while to figure out: How can I use the long data frame and iterate both over each candidate but also over each question to produce the desired markdown content?\r\nI first tried rendering separate html documents for each candidate, but the downside of that approach is that then I’d have to set up separate pages on WordPress and copy-and-paste dozens of html files.\r\nRendering a single html file required a different approach: the knitr::knit_child function.\r\nThe documentation in the “R Markdown Cookbook” wasn’t super easy to follow, but eventually I got it!\r\nFirst we define a function that take the candidate name as its input.\r\nThe responses are then filtered to just those of that candidate.\r\nThe district and district_short variables for that candidate are created–those are used for formatting the output.\r\nThen comes the key piece: we ask knitr to knit a child document using the output_council_child.Rmd template.\r\n\r\n\r\n\r\nThe child template is pretty simple: No yaml header and just a bunch of markdown plus inline R code to format the output.\r\nHere are the first few lines.\r\nNow that we have the function to knit the child document defined, we can just iterate over all the candidate names with lapply and then turn the list output into simple text separated by line breaks (the \\n).\r\nBecause of the chunk option results='asis' the Markdown generated just gets inserted into the rest of the Markdown in combined-report.Rmd and then knitted into html (or pdf).\r\n{r results='asis', echo=FALSE}\r\nres <- lapply(responses_mayor$`Your name`, knit_answer_child_mayor)\r\ncat(unlist(res), sep = '\\n')\r\nNow we just do the same for the common council candidates (different child template because the number of questions is different) and that’s it!\r\nNow you can open the generated html file in your text editor (or your web browser’s source viewer) and copy the content into the custom html block in WordPress!\r\nAnd this is what the end result looks like: https://www.madisonbikes.org/madison-spring-elections-2023/\r\nEnhancements\r\nOne key enhancement would be to automate the publishing to WordPress.\r\nRather than copying-and-pasting the html into WordPress, wouldn’t it be nice if the content could be pushed through the WordPress API?\r\nThere are some packages and tutorials on how to do this within R, but the packages are very old and I didn’t have much luck with the tutorials.\r\n\r\nOn the other hand, this may decrease response rates.↩︎\r\nUsing the knit button in RStudio only seems to produce the first specified output format.\r\nSo you may have to manually edit the yaml header.↩︎\r\nFor the next iteration, it may make sense to fix this in the form itself.↩︎\r\n",
    "preview": "posts/2023-02-18-using-knitr-and-rmarkdown-to-format-google-forms-responses/vote.jpg",
    "last_modified": "2023-02-19T09:00:07-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-03-bicycle-and-pedestrian-intersection-crashes-in-madison/",
    "title": "Bicycle and pedestrian intersection crashes in Madison",
    "description": "A quick look at crash data to support efforts to no longer allow right turns on red.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-12-03",
    "categories": [
      "Madison (WI)",
      "Vision Zero",
      "map"
    ],
    "contents": "\r\nThere are some efforts underway to no longer allow right turns on red in Madison.\r\nFor people walking and biking, right-turning motor vehicles pose a threat, as is demonstrated in this video of an intersection where I myself recently had a very close call. Other cities in the US, such as Washington (DC) or cambridge (MA) are in the process of eliminating right turns on red.\r\nFor our local effort, I offered to help with some crash data analysis. It is difficult to determine how many crashes can be directly attributed to right on red, we can at least answer the question: How many crashes involving people walking and biking occur at signalized intersections?\r\nCrash data\r\nCrash data is available from Community Maps the statewide crash data reporting portal.\r\nFor registered users, one of the available filters is for “intersection” crashes.\r\nWe start by downloading all crashes in the City and Town of Madison that have an intersection flag as well as the bicycle or pedestrian flag.\r\nScreenshot of Community Maps search options\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\nlibrary(tmap)\r\ntmap_mode(\"view\")\r\n\r\n\r\nped_crashes <- read_csv(\"data/crash-records.csv\")\r\nbike_crashes <- read_csv(\"data/crash-records(1).csv\")\r\nall_crashes <- rbind(ped_crashes, bike_crashes)\r\n\r\nall_crashes <- all_crashes |> \r\n  mutate(bike_ped = case_when(\r\n    PEDFLAG == \"Y\" ~ \"ped\",\r\n    BIKEFLAG == \"Y\" ~ \"bike\"\r\n  ),\r\n  severity = case_when(\r\n    INJSVR == \"K\" ~ \"Fatality\",\r\n    INJSVR == \"A\" ~ \"Serious injury\",\r\n    INJSVR == \"B\" ~ \"Minor injury\",\r\n    INJSVR == \"C\" ~ \"Possible injury\",\r\n    INJSVR == \"O\" ~ \"No injury\"\r\n  ),\r\n  severity = factor(severity, \r\n                    levels = c(\"Fatality\",\r\n                               \"Serious injury\",\r\n                               \"Minor injury\",\r\n                               \"Possible injury\",\r\n                               \"No injury\"))\r\n  )\r\n\r\n\r\nMapping the crashes shows that something weird is going on:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(st_as_sf(all_crashes, coords = c(\"LONGITUDE\", \"LATITUDE\")))+\r\n  tm_dots()\r\n\r\n\r\n\r\nCrashes in the middle of the ocean?\r\nNo: Crashes that were not geocoded and therefore have coordinates of 0,0.\r\nI looked up the crash reports for all these and manually added coordinates:\r\n\r\n\r\nShow code\r\n\r\n#manually fix locations of one unmapped crash\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245X0S\",]$LATITUDE <- 43.08952868325321\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245X0S\",]$LONGITUDE <- -89.48608767371645\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WX5\",]$LATITUDE <- 43.0608101752285\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WX5\",]$LONGITUDE <- -89.50257794338208\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0FV1GG7\",]$LATITUDE <- 43.05027679273423\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0FV1GG7\",]$LONGITUDE <- -89.4000044121047\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L048CTMJ\",]$LATITUDE <- 43.059824781314845\r\nall_crashes[all_crashes$DOCTNMBR == \"01L048CTMJ\",]$LONGITUDE <- -89.40061771700978\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WVQ\",]$LATITUDE <- 43.03278427434284\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WVQ\",]$LONGITUDE <- -89.4597152111326\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L048CTLT\",]$LATITUDE <- 43.07325624571523\r\nall_crashes[all_crashes$DOCTNMBR == \"01L048CTLT\",]$LONGITUDE <- -89.40242558562917\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0CR2KRW\",]$LATITUDE <- 43.07325624571523\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0CR2KRW\",]$LONGITUDE <- -89.40242558562917\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0W78HMJ\",]$LATITUDE <- 43.06057645635835\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0W78HMJ\",]$LONGITUDE <- -89.50293626341433\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WSZ\",]$LATITUDE <- 43.0831299001578\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0245WSZ\",]$LONGITUDE <- -89.47595215839556\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L05QT5LB\",]$LATITUDE <- 43.114230724048014\r\nall_crashes[all_crashes$DOCTNMBR == \"01L05QT5LB\",]$LONGITUDE <- -89.35812769493853\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L06ZV8W2\",]$LATITUDE <- 43.07322351699731\r\nall_crashes[all_crashes$DOCTNMBR == \"01L06ZV8W2\",]$LONGITUDE <- -89.40072273726425\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0CS9M1F\",]$LATITUDE <- 43.04172765662615\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0CS9M1F\",]$LONGITUDE <- -89.39313149033856\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0TS2DB6\",]$LATITUDE <- 43.06769902424961\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0TS2DB6\",]$LONGITUDE <- -89.4008236046175\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L07LCTM4\",]$LATITUDE <- 43.04634165079267\r\nall_crashes[all_crashes$DOCTNMBR == \"01L07LCTM4\",]$LONGITUDE <- -89.48893584984673\r\n\r\n# not 100% sure about this one; description kinda confusing\r\nall_crashes[all_crashes$DOCTNMBR == \"01L06KHWV6\",]$LATITUDE <- 43.033329627105026\r\nall_crashes[all_crashes$DOCTNMBR == \"01L06KHWV6\",]$LONGITUDE <- -89.40788999291192\r\n\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0TS2D8H\",]$LATITUDE <- 43.06202954006571\r\nall_crashes[all_crashes$DOCTNMBR == \"01L0TS2D8H\",]$LONGITUDE <- -89.40412105286146\r\n\r\n\r\n\r\nall_crashes_sf <- st_as_sf(all_crashes, coords = c(\"LONGITUDE\", \"LATITUDE\"))\r\n\r\n\r\nWe map again to make sure it looks good:\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"plot\")\r\ntm_shape(all_crashes_sf) +\r\n  tm_dots(\"bike_ped\", size = 1)\r\n\r\n\r\n\r\nSignalized intersections\r\nThe City’s open data portal has intersection controls for all intersections in the county.\r\nSignalized intersections are coded SL.\r\n\r\n\r\nShow code\r\n\r\nintersection_control <- st_read(\"data/Intersection_Control.geojson\")\r\n\r\nReading layer `Intersection_Control' from data source \r\n  `C:\\Users\\user1\\Documents\\website\\_posts\\2022-12-03-bicycle-and-pedestrian-intersection-crashes-in-madison\\data\\Intersection_Control.geojson' \r\n  using driver `GeoJSON'\r\nSimple feature collection with 8361 features and 5 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -89.83849 ymin: 42.84596 xmax: -89.00865 ymax: 43.29404\r\nGeodetic CRS:  WGS 84\r\n\r\nShow code\r\n\r\nsignals <- intersection_control |> \r\n  filter(IntControl == \"SL\")\r\n\r\n\r\nNow we create a 50-meter buffer1 around the signal locations and only keep crashes within that buffer:\r\n\r\n\r\nShow code\r\n\r\nst_crs(all_crashes_sf) <- 4326\r\n\r\n\r\ncrashes_at_signals <- all_crashes_sf[st_within(all_crashes_sf, st_buffer(signals, dist = 50)) %>% lengths > 0,]\r\n\r\n\r\nNow we can map all crashes at signalized intersections, separated into bike and bike crashes:\r\nBike crashes at signalized intersections\r\n\r\n\r\nShow code\r\n\r\npopup_vars <- c(\"DOCTNMBR\", \"bike_ped\", \"severity\", \"ACCDDATE\",\"ACCDHOUR\",\"ONRDWY\",\"ONRDWYDIR\",\"ATRDWY\",\"INTDIS\",\"INTDIR\",\"TOTFATL\",\"TOTINJ\",\"TOTVEH\",\"MNRCOLL\",\"ACCDTYPE\",\"AGGRFLAG\",\"BUSFLAG\",\"CMVFLAG\",\"CONSZONE\",\"DISTRCTD\",\"IMPAIRED\",\"INTRFLAG\",\"LANEDP1U\",\"LANEDP2U\",\"SPEEDFLAG\",\"TEENDRVR\",\"65+DRVR\")\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(crashes_at_signals |> filter(bike_ped == \"bike\")) +\r\n  tm_dots(\"severity\",\r\n          popup.vars = popup_vars) +\r\n  tm_layout(main.title = \"Bike intersection crashes\")\r\n\r\n\r\n\r\nPed crashes at signalized intersections\r\n\r\n\r\nShow code\r\n\r\ntm_shape(crashes_at_signals |> filter(bike_ped == \"ped\")) +\r\n  tm_dots(\"severity\",\r\n          popup.vars = popup_vars) +\r\n  tm_layout(main.title = \"Pedestrian intersection crashes\")\r\n\r\n\r\n\r\nSome summary statistics.\r\nOverall, there were 579 intersection crashes involving people walking and biking between January 2012 and now.\r\nOut of those, 337 (58%) happened at signalized intersections. These are the numbers of all intersection crashes, grouped by ped/bike and injury severity:\r\n\r\n\r\nShow code\r\n\r\nall_crashes |> \r\n  group_by(bike_ped, severity) |> \r\n  tally() |> \r\n  gt::gt()\r\n\r\n\r\nseverity\r\n      n\r\n    bike\r\n    Fatality\r\n2Serious injury\r\n32Minor injury\r\n176Possible injury\r\n58No injury\r\n33ped\r\n    Fatality\r\n5Serious injury\r\n55Minor injury\r\n137Possible injury\r\n62No injury\r\n19\r\n\r\nAnd here are the ones at signalized intersections only:\r\n\r\n\r\nShow code\r\n\r\ncrashes_at_signals |> \r\n  st_drop_geometry() |> \r\n  group_by(bike_ped, severity) |> \r\n  tally() |> \r\n  gt::gt()\r\n\r\n\r\nseverity\r\n      n\r\n    bike\r\n    Fatality\r\n1Serious injury\r\n19Minor injury\r\n103Possible injury\r\n37No injury\r\n20ped\r\n    Fatality\r\n4Serious injury\r\n31Minor injury\r\n75Possible injury\r\n37No injury\r\n10\r\n\r\n\r\n50 meters is based on some trial and error with different buffer sizes.\r\nToo small and you risk undercounting (e.g. crashes at large intersections); too big a buffer, and you may attribute crashes at a nearby unsignalized intersection to the signalized one.↩︎\r\n",
    "preview": "posts/2022-12-03-bicycle-and-pedestrian-intersection-crashes-in-madison/distill-preview.png",
    "last_modified": "2022-12-03T16:15:58-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-23-coffeeneuring-challenge-2022/",
    "title": "Coffeeneuring Challenge 2022",
    "description": "Making a map of this year's season",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-11-23",
    "categories": [
      "coffee",
      "map",
      "biking"
    ],
    "contents": "\r\nAlmost every year I participate in the Coffeeneuring Challenge. You can find out more what coffeeneuring is and check out my past adventures over on my bike blog. But basically you have to bike to seven different coffee locations over the course of about six weeks. Now that the 2022 season is over, it seemed like a good opportunity to use my coffeeneuring data for the #30DayMapChallenge.\r\n\r\n\r\nShow code\r\n\r\nlibrary(googlesheets4)\r\nlibrary(tidyverse)\r\nlibrary(tmap)\r\nlibrary(tmaptools)\r\nlibrary(sf)\r\nlibrary(extrafont)\r\n\r\n\r\nFirst we read in the spreadsheet where I kept track of my rides and destinations.\r\n\r\n\r\nShow code\r\n\r\ndestinations <- read_sheet(\"https://docs.google.com/spreadsheets/d/1QaZ0BPOxJs-wUg0LMardUEjCvDd8h6GUirUl4VOzBDU/edit?usp=sharing\") |> \r\n    separate(coordinates, into = c(\"lon\", \"lat\"), sep = \", \", convert = T) |> \r\n    st_as_sf(coords = c(\"lat\", \"lon\"))\r\n\r\ndestinations |> \r\n  kableExtra::kable()\r\n\r\n\r\nname\r\n\r\n\r\ndate\r\n\r\n\r\ncategory\r\n\r\n\r\nstrava\r\n\r\n\r\ngeometry\r\n\r\n\r\nCrema Café\r\n\r\n\r\n2022-10-15\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/7966997401\r\n\r\n\r\nPOINT (-89.32413 43.07862)\r\n\r\n\r\nParadiddle’s Café\r\n\r\n\r\n2022-10-09\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/7939047633\r\n\r\n\r\nPOINT (-88.99078 43.19471)\r\n\r\n\r\nAlice Good\r\n\r\n\r\n2022-10-21\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/7998212282\r\n\r\n\r\nPOINT (-89.53358 42.98808)\r\n\r\n\r\nHodge Podge\r\n\r\n\r\n2022-11-06\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/8079719303\r\n\r\n\r\nPOINT (-89.51824 42.99536)\r\n\r\n\r\nGrace\r\n\r\n\r\n2022-11-20\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/8146150714\r\n\r\n\r\nPOINT (-89.51179 43.09536)\r\n\r\n\r\nRock River, Byron\r\n\r\n\r\n2022-10-23\r\n\r\n\r\ncoffee outside\r\n\r\n\r\nhttps://www.strava.com/activities/8009683012\r\n\r\n\r\nPOINT (-89.25426 42.12329)\r\n\r\n\r\nLeopold Pump Track\r\n\r\n\r\n2022-10-30\r\n\r\n\r\ncoffee outside\r\n\r\n\r\nhttps://www.strava.com/activities/8044807447\r\n\r\n\r\nPOINT (-89.41957 43.03036)\r\n\r\n\r\nFirefly\r\n\r\n\r\n2022-10-16\r\n\r\n\r\ncoffee shop\r\n\r\n\r\nhttps://www.strava.com/activities/7973704511\r\n\r\n\r\nPOINT (-89.38395 42.92652)\r\n\r\n\r\nWe can map the destinations:\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(destinations) +\r\n  tm_dots(\"category\")\r\n\r\n\r\n\r\nYou may wonder about the one dot very far south. This was on a very long two-day ride to Illinois I did in October!\r\nNext we add lines for the actual rides to these destinations. I don’t have access to the Strava API, and so I manually downloaded the gpx files and saved them in a folder. We use purrr to read them in and simplify the tracks with st_simplify to reduce the size.\r\n\r\n\r\nShow code\r\n\r\nprocess_gpx <- function(x){\r\n  df <- read_GPX(x)\r\n  df[[1]] |> #read_GPX outputs list\r\n    st_simplify(dTolerance = 100) #distance is in meters\r\n}\r\n\r\ntracks <- list.files(\"data/\", full.names = T) |> \r\n  map_dfr(process_gpx)\r\n\r\n\r\nLet’s add the tracks to the above map:\r\n\r\n\r\nShow code\r\n\r\n  tm_shape(destinations)+\r\n  tm_dots(\"category\") +\r\n  tm_shape(tracks) + \r\n  tm_lines()\r\n\r\n\r\n\r\nWe have all the data elements together now. Let’s create a nice static map. For that we need a base map. The Stamen Watercolor is always a nice option. Note the +c(...) after the bounding box command: This enlarges the bounding box in each direction so that our destinations aren’t at the very edge of the map and to make the map a little more square.\r\n\r\n\r\nShow code\r\n\r\nbasemap <- read_osm(bb(destinations)+c(-.3, -.1, .3, .1), \r\n                    zoom = 10, \r\n                    type = \"stamen-watercolor\")\r\n\r\n\r\nRather than using color to distinguish between the category of destination, we can use custom icons with the tmap_icons() function. I created two icons in Inkscape, based on Fontawesome icons.\r\n\r\nNow it’s just a matter of putting all the layers together and making adjustments:\r\n\r\n\r\nShow code\r\n\r\nmy_icons <- tmap_icons(c(\"img/coffee-inside.png\", \"img/coffee-outside.png\"))\r\n\r\ntmap_mode(\"plot\")\r\ntm_shape(basemap) +\r\n  tm_rgb() +\r\n  tm_shape(tracks) +\r\n  tm_lines(lwd = 1.5, \r\n           col = \"darkgreen\") +\r\n  tm_shape(destinations) +\r\n  tm_symbols(\r\n    shape = \"category\",\r\n    shapes = my_icons,\r\n    size = .1,\r\n    title.shape = \"Type of destination\",\r\n    border.col = NULL,\r\n    alpha = .8\r\n  ) +\r\n  tm_text(\r\n    \"name\",\r\n    size = .5,\r\n    remove.overlap = T,\r\n    col = \"white\",\r\n    just = \"left\",\r\n    bg.color = \"darkgrey\",\r\n    bg.alpha = .7\r\n  ) +\r\n  tm_compass() +\r\n  tm_layout(\r\n    legend.outside = T,\r\n    main.title = \"Harald’s 2022 Coffeeneuring Season\",\r\n    fontfamily = \"Roboto Condensed\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-23-coffeeneuring-challenge-2022/distill-preview.png",
    "last_modified": "2022-11-25T11:35:40-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-19-the-color-of-roads/",
    "title": "The color of roads",
    "description": "What colors are most represented in the road names of Dane County?",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-11-19",
    "categories": [
      "Madison (WI)",
      "Dane County (WI)",
      "map"
    ],
    "contents": "\r\nSometimes you start a project and it ends up quite different from you had envisioned. I was going to do a quick and simple map for the #30DayMapChallenge theme “Blue.” Pull road data for Dane County, filter for names that contain “blue,” plot to a map with roads and road names. Sounds simple, right? Of course it is not.\r\nGetting all the data in is straightforward with tigris and tmaptools.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tigris)\r\noptions(tigris_use_cache = TRUE)\r\nlibrary(tmap)\r\nlibrary(tmaptools)\r\nlibrary(sf)\r\nlibrary(tidyverse)\r\nlibrary(extrafont)\r\nloadfonts(device = \"all\")\r\n\r\nwi_roads <- roads(\"WI\", \"Dane\")\r\n\r\n# county shapefile to calculate bounding box for basemap download\r\ndane <- tigris::counties(\"WI\", cb = T) %>% \r\n  filter(NAME == \"Dane\")\r\n\r\n#get basemap from OSM\r\ndane_osm <- read_osm(bb(dane), zoom = 10, type = \"stamen-toner\", )\r\n\r\nblue_dane <- wi_roads |> \r\n  mutate(blue = ifelse(str_detect(FULLNAME, \"[Bb]lue\"), \"blue\", \"not blue\")) |> \r\n  filter(blue == \"blue\") #keep only blue roads\r\n\r\nblue_dane\r\n\r\nSimple feature collection with 58 features and 5 fields\r\nGeometry type: LINESTRING\r\nDimension:     XY\r\nBounding box:  xmin: -89.83851 ymin: 42.9267 xmax: -89.02944 ymax: 43.23245\r\nGeodetic CRS:  NAD83\r\nFirst 10 features:\r\n        LINEARID            FULLNAME RTTYP MTFCC\r\n1  1103738774528 Blue Bill Park Dr S     M S1400\r\n2  1103738774530 Blue Bill Park Dr S     M S1400\r\n3   110689766034 N Blue Bill Park Dr     M S1400\r\n4  1106092748301    S Blue Mounds St     M S1400\r\n5   110689763919    E Blue Mounds Rd     M S1400\r\n6  1103676871652    E Blue Mounds Rd     M S1400\r\n7   110689763921    W Blue Mounds Rd     M S1400\r\n8   110689763922    W Blue Mounds Rd     M S1400\r\n9   110689762152       Blue Ridge Ct     M S1400\r\n10  110689765583   Blue Mountain Ave     M S1400\r\n                         geometry blue\r\n1  LINESTRING (-89.41119 43.14... blue\r\n2  LINESTRING (-89.41047 43.15... blue\r\n3  LINESTRING (-89.4134 43.158... blue\r\n4  LINESTRING (-89.74511 42.99... blue\r\n5  LINESTRING (-89.73772 42.95... blue\r\n6  LINESTRING (-89.74274 42.96... blue\r\n7  LINESTRING (-89.81902 42.96... blue\r\n8  LINESTRING (-89.81824 42.97... blue\r\n9  LINESTRING (-89.49303 43.07... blue\r\n10 LINESTRING (-89.81777 43.01... blue\r\n\r\nBut a map shows that more work is needed:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(dane_osm) +\r\n  tm_rgb() +\r\n  tm_shape(blue_dane) +\r\n  tm_lines(\"blue\", lwd = 2) +\r\n  tm_text(\"FULLNAME\", col = \"blue\")\r\n\r\n\r\n\r\nNot good: The labels are too crowded, and you can see that a single way with the same name often is split into multiple segments, e.g. Blue Mounds Trail on the western edge of the map. I spent hours trying to fix these things through one of the various geometric or other tools from the sf package. But in the end, nothing worked. (If you have suggestions on how to combine adjoining ways, I’m all ears!)\r\nInstead, I pivoted to making a map of all the road name colors in Dane County. The colors are extracted with simple regular expressions. This works well for all colors except “red” – RiveREDege Ct, LaREDo Ct, or EldRED St are just some of the false positives. For one county these can be cleaned manually, but for a larger dataset this would be a problem.\r\n\r\n\r\nShow code\r\n\r\n# deal with false-positive reds\r\nnot_red <- c(\"Riveredge Rd\", \r\n             \"Laredo Ct\",\r\n             \"Eldred St\", \r\n             \"Fredericksburg Ct\", \r\n             \"Frederick Ct\", \r\n             \"Arboredge Way\", \r\n             \"Frederick Cir\",\r\n             \"Redan Dr\",\r\n             \"Meredith Way\",\r\n             \"Claredon Dr\",\r\n             \"Frederick St\",\r\n             \"Fredericksburg Ln\",\r\n             \"Meredithe Ave\",\r\n             \"Arboredge Way\",\r\n             \"Fredenberg Rd\",\r\n             \"Mildred Ct\",\r\n             \"Hubred Ln\",\r\n             \"Covered Bridge Trl\",\r\n             \"Saddle Bred Ln\",\r\n             \"Saddlebred Ln\")\r\n\r\nblue_dane <- wi_roads |> \r\n  mutate(blue = case_when(str_detect(FULLNAME, \"[Bb]lue\") ~ \"blue\",\r\n                          str_detect(FULLNAME, \"[Bb]lack\") ~ \"black\",\r\n                          str_detect(FULLNAME, \"[Rr]ed\") ~ \"red\",\r\n                          str_detect(FULLNAME, \"[Yy]ellow\") ~ \"yellow\",\r\n                          str_detect(FULLNAME, \"[Bb]rown\") ~ \"brown\",\r\n                          str_detect(FULLNAME, \"[Gg]r[ae]y\") ~ \"grey\",\r\n                          str_detect(FULLNAME, \"[Gg]reen\") ~ \"green\",\r\n                          T ~ NA_character_)) |> \r\n  filter(!is.na(blue) & !blue %in% not_red)\r\n\r\n\r\nWhich color is most common?\r\n\r\n\r\nShow code\r\n\r\nblue_dane |> \r\n  st_drop_geometry() |>\r\n  group_by(blue) |> \r\n  tally(sort = T) |> \r\n  knitr::kable()\r\n\r\nblue\r\nn\r\ngreen\r\n88\r\nred\r\n69\r\nblue\r\n58\r\nblack\r\n40\r\ngrey\r\n13\r\nbrown\r\n8\r\nyellow\r\n8\r\n\r\nGreen! And here are the two maps, one dynamic and one static:\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(blue_dane) +\r\n  tm_lines(\"blue\",lwd = 3, alpha = .8,  id = \"FULLNAME\") +\r\n  tm_layout(title = \"The Color of Dane County Road Names\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"plot\")\r\ntm_shape(dane_osm) +\r\n  tm_rgb(alpha = .7) +\r\n  tm_shape(blue_dane) +\r\n  tm_lines(\"blue\",lwd = 3, alpha = .8) +\r\n  tm_layout(title = \"The Color of Dane County Road Names\", fontfamily = \"Roboto Condensed\", title.bg.color = \"lightgrey\", title.bg.alpha = .8)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-19-the-color-of-roads/distill-preview.png",
    "last_modified": "2022-11-19T19:51:08-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-16-food-establishment-data-from-madison/",
    "title": "#30DayMapChallenge: Restaurant heat map of Madison",
    "description": "Challenge 15: Food/drink",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-11-16",
    "categories": [
      "Madison (WI)",
      "map"
    ],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\nlibrary(tmap)\r\nlibrary(mapboxapi)\r\nlibrary(leaflet)\r\nlibrary(leaflet.extras)\r\n\r\n\r\nAnother quick #30DayMapChallenge post. The prompt for day 15: Food/drink. Public Health Madison & Dane County have a dataset on health inspections which includes all licensed restaurants in the county. The data are not geocoded, and so I use the mapboxapi package to geocode the locations:\r\n\r\n\r\nShow code\r\n\r\nestablishments <- read_csv(\"data/Licensed_Establishment.csv\")\r\n\r\n# geocode and save geocoded data\r\n# est_sf <- establishments %>% \r\n#   rowwise() %>% \r\n#   mutate(geometry = mb_geocode(AddressFull, output = \"sf\"),\r\n#          geometry = geometry$geometry)\r\n# \r\n# est_sf <- st_sf(est_sf)\r\n# \r\n# write_rds(est_sf, file = \"data/licensed_establishment_geocoded.RDS\")\r\nest_sf <- readRDS(\"data/licensed_establishment_geocoded.RDS\")\r\n\r\n\r\nThe dataset includes “all operational licensed establishments receiving routine inspections,” that is, not just restaurants. A quick tmap (we have to filter out a food cart that’s located in Milwaukee):\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"plot\")\r\nest_sf %>% \r\n  filter(AddrCity != \"MILWAUKEE\") %>% \r\n  tm_shape() +\r\n  tm_dots(\"EstablishmentType\") +\r\n  tm_layout(legend.outside = T)\r\n\r\n\r\n\r\nNow we filter to only establishments that are of the type “Primarily Restaurant” and create a heat map. Heat maps look cool, but I admittedly don’t fully understand how leaflet.extras generates them under the hood – certainly the map looks very different depending on the zoom level.\r\n\r\n\r\nShow code\r\n\r\nleaflet(est_sf %>% filter(EstablishmentType == \"Primarily Restaurant\")) %>% \r\n  addTiles() %>% \r\n  addHeatmap(radius = 6)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-16-food-establishment-data-from-madison/distill-preview.png",
    "last_modified": "2022-11-19T19:43:55-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-11-13-30daymapchallenge-5-minute-map/",
    "title": "#30DayMapChallenge: 5-minute map",
    "description": "5-minute isochrones around Metro Transit bus stops",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-11-13",
    "categories": [
      "Madison (WI)",
      "transit",
      "map"
    ],
    "contents": "\r\nUsually I don’t have the time to participate in the #30DayMapChallenge, but today I felt like putting together a map for challenge 13: 5-minute map. My interpretation of the prompt: How far can you walk within a 5 minutes from each of Madison Metro’s ~2100 bus stops?\r\n\r\n\r\nShow code\r\n\r\nlibrary(tmap)\r\nlibrary(sf)\r\nlibrary(mapboxapi)\r\nlibrary(tmaptools)\r\n\r\n# load bus stops from Madison Open Data portal\r\n# https://data-cityofmadison.opendata.arcgis.com/datasets/cityofmadison::metro-transit-bus-stops/explore\r\nstops <- st_read(\"www/data/Metro_Transit_Bus_Stops.geojson\")\r\n\r\nReading layer `Metro_Transit_Bus_Stops' from data source \r\n  `C:\\Users\\user1\\Documents\\website\\_posts\\2022-11-13-30daymapchallenge-5-minute-map\\www\\data\\Metro_Transit_Bus_Stops.geojson' \r\n  using driver `GeoJSON'\r\nSimple feature collection with 2103 features and 25 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -89.56386 ymin: 42.98772 xmax: -89.24482 ymax: 43.17654\r\nGeodetic CRS:  WGS 84\r\n\r\nShow code\r\n\r\n# create isochrones. API key required\r\n# walk_5min <- mb_isochrone(stops,\r\n#                           profile = \"walking\",\r\n#                           time = 5)\r\nwalk_5min <- readRDS(\"www/data/Metro_Transit_Bus_Stops.RDS\")\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# create bounding box around stop isochrones\r\nmetro_bb <- bb(walk_5min)\r\n\r\n#download basemap\r\nbasemap <- read_osm(metro_bb, zoom = 12, type = \"stamen-watercolor\")\r\n\r\ntmap_mode(\"plot\")\r\ntmap_options(check.and.fix = TRUE)\r\ntm_shape(basemap)+\r\n  tm_rgb(alpha = .8) +\r\ntm_shape(walk_5min) +\r\n  tm_polygons(alpha = .3) +\r\n  tm_shape(stops)+\r\n  tm_dots(alpha = .5) +\r\n  tm_layout(title = \"5-minute walksheds around\\nMadison Metro stops\", \r\n            title.position = c(\"LEFT\", \"TOP\")) +\r\n  tm_credits(\"Basemap: OpenStreetMap Contributors, Stamen, Data: City of Madison, Visualization: Harald Kliems\",\r\n             position = c(\"right\", \"BOTTOM\"),\r\n             size = 200, align = \"right\", )\r\n\r\n\r\nShow code\r\n\r\n            # title.bg.color = \"lightgrey\",\r\n            # title.bg.alpha = .8, title.position = c(\"LEFT\", \"TOP\"))\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-13-30daymapchallenge-5-minute-map/30daymapchallenge-5-minute-map_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-11-14T12:02:07-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/latest-commute-mode-share-madison-2021/",
    "title": "What's the latest on commute mode share in Madison?",
    "description": "Working from home has seen an unprecedented boom, while biking stagnates.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-10-12",
    "categories": [
      "Madison (WI)",
      "transportation",
      "American Community Survey"
    ],
    "contents": "\r\nOne of the high holidays for census data nerds is ACSmas: The annual release of 1-year estimates from the American Community Survey. Last year’s ACSmas was cancelled because of the pandemic, but this September we finally got our fix of data for 2021.\r\nACSmas is cancelled this year, everything sucks https://t.co/rGsjpg0WE7— dadbode miller (@bikepedantic) July 29, 2021 I was especially excited about the commuting data: What did the pandemic do to people’s trips to work? How much of a shift to working from home would we see? And was the data quality actually going to be good enough to draw any firm conclusions? What follows is an in-depth look at the ACS commuting data for the City of Madison.\r\nMode share over time\r\nACS mode share data is available all the way back to 2010. In previous posts on the Madison Bikes blog I had already looked at these time series data – and frankly, it was not that exciting: By and large the percentages of different modes of getting to worked stayed the same. Biking didn’t grow but neither shrunk. It seemed highly likely that the pandemic would have changed things up. And it did:\r\n\r\n\r\nShow code\r\n\r\nlibrary(tmap)\r\nlibrary(tidycensus)\r\nlibrary(tidyverse)\r\nlibrary(gt)\r\n\r\n# variables <- load_variables(dataset = \"acs1/subject\", year = \"2021\")\r\n\r\nget_msn_mode_share <- function(year) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"S0801\", geography = \"place\", state = 55, cache_table=T)\r\n  acs_data |> \r\n    filter(NAME == \"Madison city, Wisconsin\") |> \r\n    mutate(year = year)\r\n}\r\n\r\n\r\n# variable_readable = case_when(\r\n#   variable == \"S0801_C01_002\" ~ \"Drive\",\r\n#   variable == \"S0801_C01_009\" ~ \"Transit\",\r\n#   variable == \"S0801_C01_010\" ~ \"Walk\",\r\n#   variable == \"S0801_C01_011\" ~ \"Bike\",\r\n#   variable == \"S0801_C01_013\" ~ \"Work from home\",\r\n#   variable == \"S0801_C02_002\" ~ \"Drive, male\",\r\n#   variable == \"S0801_C02_009\" ~ \"Transit, male\",\r\n#   variable == \"S0801_C02_010\" ~ \"Walk, male\",\r\n#   variable == \"S0801_C02_011\" ~ \"Bike, male\",\r\n#   variable == \"S0801_C02_013\" ~ \"Work from home, male\",\r\n#   variable == \"S0801_C03_002\" ~ \"Drive, female\",\r\n#   variable == \"S0801_C03_009\" ~ \"Transit, female\",\r\n#   variable == \"S0801_C03_010\" ~ \"Walk, female\",\r\n#   variable == \"S0801_C03_011\" ~ \"Bike, female\",\r\n#   variable == \"S0801_C03_013\" ~ \"Work from home, female\",\r\n#   \r\n#   \r\n# )\r\n\r\n\r\n\r\nmsn_mode_share <- map_dfr(c(2010:2019, 2021), get_msn_mode_share)\r\n\r\nmsn_mode_share <- msn_mode_share |> \r\n  mutate(gender = case_when(str_detect(variable, \"^S0801_C01\") ~ \"total\",\r\n                            str_detect(variable, \"^S0801_C02\") ~ \"male\",\r\n                            str_detect(variable, \"^S0801_C03\") ~ \"female\"),\r\n         mode_readable = case_when(\r\n           str_detect(variable, \"S0801_C0[1-3]_002\") ~ \"Drive\",\r\n           str_detect(variable, \"S0801_C0[1-3]_009\") ~ \"Transit\",\r\n           str_detect(variable, \"S0801_C0[1-3]_010\") ~ \"Walk\",\r\n           str_detect(variable, \"S0801_C0[1-3]_011\") ~ \"Bike\",\r\n           str_detect(variable, \"S0801_C0[1-3]_013\") ~ \"Work from home\"))\r\n        \r\n# data frame for the ggrepel labels on the right of the plot\r\nmsn_mode_share_2021 <-  msn_mode_share |> \r\n  filter(year == 2021 & !is.na(mode_readable))\r\n\r\nmsn_mode_share |> \r\n  filter(!is.na(mode_readable) & gender == \"total\") |> \r\n  group_by(mode_readable, year) |> \r\n  ggplot(aes(year, estimate, color = mode_readable)) +\r\n  geom_line(size = 1.2) +\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate + moe), alpha = .8,\r\n                  size = 1,fatten = 1) +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_x_continuous(breaks = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2021), minor_breaks = NULL, limits = c(2010, 2022.5)) +\r\n  ylab(\"estimate (%)\") +\r\n  labs(title = \"Working from home almost quintupled\\nbetween 2019 and 2021\",\r\n       subtitle =\"City of Madison commute mode share, 2010-2021\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: Harald Kliems\") +\r\n    ggrepel::geom_text_repel(data = msn_mode_share_2021 |> filter(gender == \"total\"), aes(label = paste0(mode_readable, \" \", estimate, \"%\")), nudge_x = 1) +\r\n  theme(legend.position = \"none\")\r\n\r\n\r\n\r\nFrom 2019 to 2021, the share of people working from home almost quintupled! Fewer people drove to work, and commuting by bus became much less common. For walking and biking the changes were downward too but within the margins of error.\r\n\r\n\r\nShow code\r\n\r\nmsn_mode_share |> \r\n  filter(!is.na(mode_readable) & gender == \"total\" & year >= 2019) |> \r\n  group_by(mode_readable, year) |> \r\n  pivot_wider(names_from = year, values_from = c(estimate, moe)) |> \r\n  ggplot() +\r\n    geom_segment(aes(x=mode_readable, xend=mode_readable, y=estimate_2019, yend=estimate_2021), color=\"grey\", arrow = arrow(length = unit(2, \"mm\"))) +\r\n  geom_point(aes(x = mode_readable, y = estimate_2019), color = 2019) +\r\n    geom_point(aes(x = mode_readable, y = estimate_2021), color = 2021) +\r\n  geom_text(aes(x = mode_readable, \r\n                y = (estimate_2021 + estimate_2019)/2, \r\n                label = paste0(estimate_2021-estimate_2019, \"%\")),\r\n            nudge_x = .25)+\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  hrbrthemes::theme_ipsum() +\r\n  coord_flip() +\r\n  theme(panel.grid.major.y = element_blank()) +\r\n  ylab(\"estimate (%)\") +\r\n  xlab(element_blank()) +\r\n  labs(title = \"Change in commute mode share\",\r\n       subtitle =\"City of Madison, 2019-2021. Margins of error not shown\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: Harald\r\n       Kliems\") \r\n\r\n\r\n\r\nFor transportation planning, it’s important to not only look at percentages but also at absolute numbers: Fewer people driving to work means less peak hour congestion (which is not necessarily a good thing); fewer transit riders means lower fare revenues; and fewer bike commuters may lead to less support of biking infrastructure. And a large number of people working from home may be a problem for commercial real estate or fewer customers for downtown restaurants.\r\n\r\n\r\nShow code\r\n\r\nget_commute_total <- function(year) {\r\n  get_acs(geography = \"place\",\r\n                           state = \"WI\",\r\n                           survey = \"acs1\",\r\n                           year = year,\r\n                           table = \"C08006\",\r\n          summary_var = \"C08006_001\",\r\n          cache_table = T) |> \r\n    mutate(year = year,\r\n           )\r\n}\r\n\r\n\r\ncommute_numbers <- map_dfr(c(2019, 2021), get_commute_total)\r\n\r\ncommute_numbers_msn <- commute_numbers |> \r\n  filter(NAME == \"Madison city, Wisconsin\") |> \r\n  mutate(mode_readable = case_when(\r\n           str_detect(variable, \"C08006_001\") ~ \"Total\",\r\n           str_detect(variable, \"C08006_002\") ~ \"Drive\",\r\n           str_detect(variable, \"C08006_008\") ~ \"Transit\",\r\n           str_detect(variable, \"C08006_009\") ~ \"Bike\",\r\n           str_detect(variable, \"C08006_010\") ~ \"Walk\",\r\n           str_detect(variable, \"C08006_012\") ~ \"Work from home\")) |> \r\n  filter(!is.na(mode_readable))\r\n\r\ncommute_numbers_msn |> \r\n  pivot_wider(id_cols = mode_readable, names_from = year, values_from = c(estimate, moe)) |> \r\n  mutate(mode_readable = fct_reorder(mode_readable, estimate_2021)) |> \r\n  group_by(mode_readable) |> \r\n  mutate(moe_diff = moe_sum(moe = c(moe_2021, moe_2019), estimate = c(estimate_2021, estimate_2019))) |> \r\n  ggplot() +\r\n    geom_segment(aes(x=mode_readable, xend=mode_readable, y=estimate_2019, yend=estimate_2021), color=\"grey\", arrow = arrow(length = unit(2, \"mm\"))) +\r\n  geom_point(aes(x = mode_readable, y = estimate_2019), color = 2019) +\r\n    geom_point(aes(x = mode_readable, y = estimate_2021), color = 2021) +\r\n  geom_text(aes(x = mode_readable, \r\n                y = (estimate_2021 + estimate_2019)/2, \r\n                label = paste0(scales::number_format(style_positive = \"plus\",big.mark = \",\")(estimate_2021-estimate_2019), \" (±\", round(moe_diff, 0), \")\")),\r\n            nudge_x = .25)+\r\n  hrbrthemes::scale_color_ipsum(\r\n    #name = element_blank()\r\n    ) +\r\n  hrbrthemes::theme_ipsum() +\r\n  coord_flip() +\r\n  theme(panel.grid.major.y = element_blank()) +\r\n  ylab(\"number of workers\") +\r\n  xlab(element_blank()) +\r\n  labs(title = \"Change in number of commuters, 2019-2021\",\r\n       subtitle =\"City of Madison. Statistically insignifant changes greyed out\",\r\n       caption = \"American Community Survey 1-year estimates, Table C08006\\nVisualization: HaraldKliems\") +\r\n  gghighlight::gghighlight(abs(estimate_2021-estimate_2019) > moe_diff, use_direct_label = F)\r\n\r\n\r\n\r\nThe total number of workers and of bike and walk commuters didn’t change significantly. But there are about 21000 fewer drivers, 11,000 fewer bus commuters, and over 30,000 more people working from home. For a city of Madison’s size, these changes are big!\r\nWhat about those still going to the office?\r\nWhen I first shared the above graphs some people pointed out: “Working from home isn’t really commuting! What do these percentages look like when we only account for the people who still leave the house to get to work?” Or to put it differently: Before the pandemic, you and 99 of your coworkers went to the office every day, and 10 of you biked. In 2021, half of your coworkers now work from home. Of the remaining 50 people in the office, do you still have 5 (i.e. 10%) people who bike to work?\r\n\r\n\r\nShow code\r\n\r\nget_msn_mode_counts <- function(year) {\r\n  acs_data <- get_acs(year = year, survey = \"acs1\", table = \"C08006\", geography = \"place\", state = 55, cache_table=T, summary_var = \"C08006_001\")\r\n  acs_data |> \r\n    filter(NAME == \"Madison city, Wisconsin\") |> \r\n    mutate(year = year)\r\n}\r\n\r\nmsn_counts <- map_dfr(c(2019, 2021), get_msn_mode_counts)\r\n\r\nmsn_counts <- msn_counts |> \r\n  mutate(summary_est = case_when(year == 2019 ~summary_est - 9050,\r\n                                 year == 2021 ~ summary_est - 40279),\r\n         mode_readable = case_when(\r\n    variable == \"C08006_001\" ~ \"Total\",\r\n      variable == \"C08006_002\" ~ \"Drive\",\r\n      variable == \"C08006_003\" ~ \"Drove alone\",\r\n      variable == \"C08006_004\" ~ \"Carpooled\",\r\n      variable == \"C08006_008\" ~ \"Transit\",\r\n      variable == \"C08006_009\" ~ \"Bike\",\r\n      variable == \"C08006_010\" ~ \"Walk\",\r\n      variable == \"C08006_011\" ~ \"Other\",\r\n      variable == \"C08006_012\" ~ \"Work from home\"\r\n  ),\r\n  mode_share = estimate/summary_est) |> \r\n  filter(!is.na(mode_readable)) |> \r\n  filter(mode_readable %in% c(\"Drive\", \"Transit\", \"Bike\", \"Walk\", \"Other\")) |> \r\n  mutate(mode_readable = fct_relevel(mode_readable, \r\n                                     \"Other\",\r\n                                                \"Bike\", \r\n                                                \"Walk\", \r\n                                                \"Transit\", \r\n                                                \"Drive\")) |> \r\n  group_by(year,mode_readable) %>% \r\n    mutate(pos = cumsum(mode_share) - mode_share/2)\r\nmsn_counts |> \r\n  ggplot(aes(as.factor(year), mode_share, fill = mode_readable)) +\r\n  geom_col(position = \"stack\") +\r\n  geom_text(aes(label = mode_readable), position = position_stack(vjust = 0.5), color = \"white\",\r\n            size = 3) +\r\n  scale_y_continuous(labels = scales::label_percent()) +\r\n    hrbrthemes::scale_fill_ipsum(name = \"element_blank()\") +\r\n  hrbrthemes::theme_ipsum() +\r\n  ylab(\"Commute mode share estimate\") +\r\n  xlab(element_blank()) +\r\n  theme(legend.position = \"none\",   panel.grid.major.x = element_blank()) +\r\n  labs(title = \"Transit commutes were the big loser\",\r\n       subtitle =\"City of Madison, working from home excluded\",\r\n       caption = \"American Community Survey 1-year estimates, Table B08006\\nVisualization: Harald Kliems\")\r\n\r\n\r\n\r\nThis is the same data in table form.\r\n\r\n\r\nShow code\r\n\r\nmsn_counts |> \r\n  \r\n  pivot_wider(id_cols = mode_readable, names_from = year, values_from = mode_share, names_prefix = \"mode_share_\") |> \r\n  mutate(change = mode_share_2021- mode_share_2019) |> \r\n  select(mode_readable, mode_share_2019, mode_share_2021, change) |>\r\n  ungroup() |> \r\n  gt() |> \r\n  tab_header(title = \"Change in commute mode for workers not working from home\") |> \r\n  fmt_percent(columns = c(mode_share_2021, mode_share_2019, change), decimals = 0) |> \r\n  tab_spanner(\r\n    label = \"Mode share\",\r\n    columns = c(mode_share_2019, mode_share_2021)\r\n  ) |> \r\n  cols_label(mode_readable = \"Commute mode\", \r\n             mode_share_2021 = \"2021\",\r\n             mode_share_2019 = \"2019\",\r\n             change = \"Change (percentage points)\") |> \r\n   data_color(\r\n    columns = c(change),\r\n    colors = scales::col_numeric(\r\n      palette = \"viridis\",\r\n    domain = NULL )) |> \r\n   tab_source_note(\r\n    source_note = \"Data: American Community Survey 1-year estimates, Table B08006. Margins of error not shown.\"\r\n  )\r\n\r\n\r\nChange in commute mode for workers not working from home\r\n    Commute mode\r\n      \r\n        Mode share\r\n      \r\n      Change (percentage points)\r\n    2019\r\n      2021\r\n    Drive\r\n74%\r\n79%\r\n6%Transit\r\n10%\r\n4%\r\n−6%Bike\r\n4%\r\n4%\r\n−0%Walk\r\n11%\r\n11%\r\n1%Other\r\n1%\r\n2%\r\n0%Data: American Community Survey 1-year estimates, Table B08006. Margins of error not shown.\r\n    \r\n\r\nCalculating the margins of error for these estimates is complex and I have not done that. But similar to the error bars we have seen above, changes of less than 2% points are probably not meaningful. So what do we take away from the table and chart? Of the people who still commute to the office, taking the bus is much less common while driving has gone up. The other percentages are basically unchanged. This makes sense: Bus service was reduced and many people perceived riding the bus as a risk for infection. Conversely, peak hour congestion for drivers went down (remember: overall the number of people commuting by car went down even when their percentage here went up) and some employers reduced or waived parking fees.\r\nMode share by gender\r\nDid commuting trends in Madison differ by gender?\r\nEspecially for biking, research has shown that in the US women are underrepresented in bike commuting, take fewer trips on bike share bikes, and generally have different travel patterns compared to men. (Hosford and Winters 2019; Ravensbergen, Fournier, and El-Geneidy 2022) In the 2021 data for Madison, we do not see these differences.\r\nLooking only at the estimates, it may appear that for bike commuting there is indeed a gender difference: 3.6% of men rode their bike to work whereas only 2% of women did so. But when we look at the error bars around the estimates, we see that for all modes they overlap between the two genders and therefore the differences we see in the data may well be by chance.\r\n\r\n\r\nShow code\r\n\r\n# faceted plot by gender\r\nmsn_mode_share |> \r\n  filter(!is.na(mode_readable)) |> \r\n  group_by(mode_readable, year, gender) |> \r\n  ggplot(aes(year, estimate, color = mode_readable)) +\r\n  geom_line(size = 1.5) +\r\n  hrbrthemes::scale_color_ipsum(name = \"element_blank()\") +\r\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate + moe), size = 1.3, fatten = 1.5, alpha = .7) +\r\n  hrbrthemes::theme_ipsum() +\r\n  scale_x_continuous(breaks = c(2010, 2012, 2014, 2016,  2019, 2021), minor_breaks = NULL, limits = c(2010, 2023)) +\r\n  ylab(\"estimate (%)\") +\r\n  labs(title = \"How people get to work in Madison does not differ by gender\",\r\n       subtitle =\"City of Madison commute mode share, 2010-2021\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: Harald Kliems\") +\r\n  geom_text(data = msn_mode_share_2021, aes(label = paste0(mode_readable, \" \", estimate, \"%\")), nudge_x = 1) +\r\n  theme(legend.position = \"none\") +\r\n  facet_wrap(~ fct_relevel(gender, \"total\", \"female\", \"male\"))\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nmsn_mode_share_2021 |> \r\n  filter(gender != \"total\") |> \r\n  ggplot(aes(fct_reorder(mode_readable, estimate), estimate, color = gender)) +\r\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate + moe), \r\n                  size = 1.3, \r\n                  fatten = 1.1,\r\n                  #alpha = .7,\r\n                  position = position_dodge(width = 0.3)) +\r\n  coord_flip() +\r\n  hrbrthemes::scale_color_ipsum(name = element_blank(),\r\n                                breaks = c(\"male\", \"female\"),\r\n                                guide = guide_legend(override.aes = list(shape = NA, size = 5))) +\r\n  hrbrthemes::theme_ipsum() +\r\n  theme(panel.grid.major.y = element_blank()) +\r\n  ylab(\"estimate (%)\") +\r\n  xlab(element_blank()) +\r\n  labs(title = \"How people get to work in Madison\\ndoes not differ by gender\",\r\n       subtitle = \"2021 estimates and 90% margin of error\",\r\n       caption = \"American Community Survey 1-year estimates, Table S0801\\nVisualization: Harald Kliems\") +\r\n   annotate(\r\n    geom = \"curve\", x = \"Transit\", y = 15, xend = \"Bike\", yend = 3, \r\n    curvature = -.3, arrow = arrow(length = unit(2, \"mm\"))\r\n  ) +\r\n  annotate(geom = \"text\", y = 15, x = \"Transit\", label = \"Error bars still overlap\", hjust = \"left\")\r\n\r\n\r\n\r\nMode share by race and ethnicity\r\nMuch has been written about the racial disparities of the COVID epidemic. Infection rates, the outcomes of an infection, and who was and wasn’t able to work from home often followed the patterns of structural racism. The American Community Survey data does provide a breakdown of commute modes by race. However, the more you break down the data into smaller categories, the larger the margins of error get. We saw above that for the total population of Madison or for the mode share by gender, the margins of error were already sizable. If we now break down the data into race and ethnicity categories, this becomes even more problematic. And because Madison is predominantly White, the error margins are largest for non-White population groups. So when you look at the following graph, note the wide error bars and not just the point estimates. Whenever the error bars overlap, the difference between the two groups is likely due to chance.\r\n\r\n\r\nShow code\r\n\r\nget_commute_by_race <- function(table_id) {\r\n  get_acs(geography = \"place\",\r\n                           state = \"WI\",\r\n                           survey = \"acs1\",\r\n                           year = 2021,\r\n                           table = table_id,\r\n          summary_var = paste0(table_id, \"_001\"),\r\n          cache_table = T) |> \r\n    mutate(table = table_id,\r\n           )\r\n}\r\n\r\n\r\ntables <- c(\"B08105A\", \r\n            \"B08105B\", \r\n            \"B08105C\", \r\n            \"B08105D\", \r\n            \"B08105E\",\r\n            \"B08105F\",\r\n            \"B08105G\",\r\n            \"B08105H\",\r\n            \"B08105I\")\r\n\r\ncommute_by_race <- map_dfr(tables, get_commute_by_race)\r\n\r\ncommute_by_race_msn <- commute_by_race |> \r\n  filter(NAME == \"Madison city, Wisconsin\") |> \r\n  mutate(race_ethnicity = case_when(str_detect(table, \"A$\") ~ \"White Alone\",\r\n                                    str_detect(table, \"B$\") ~ \"Black/African American\",\r\n                                    str_detect(table, \"C$\") ~ \"American Indian and Alaska Native Alone\",\r\n                                    str_detect(table, \"D$\") ~ \"Asian\",\r\n                                    str_detect(table, \"E$\") ~ \"Native Hawaiian Alone\",\r\n                                    str_detect(table, \"F$\") ~ \"Other Race Alone\",\r\n                                    str_detect(table, \"G$\") ~ \"Two or more races\",\r\n                                    str_detect(table, \"H$\") ~ \"Non-Hispanic White\",\r\n                                    str_detect(table, \"I$\") ~ \"Hispanic/Latino\"),\r\n         mode_readable = case_when(\r\n           str_detect(variable, \"B08105[:alpha:]_001\") ~ \"Total\",\r\n           str_detect(variable, \"B08105[:alpha:]_002\") ~ \"Drove alone\",\r\n           str_detect(variable, \"B08105[:alpha:]_003\") ~ \"Carpooled\",\r\n           str_detect(variable, \"B08105[:alpha:]_004\") ~ \"Transit\",\r\n           str_detect(variable, \"B08105[:alpha:]_005\") ~ \"Walked\",\r\n           str_detect(variable, \"B08105[:alpha:]_006\") ~ \"Cab, motorcycle, bike, other\",\r\n           str_detect(variable, \"B08105[:alpha:]_007\") ~ \"Work from home\")) |> \r\n  filter(!is.na(estimate))\r\n\r\n\r\np <- commute_by_race_msn |> \r\n  mutate(proportion = estimate / summary_est,\r\n         prop_moe = moe_prop(estimate, summary_est, moe, summary_moe),\r\n         ratio_moe = moe_ratio(estimate, summary_est, moe, summary_moe)) |> \r\n  filter(mode_readable != \"Total\" & race_ethnicity != \"Two or more races\" & race_ethnicity != \"White Alone\") |> \r\n  ggplot(aes(fct_reorder(mode_readable, proportion), proportion, color = race_ethnicity)) +\r\n  geom_pointrange(aes(ymin = proportion - prop_moe, ymax = proportion + prop_moe),\r\n                  position = position_dodge(width = 0.3)) +\r\n  coord_flip() +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  hrbrthemes::scale_color_ipsum(name = element_blank()) +\r\n  hrbrthemes::theme_ipsum() +\r\n  theme(panel.grid.major.y = element_blank(),\r\n        legend.position = \"bottom\") +\r\n  ylab(\"mode share estimate and margin of error\") +\r\n  xlab(element_blank())+\r\nlabs(title = \"Madison commute mode share by race/ethnicity\",\r\n       subtitle = \"2021 estimates and 90% margin of error\",\r\n       caption = \"American Community Survey 1-year estimates, Tables B08105A-I\\nVisualization: Harald Kliems\")\r\n\r\np+\r\n  #drive alone\r\n  annotate(\r\n    geom = \"curve\", x = 5.4, y = .70, xend = 6.1, yend = .561, \r\n    curvature = -.2, arrow = arrow(length = unit(2, \"mm\")), \r\n  ) +\r\n    annotate(\r\n    geom = \"curve\", x = 5.4, y = .70, xend = 5.95, yend = .575, \r\n    curvature = -.7, arrow = arrow(length = unit(2, \"mm\")), \r\n  ) +\r\n  annotate(geom = \"text\", y = .705, x = 5.4, label = \"Error bars don't overlap\", hjust = \"left\") +\r\n  #work from home\r\n    annotate(\r\n    geom = \"curve\", x = 5.7, y = .27, xend = 5, yend = .23, \r\n    curvature = -.2, arrow = arrow(length = unit(2, \"mm\")) \r\n  ) +\r\n  annotate(geom = \"text\", y = .26, x = 5.8, label = \"White and Asian commuters are\\nmore likely to work from home\", hjust = \"right\") +\r\n    #carpool\r\n    annotate(\r\n    geom = \"curve\", x = 3.8, y = .23, xend = 3.1, yend = .125, \r\n    curvature = .3, arrow = arrow(length = unit(2, \"mm\")) \r\n  ) +\r\n  annotate(geom = \"text\", y = .25, x = 3.8, label = \"LatinX workers carpool the most\\n(but note that wide margin of error)\", hjust = \"left\")\r\n\r\n\r\n\r\nLet’s go through mode by mode:\r\nFor driving alone, all the error bars overlap, except for non-Hispanic White compared to Black. Black commuters appear to drive to work at a higher proportion than non-Hispanic White ones.\r\nFor working from home we see a split between non-Hispanic White and Asian on the higher end and Black and Hispanic/Latino on the lower end. Much has been written how some jobs that were deemed essential, such as care or service work as well as many manufacturing jobs, are ones that can’t be done remotely and that are often done by Black or Hispanic workers.\r\nThe share of people walking to work doesn’t differ significantly between any of the groups.\r\nCarpooling appears to be especially common among Hispanic/LatinX workers. But note the wide error bar again: The only pairing where the bars do not overlap is between Hispanic/LatinX and non-Hispanic White.\r\nThe next category lumps together cabs, motorcycles, bicycles, and other means of transportation to work. Like with walking, there are no significant differences between the groups.\r\nThe same is true for transit: All the error bars overlap, i.e. no significant differences.\r\nThere’s more than ACS: Bike count analysis webinar\r\nWebinarThe American Community Survey only captures commuting, and trips to work only make up about 20% of all our trips. There are other limitations to the data, e.g. for work trips that involve more than one mode or for people on hybrid work arrangements. Therefore we can only conclude so much from it and should try to include other sources of data about biking in Madison.\r\nOne such source are the numerous bike counters that we have on our paths and roads. If you live in Madison, you are probably familiar with the Eco-Counter displays on the Southwest Path at Camp Randall and on the Cap City Trail at North Shore Drive.\r\nBut there are numerous other counters throughout the city, and we’re in the fortunate position to have help with analyzing the data from these counters: Madison Bikes together with Bike Fitchburg were awarded a data analysis grant from the League of American Bicyclists and Eco-Counter!\r\nWith the help from the city, we shared loads of counter data with them and the data analysis specialists at Eco Counter will analyze it.\r\nI haven’t see the results yet, but on October 26 you can join a free webinar where we will present results from the analysis and show how they are important for bike advocacy. Sign up here.\r\n\r\n\r\n\r\nHosford, Kate, and Meghan Winters. 2019. “Quantifying the Bicycle Share Gender Gap.” Findings, November. https://doi.org/10.32866/10802.\r\n\r\n\r\nRavensbergen, Léa, Juliette Fournier, and Ahmed El-Geneidy. 2022. “Exploratory Analysis of Mobility of Care in Montreal, Canada.” Transportation Research Record, July, 03611981221105070. https://doi.org/10.1177/03611981221105070.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/latest-commute-mode-share-madison-2021/analysis_2021_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2023-04-04T08:14:31-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 949
  },
  {
    "path": "posts/2022-08-26-using-r-markdown-and-msgxtractr-to-process-an-open-records-request/",
    "title": "Wrangling unwieldy open records data",
    "description": "Using R, {msgxtractor}, and R Markdown to wrangle a set of msg files",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-08-26",
    "categories": [
      "open records",
      "purrr",
      "RMarkdown"
    ],
    "contents": "\r\nA friend recently asked for assistance with data she received from an open records request.\r\n\r\n\r\nemotionally I was not prepared for WISDOT to send me a Box folder filled with documents that I can’t figure out how to download in response to my obnoxious open records request from ~6 months ago\r\n\r\n— marybeth (unfortunately) (@marbeff) August 16, 2022\r\n\r\nThe data she received consisted of a zip file with 70-something *.msg files. This is a proprietary Microsoft file format used by Outlook for storing emails. I quickly checked if there was an R package for handling the data, and indeed there is: msgextractor. With this package and some R and RMarkdown I figured converting the files to something more convenient would not be too difficult.\r\nIn the end it took me much longer than I had hoped, but it works. The process has two pieces: An R script with a function to read in a single email, extract some information, and save the email’s attachments in a separate folder. The second piece: A RMarkdown .Rmd template that uses variables generated in the function to create an HTML file with email metadata, the body, and links to the attachments. The R then script then iterates the function over the list of *.msg files with the walk function from the {purr} package. This creates one HTML file for each of the emails.\r\nHere’s the R script:\r\n\r\n\r\nlibrary(msgxtractr) #install with devtools::install_github(\"hrbrmstr/msgxtractr\") \r\nlibrary(tidyverse)\r\nlibrary(here)\r\n\r\n# read list of msg files in folders\r\nfiles <- list.files(\"data/All Files/\")\r\n\r\n# extract and save attachments and return paths\r\nwrite_msg_files <- function(msg_file){\r\n  msg <- read_msg(paste0(\"data/All Files/\", msg_file))\r\n  dir_name <- paste0(\"output/\", msg_file)\r\n  dir.create(dir_name)\r\n  save_attachments(msg_obj = msg, path = dir_name)\r\n  #return path and file names for attachments\r\n  attachment_names <- list.files(dir_name)\r\n  attachment_paths <- paste0(msg_file, \"/\", attachment_names)\r\n  # contents for output documents\r\n  email <- data_frame(email = msg_file,\r\n                   from = unlist(msg$headers$From),\r\n                   to = unlist(msg$headers$To),\r\n                   cc = unlist(msg$headers$CC),\r\n                   subject = unlist(msg$headers$Subject),\r\n                   date = unlist(msg$headers$Date),\r\n                   body = paste(msg$body$text, collapse = \"\\n\"),\r\n                   attachments = list(attachment_paths))\r\n  \r\n  #render output document\r\n  rmarkdown::render(\r\n    input = \"output_template.Rmd\",\r\n    output_file = here(\"output\", paste0(msg_file, \".html\"))\r\n  )\r\n}\r\n\r\n\r\n# iterate over list of files\r\nwalk(files, write_attachments)\r\n\r\n\r\n\r\nAnd this is what the output template looks like:\r\n\r\n    ---\r\n    output: html_document\r\n    ---\r\n\r\n    ```{r setup, include=FALSE}\r\n    knitr::opts_chunk$set(echo = F)\r\n```\r\n\r\n    ```{r}\r\n    library(msgxtractr)\r\n    library(tidyverse)\r\n    ```\r\n    # `r email[1,]$email`\r\n\r\n    **From:** `r email[1,]$from`\r\n\r\n    **To:** `r email$to`\r\n\r\n    **CC:** `r email$cc`\r\n\r\n    **Date:** `r email$date`\r\n\r\n    **Subject:** `r email$subject`\r\n\r\n    ```{r results='asis'}\r\n    cat(email[1,]$body)\r\n    ```\r\n\r\n    ## Attachments\r\n    ```{r results='asis'}\r\n    #creates markdown links to attachments\r\n    map_chr(email$attachments[[1]], ~paste0(\"[\", ., \"](\", ., \")\"))\r\n    ```\r\n\r\nThis works nicely, with one exception: There were two msg files whose content was another msg file as an attachment (which in turn had attachments). msgxtractor currently can’t deal with these and so I had to remove them from the dataset.\r\nHere’s what the output:\r\nScreenshot of html outputMaybe this can be useful for others. The repository is on Github\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-08-26T18:28:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/how-much-housing-is-being-built-in-madison/",
    "title": "How much housing is being built in Madison?",
    "description": "A look at housing permit data over time.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-08-06",
    "categories": [
      "housing",
      "Madison (WI)"
    ],
    "contents": "\r\n\r\nContents\r\nPopulation growth\r\nHousing permits over time\r\nTypes of housing\r\n\r\nMuch of the US is in a housing crisis, and Madison is no exception. Our population is growing, and the growth of housing stock has not kept pace, leading to higher rents and real estate prices. There have been iniatives to increase the production of housing, but how successful have they been? Inspired by a thread on Twitter, this post looks at building permit data.\r\n\r\n\r\nShow code\r\n\r\nlibrary(dplyr)\r\nlibrary(readr)\r\nlibrary(tidyr)\r\nlibrary(lubridate)\r\nlibrary(ggplot2)\r\nlibrary(tidycensus)\r\nlibrary(forcats)\r\n\r\n\r\n\r\nPopulation growth\r\nMadison has been and continues to be one of the fasted growing cities in the state of Wisconsin. Between 1970 and 2020, the city added almost 100,000 new residents. That is an increase of about 56 percent.1\r\n\r\n\r\nShow code\r\n\r\n# Population time series Madison city\r\npop <- read_csv(\"data/nhgis0002_ts_nominal_place.csv\")\r\n\r\n\r\npop %>% \r\n  filter(NHGISCODE == \"G55048000\" & YEAR %in% c(\"1970\", \"1980\", \"1990\", \"2000\", \"2010\", \"2020\")) %>% \r\n  # mutate(difference = AV0AA - lag(AV0AA)) %>% \r\n  # relocate(difference)\r\n  ggplot(aes(as.numeric(YEAR), AV0AA)) +\r\n  geom_line(size = 1.5) +\r\n  xlab(\"Year\") +\r\n  ylab(\"Population\")+\r\n  scale_y_continuous(labels = scales::comma, limits = c(0, 275000)) +\r\n  theme(panel.grid.minor.x = element_blank()) +\r\n  labs(caption = \"Data: US Census, IPUMS NHGIS, University of Minnesota, www.nhgis.org\\nVisualization: Harald Kliems\",\r\n       title = \"Madison's population grew from\\n173,000 in 1970 to 270,000 in 2020 (+56%)\") +\r\n  hrbrthemes::theme_ipsum_rc()\r\n\r\n\r\n\r\n\r\nThese new residents all need housing, and in general household sizes have been shrinking, requiring even more housing units. What does housing production look like in the city and the surrounding metro area?\r\nHousing permits over time\r\nThe Office of Policy Development and Research within the Department of Housing and Urban Development (HUD) keeps track of building permits issues on a monthly basis. Monthly data is choppy: A single large development can easily throw off the total in a month. Thus we look at the data in half year periods:\r\n\r\n\r\nShow code\r\n\r\n# housing permits Madison city\r\npermits <- read_csv(\"data/BuildingPermits(1).csv\")\r\n\r\n#housing permits for Madison CBSA\r\nCBSA <- read_csv(\"data/BuildingPermits - Madison CBSA.csv\")\r\n\r\n# combine and prep city and CBSA data\r\npermits_all <- permits %>% \r\n  rbind(CBSA) %>% \r\n    filter(Location == \"Madison, WI\" | Location == \"MADISON\") %>% \r\n    mutate(date = ym(paste(Year, Month)),\r\n         quarter = quarter(date),\r\n         semester = semester(date),\r\n         place = case_when(Location == \"MADISON\" ~ \"City of Madison\",\r\n                              Location == \"Madison, WI\" ~ \"Madison metro area\")) %>% \r\n  filter(date < ymd(\"2022-07-01\"))  #remove 0 values for future months\r\n\r\n  \r\n\r\n\r\npermits_all %>% \r\n  filter(`Series Code` == 1) %>% #total units\r\n  group_by(Year, semester, place) %>% \r\n  summarise(permits = sum(Permits), date) %>% \r\n  distinct(Year, semester, permits, place, .keep_all = T) %>% \r\n  pivot_wider(names_from = place, values_from = permits) %>% \r\n  mutate(metro_without_city = `Madison metro area` - `City of Madison`) %>% \r\n  pivot_longer(cols = c(metro_without_city, `City of Madison`, `Madison metro area`), names_to = \"place\", values_to = \"permits\") %>% \r\n  filter(place %in% c(\"metro_without_city\", \"City of Madison\")) %>% \r\n  ggplot(aes(date, permits, color = fct_reorder(place, permits))) +\r\n  geom_line(size = 1.5) +\r\n  ylim(0,2000)+\r\n  hrbrthemes::scale_color_ipsum(name = NULL, labels = c(\"Madison metro area,\\nwithout City\", \"City of Madison only\")) +\r\n  hrbrthemes::theme_ipsum_rc() +\r\n  theme(legend.position = \"right\") +\r\n  labs(title = \"New housing units permitted in the City of\\n Madison and the surrounding metro area\", subtitle = \"Both the total number and the trend over\\ntime are similar between the two\",\r\n       caption = \"Data: https://socds.huduser.gov/permits/\\nVisualization: Harald Kliems\\nMadison metro area encompasses Dane, Green, Iowa, and Columbia counties\")\r\n\r\n\r\n\r\n\r\nHousing production in the City of Madison was strong during the early aughts. And in the surrounding metro area it was even stronger. But once the financial crisis hit, it took almost a decade until housing construction returned to previous levels. We can also see the shock the early stage of the COVID-19 pandemic, and how that permitting backlog cleared over the course of 2020.\r\nHow do the 946 units permitted in Madison in the first half of 2022 compare to other cities? Not too badly:\r\n\r\nSome other housing start totals at the half-year mark:Seattle: 5,542 unitsDenver: 4,707 unitsBoston: 2,610 unitsWashington: 2,471 unitsPhiladelphia: 1,265 unitsPortland: 873 unitsSan Jose: 854 unitsOakland: 755 unitsSan Francisco: 660 units (lol, lmao) https://t.co/137zelFEsN— Alex Schieferdecker (@alexschief) August 2, 2022 We permitted more units than Portland (pop 652k), Oakland (441k), San Jose (1,013k), and San Francisco (874k)!\r\nTypes of housing\r\nThe previous section describes the growth in total units of housing. But were these newly permitted units single-family homes or parts of multi-family housing?\r\n\r\n\r\nShow code\r\n\r\npermits_all %>% \r\n  filter(`Series Code` <= 2) %>% \r\n  pivot_wider(names_from = Series, values_from = Permits, id_cols = c(Location, Year, Month)) %>% \r\n  janitor::clean_names() %>% \r\n  mutate(location = case_when(location == \"Madison, WI\" ~ \"madison_metro\",\r\n                              location == \"MADISON\" ~ \"madison_city\")) %>% \r\n  group_by(year, location) %>% \r\n  summarise(total_units = sum(total_units), sf_units = sum(units_in_single_family_structures)) %>% \r\n  pivot_wider(names_from = location, values_from = c(total_units, sf_units)) %>% \r\n  mutate(metro_without_city_sf = sf_units_madison_metro - sf_units_madison_city,\r\n         metro_without_city_total = total_units_madison_metro - total_units_madison_city,\r\n         prop_sf_madison_city = sf_units_madison_city/total_units_madison_city,\r\n         prop_sf_metro_without_city = metro_without_city_sf/metro_without_city_total) %>% \r\n  select(year, starts_with(\"prop\")) %>% \r\n  pivot_longer(cols = starts_with(\"prop\"), \r\n               names_to = \"place\", \r\n               values_to = \"prop_sf\",\r\n               names_prefix = \"prop_sf_\") %>% \r\nggplot(aes(year, prop_sf, color = fct_reorder(place, -prop_sf))) +\r\n  geom_line(size = 1.5) +\r\n  labs(title = \"Proportion of newly permitted single family homes\\nto total units permitted\",\r\n       subtitle = \"The share of SF homes to total units permitted has declined over time. In the City of Madison the\\nshare is much lower than in the surrounding metro area.\",\r\n       caption = \"Data: https://socds.huduser.gov/permits/\\nVisualization: Harald Kliems\",\r\n       x = \"year\",\r\n       y = \"Proportion single-family\") +\r\n  hrbrthemes::scale_color_ipsum(name = NULL, \r\n                       labels = c(\"Madison metro area,\\nwithout City\", \"City of Madison only\")) +\r\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1), \r\n                     limits = c(0,1)) +\r\n  hrbrthemes::theme_ipsum_rc()\r\n\r\n\r\n\r\n\r\nIt is probably no surprise that in the metro area, single-family homes make a higher share of newly permitted units than in Madison. But both in the metro area and the city, the proportion of newly built single-family homes has sharply declined from its heights during and shortly after the Great Recession. In the city, only about 20% of units built are single-family homes, and in the metro area the share has dipped below 50% in recent years.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSome part of the growth is the result of expanding city limits; however, in general the newly annexed areas did not have large populations.]↩︎\r\n",
    "preview": "posts/how-much-housing-is-being-built-in-madison/new_housing_madison_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-06T07:00:03-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/how-far-can-you-go/",
    "title": "How far can you go?",
    "description": "Comparing how far you can travel by bus, bike, and e-bike in Madison, before and after the bus network redesign",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-04-30",
    "categories": [
      "transit",
      "map",
      "Madison (WI)"
    ],
    "contents": "\r\n\r\nContents\r\nExtracting the maps\r\nIsochrones with openrouteservice\r\nThe app\r\n\r\nMadison Metro is redesigning their bus network. As someone who is passionate about transit and as a member of the Transportation Commission, I have been following the project closely. When I saw a number of isochrone maps showing how far you can get by transit within 45 minutes, I was curious: How do those isochrones compare to ones by bike from the same location? I decided to build a Shiny app. This post provides some technical detail. To go straight to the map, click here\r\n\r\n\r\n\r\nFigure 1: Screenshot of the app\r\n\r\n\r\n\r\nExtracting the maps\r\nMetro published the maps as appendices in pdf format, with one map per page. With a large number of maps, extracting them manually and renaming each one seemed like a lot of work. Instead I let R handle the process.\r\nExtracting the pages from the pdf as images is easy with the magick package. But how do you name the images in a way that reflects their location? So, for example, “Raymond at McKenna.png”? Well, all the maps have the name of the location in the same place on the page. And so can again use magick to crop the page to just that area and then use optical character recognition to extract the text.\r\nThe cropping required some of trial and error to get the area right. For example, I initially cropped the bottom of the area too close, leading to characters like “p” or “q” being mis-recognized as “o”. But once the area was correct, the OCR worked almost flawlessly.\r\nHere is the full function:\r\n\r\n\r\nread_page <- function(page, pdf_name){\r\n  p <- image_read_pdf(pdf_name, pages = page, density = 600)\r\n   location <- p %>% image_crop(geometry = \"2400x200+60+385\") %>% \r\n   image_ocr()\r\n   path_name = paste0(str_sub(location, end = -2), \".png\")\r\n   image_write(p, path = path_name)\r\n}\r\n\r\n\r\n\r\nAll that remains is then to iterate the function over the pages of the pdf files with map:\r\n\r\n\r\nmap(1:pdf_info(\"data/AdditionalIsochrones.pdf\")$pages, ~ read_page(page = ., pdf_name = \"data/AdditionalIsochrones.pdf\"))\r\n\r\n\r\n\r\nIsochrones with openrouteservice\r\nBefore we can generate isochrones, we need a shapefile of the start locations. I maybe could have used the names extracted in the previous step and run them through a geocoder to automate the process. Instead I put the locations on a map manually in QGIS and exported the locations as a geojson.\r\nNext, we need a function that takes three inputs (location [sf_point object], type of vehicle, time range) and return a location (sf_polygon).\r\n\r\n\r\ncreate_iso <- function(location, profile_1, range_1){\r\n  iso <- ors_isochrones(locations = st_coordinates(location), \r\n                                 profile = ors_profile(mode = profile_1),\r\n                                 range = range_1,\r\n                       output = \"sf\")\r\n  iso$geometry\r\n}\r\n\r\n\r\n\r\nThe OpenRouteService API has a limit of 5 locations per call, and so we create the isochrones one after the other with a mutate call. For this we need to set up the data frame of locations so that it has one row for each location, each vehicle type, and each time range.1\r\n\r\n\r\norigins <- st_read(\"data/network_redesign_travel_time_map_locations.geojson\")\r\n\r\n\r\nReading layer `network_redesign_travel_time_map_locations' from data source `C:\\Users\\user1\\Documents\\network_redesign_isochrones\\data\\network_redesign_travel_time_map_locations.geojson' \r\n  using driver `GeoJSON'\r\nSimple feature collection with 41 features and 2 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -89.52713 ymin: 43.01574 xmax: -89.26672 ymax: 43.14486\r\nGeodetic CRS:  WGS 84\r\n\r\norigins_2 <- origins %>% \r\n  mutate(mode_ebike = \"e-bike\", \r\n         mode_bike = \"bike\", \r\n         time_15 = 900, \r\n         time_30 = 1800, \r\n         time_45 = 2700) %>% \r\n  pivot_longer(starts_with(\"mode\"),\r\n               names_to = \"mode_var\",\r\n               values_to = \"bike_type\") %>% pivot_longer(starts_with(\"time_\"),\r\n                                                         names_to = \"time_var\", \r\n                                                         values_to = \"time\") %>% \r\n  select(-c(mode_var, time_var, id))\r\n\r\nhead(origins_2) %>% \r\n  kableExtra::kable()\r\n\r\n\r\n\r\nname\r\n\r\n\r\ngeometry\r\n\r\n\r\nbike_type\r\n\r\n\r\ntime\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\ne-bike\r\n\r\n\r\n900\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\ne-bike\r\n\r\n\r\n1800\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\ne-bike\r\n\r\n\r\n2700\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\nbike\r\n\r\n\r\n900\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\nbike\r\n\r\n\r\n1800\r\n\r\n\r\nAnderson at Wright\r\n\r\n\r\nPOINT (-89.33044 43.12018)\r\n\r\n\r\nbike\r\n\r\n\r\n2700\r\n\r\n\r\nNow we just run a mutate call with the rowwise function of dplyr and create isochrones for each row 2 :\r\n\r\n\r\nbike_isochrones <- origins_2 %>% \r\n  rowwise() %>% \r\n  mutate(iso = create_iso(geometry, bike_type, time))\r\n\r\n\r\n\r\nWe save the dataframe as an RDS file that we can then use in the app.\r\nThe app\r\nNow we have all the pieces in place for the Shiny app. To get a layout with the maps side-by-side I used the fluidpage way of designing the UI, which was pretty easy.\r\nThe exception was the scaling of the bus map image. In the UI, the map is added with imageOutput, which takes a width and height parameter; on the server side, the image is generate with a renderImage call, which also takes dimension parameters. In theory, the Shiny UI should resize things dynamically based on the screen size, but in practice this was difficult to get right. Through trial-and-error I figured out a scaling that works on most screen sizes, but in some cases the image will still overlap other elements of the app.\r\nOther than that, the code is pretty straightforward. You can see the latest version here.\r\n\r\nAnother way to do this would have been to use pmap, iterating over the three parameters each.↩︎\r\nShoutout to the UW-Madison Data Science Hub team and Martin Gal on StackOverflow to help me figure out the rowwise step.↩︎\r\n",
    "preview": "posts/how-far-can-you-go/img/app_screenshot.jpg",
    "last_modified": "2022-04-30T11:04:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/metro-madison-ridership-has-not-recovered-from-covid/",
    "title": "Metro Madison ridership has not recovered from COVID",
    "description": "A quick time series chart of Metro ridership going back to 2002",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-03-27",
    "categories": [
      "transit",
      "Madison (WI)",
      "time series",
      "chart"
    ],
    "contents": "\r\nA very quick chart: There are large differences between how transit agencies (in the US and in Europe) have managed to recover their pre-COVID ridership. Madison Metro’s general manager has always been very open about the fact that recovery would take several years: Metro is facing staffing shortages, which prevents them from expanding service. Funding right now is not a problem, but once pandemic recovery funds run out it will be. Finally, UW-Madison as well local and state government agency have been large drivers of ridership – it remains to be seen if a loss of peak commutes can be substituted with other transit trips.\r\nBut first we have to ask: What do the ridership numbers look like? The National Transit Database provides monthly ridership time series data all the way back to 2002. Here is a time series plot of that data:\r\n\r\n\r\nShow code\r\n\r\nlibrary(readxl)\r\nbus <- read_excel(\"data/January 2022 Ajusted Database.xlsx\", sheet = 3)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nbus_long <- bus %>% \r\n  pivot_longer(cols = 10:250, names_to = \"month\", values_to = \"value\") %>% \r\n  mutate(date = my(month)) %>% \r\n  filter(Agency == \"City of Madison\" & Modes == \"MB\")\r\n\r\nbus_long %>%\r\n  ggplot(aes(date, value/1000)) +\r\n  geom_line() +\r\n  labs(title = \"Monthly unlinked passenger trips for Madison Metro\",\r\n       subtitle = \"In January 2022, ridership still was only 23% of that in January 2020\",\r\n       caption = \"Data: National Transit Database Monthly Module Adjusted Data Release\\nVisualization: Harald Kliems\") +\r\n  ylab(\"Trips (1000s)\") +\r\n   scale_x_date(\r\n    NULL,\r\n    breaks = scales::breaks_width(\"2 years\"), \r\n    labels = scales::label_date(\"'%y\")\r\n  ) +\r\n  scale_y_continuous(\r\n     labels = scales::label_comma(),\r\n     limits = c(0, 1600)) +\r\n  geom_line(data = bus_long %>% filter(date == \"2020-01-01\" | date == \"2022-01-01\"), color = \"red\", size = 1.5, linetype = 2,\r\n            alpha = 0.4) +\r\n  geom_point(data = bus_long %>% filter(date == \"2020-01-01\" | date == \"2022-01-01\"), color = \"red\", size = 2.5) +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nWe can see: While ridership is well above its worst pandemic lows, there still is a long way to go until we get back to pre-pandemic numbers. In January 2022, the latest month available, ridership was less than a quarter than what it was in January 2020.\r\n\r\n\r\n\r\n",
    "preview": "posts/metro-madison-ridership-has-not-recovered-from-covid/quick-chart-metro-ridership_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-03-27T13:18:53-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/boardings-per-bus-per-stop-a-useful-metric/",
    "title": "Boardings per bus per stop: A useful metric?",
    "description": "A follow-up on a post on Metro bus boardings per stop in Madison (WI)",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-03-06",
    "categories": [
      "transit",
      "Madison (WI)",
      "map"
    ],
    "contents": "\r\n\r\nContents\r\nRevision note 2022-03-07\r\nParsing pre-pandemic frequencies\r\nCombining frequency and boardings\r\nHow to interpret these numbers?\r\n\r\nRevision note 2022-03-07\r\nAfter the initial version of this post went up, Jonathan Mertzig pointed out that the number of departures per stop appeared to be wrong for at least one stop, Northern Lights & Epic Staff C (SB). In the post, the stop was listed with 30 departures, which is exactly twice as many as it actually has. I have identified what caused the issue and updated the post accordingly. If you notice any other errors, please let me know.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidytransit)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nlibrary(kableExtra)\r\n\r\n\r\n\r\nSomeone opened a Github issue on my blog post about creating a map of bus boardings:\r\n\r\nI know its kind of old but I was (re)visiting it in looking at the newest version of the redesign plan. It is difficult to compare ridership across stops when I know that some stops have more buses serving them than others. It would be cool to have ridership per bus or a similar measure. But I guess what we’d want is pre-pandemic frequencies. Is there even a source for old schedules were we to want to do it by hand?\r\n\r\nI responded:\r\n\r\nTo get the number of buses at each stop, you’d need 1) a pre-pandemic GTFS schedule file and 2) code to parse that data in the way you want. The former appears to be available via OpenMobilityData. For the latter, I’m not super proficient with GTFS data, but number of buses at a given stop I think is not that hard. \\[...\\] I’ll see if I find some time to play around with the old GTFS file.\r\n\r\nFortunately, there is an R package for working with GTFS data, tidytransit (Poletti et al. 2022), and the package includes helpful vignettes. So I tried answering the question.\r\nParsing pre-pandemic frequencies\r\nThe ridership data provided in my previous post uses a two-week sample from February 2020, right before the COVID-19 pandemic led to massive service reductions and ridership decline. To match the boarding with bus frequencies at that time, we download a GTFS feed from January 30, 2020 from the OpenMobilityData portal.\r\n\r\n\r\nShow code\r\n\r\ngtfs <- read_gtfs(\"data/gtfs_metro_madison_2020-01-30.zip\")\r\n\r\n\r\n\r\nWe start with a quick plot of the different service patterns in the first two months on 2020:\r\n\r\n\r\nShow code\r\n\r\nlibrary(lubridate)\r\njan_feb_2020 <- interval(\"2020-01-01\", \"2020-02-28\")\r\n\r\ngtfs$.$dates_services %>% \r\n  filter(date %within% jan_feb_2020) %>% \r\n  ggplot() + \r\n  theme_bw() + \r\n  geom_point(aes(x = date, y = service_id, color = wday(date, label = T)), size = 2)\r\n\r\n\r\n\r\n\r\nWe see that there are some services that run on some or all weekdays, some Saturday and Sunday and holiday services. We can also see that some services only run when the UW is in session/on break. Compared to the service pattern examples from the tidytransit vignettes, this is quite straightforward.\r\nThere is one exception to this straightforwardness: I realized that the final stop on a route gets counted as an additional departure. We can fix this by filtering out any stop time with a drop_off_type of 0 (i.e. passenger can only get off the bus).\r\n\r\n\r\nShow code\r\n\r\ngtfs$stop_times <- gtfs$stop_times %>% \r\n  filter(drop_off_type == 0)\r\n\r\n\r\n\r\nRather than worrying about different service patterns, we therefore simply pick a typical weekday in February: February 17, a Wednesday, and get the number of departures frequency for each stop:\r\n\r\n\r\nShow code\r\n\r\nstop_freq <- gtfs %>% \r\n  filter_feed_by_date(\"2020-02-17\") %>% \r\n  get_stop_frequency(start_time = 0*3600, end_time = 24*3600, \r\n                              by_route = F)\r\n\r\n\r\n\r\nstop_freq %>%\r\n  head(10) %>% \r\n  kbl() %>% \r\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\r\n\r\n\r\n\r\nstop_id\r\n\r\n\r\nservice_id\r\n\r\n\r\nn_departures\r\n\r\n\r\nmean_headway\r\n\r\n\r\n100\r\n\r\n\r\n88_WKD\r\n\r\n\r\n394\r\n\r\n\r\n219\r\n\r\n\r\n104\r\n\r\n\r\n88_WKD\r\n\r\n\r\n44\r\n\r\n\r\n1964\r\n\r\n\r\n107\r\n\r\n\r\n88_WKD\r\n\r\n\r\n65\r\n\r\n\r\n1329\r\n\r\n\r\n110\r\n\r\n\r\n88_WKD\r\n\r\n\r\n7\r\n\r\n\r\n12343\r\n\r\n\r\n1100\r\n\r\n\r\n88_WKD\r\n\r\n\r\n354\r\n\r\n\r\n244\r\n\r\n\r\n1101\r\n\r\n\r\n88_WKD\r\n\r\n\r\n434\r\n\r\n\r\n199\r\n\r\n\r\n1105\r\n\r\n\r\n88_WKD\r\n\r\n\r\n171\r\n\r\n\r\n505\r\n\r\n\r\n1107\r\n\r\n\r\n88_WKD\r\n\r\n\r\n81\r\n\r\n\r\n1067\r\n\r\n\r\n1112\r\n\r\n\r\n88_WKD\r\n\r\n\r\n134\r\n\r\n\r\n645\r\n\r\n\r\n1115\r\n\r\n\r\n88_WKD\r\n\r\n\r\n31\r\n\r\n\r\n2787\r\n\r\n\r\nNow we can join the departure data with the locations of the bus stops and draw a map:\r\n\r\n\r\nShow code\r\n\r\nlibrary(tmap)\r\ntmap_mode(\"view\")\r\n\r\ngtfs_shp <- gtfs %>% \r\n  gtfs_as_sf() \r\n\r\n\r\nstop_freq <- gtfs_shp$stops %>% \r\n  left_join(stop_freq, by = \"stop_id\") %>% \r\n  filter(n_departures >1) \r\n\r\nstop_freq %>% \r\n  tm_shape() +\r\n  tm_dots(size = \"n_departures\", col = \"n_departures\", alpha = .7, popup.vars = c( \"# daily departures\" = \"n_departures\"), id = \"stop_name\")\r\n\r\n\r\n\r\n\r\nWe can see the four transfer points; a cluster of stations with frequent service along the University Ave/Johnson St couplet, State St, and on the Capitol Square; and one busy quasi-transfer point at East Towne Mall.\r\nCombining frequency and boardings\r\nNow all that remains is to join our frequency data with the boardings, and then divide the boarding count by the number of departures.\r\n\r\n\r\nShow code\r\n\r\nlibrary(sf)\r\n\r\nboardings <- st_read(\"data/Metro_Transit_Ridership_by_Stop.shp\")\r\n\r\n\r\nReading layer `Metro_Transit_Ridership_by_Stop' from data source \r\n  `C:\\Users\\user1\\Documents\\bus_stop_frequencies\\data\\Metro_Transit_Ridership_by_Stop.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 2142 features and 15 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -89.56378 ymin: 42.98765 xmax: -89.2449 ymax: 43.17652\r\nGeodetic CRS:  WGS 84\r\n\r\nJust as a reminder, this is what the raw boarding numbers look like on a map:\r\n\r\n\r\nShow code\r\n\r\nboardings %>% \r\n  tm_shape() +\r\n  tm_dots(col = \"Weekday\", size = \"Weekday\", alpha = .7)\r\n\r\n\r\n\r\n\r\nNext, we join and create a map.\r\n\r\n\r\nShow code\r\n\r\nboardings <- boardings %>% \r\n  mutate(stop_id = as.character(StopID)) %>% \r\n  st_drop_geometry()\r\n\r\nstop_freq %>% \r\n  left_join(boardings, by = \"stop_id\") %>% \r\n  mutate(boardings_per_bus = Weekday/n_departures) %>% \r\n  tm_shape() +\r\n  tm_dots(col = \"boardings_per_bus\", \r\n          size = \"boardings_per_bus\", \r\n          alpha = .7, \r\n          popup.vars = c(\"Boardings per departure\" = \"boardings_per_bus\", \"Weekday departures\" = \"n_departures\", \"Weekday boardings\" = \"Weekday\"), \r\n          id = \"stop_name\",\r\n          title = \"Boardings per departure\")\r\n\r\n\r\n\r\n\r\nThis map looks quite different: The transfer points lose some prominence, whereas the stops on the UW Campus really stand out. And out in Verona there is one very prominent stop. Let’s look at the top 15 stops in a table.\r\n\r\n\r\nShow code\r\n\r\nstop_freq %>% \r\n  left_join(boardings, by = \"stop_id\") %>% \r\n  mutate(boardings_per_bus = Weekday/n_departures) %>%\r\n  st_drop_geometry() %>% \r\n  select(stop_name, boardings_per_bus, n_departures) %>% \r\n  arrange(desc(boardings_per_bus)) %>% \r\n  head(15) %>% \r\n  kableExtra::kbl(digits = 1, col.names = c(\"Stop name\", \"Boardings per bus\", \"Daily departures\")) %>% \r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\r\n\r\n\r\n\r\nStop name\r\n\r\n\r\nBoardings per bus\r\n\r\n\r\nDaily departures\r\n\r\n\r\nObservatory & Elm (EB)\r\n\r\n\r\n20.9\r\n\r\n\r\n46\r\n\r\n\r\nObservatory & Natatorium (EB)\r\n\r\n\r\n17.8\r\n\r\n\r\n46\r\n\r\n\r\nNorthern Lights & Epic Staff C (SB)\r\n\r\n\r\n16.2\r\n\r\n\r\n15\r\n\r\n\r\nLinden & N Charter (WB)\r\n\r\n\r\n12.2\r\n\r\n\r\n46\r\n\r\n\r\nLinden & N Charter (EB)\r\n\r\n\r\n12.1\r\n\r\n\r\n55\r\n\r\n\r\nObservatory & Babcock (EB)\r\n\r\n\r\n9.4\r\n\r\n\r\n46\r\n\r\n\r\nObservatory & Highland (EB)\r\n\r\n\r\n8.0\r\n\r\n\r\n46\r\n\r\n\r\nHighland & Marsh (EB)\r\n\r\n\r\n7.9\r\n\r\n\r\n114\r\n\r\n\r\nN Lake & University (NB)\r\n\r\n\r\n7.1\r\n\r\n\r\n83\r\n\r\n\r\nLinden & Henry (EB)\r\n\r\n\r\n6.6\r\n\r\n\r\n55\r\n\r\n\r\nWright & Madison College (SB)\r\n\r\n\r\n6.2\r\n\r\n\r\n33\r\n\r\n\r\nSouth Transfer Point\r\n\r\n\r\n6.2\r\n\r\n\r\n203\r\n\r\n\r\nNorth Transfer Point\r\n\r\n\r\n5.1\r\n\r\n\r\n242\r\n\r\n\r\nS Broom & W Doty (NB)\r\n\r\n\r\n5.0\r\n\r\n\r\n60\r\n\r\n\r\nCremer & Park And Ride (NB)\r\n\r\n\r\n4.8\r\n\r\n\r\n8\r\n\r\n\r\nThe Observatory & Elm EB stop on average has on average 21 passengers boarding each bus. But the stop serving the Epic campus in Verona, with only 15 daily departures is really high up there as well. There are only a few other non-UW stops in the list: One serves the Madison College campus on the north side, and one is a Park & Ride lot; and two of the four transfer points made the cut.\r\nHow to interpret these numbers?\r\nWhat does it tell us that a high number of people board a bus at a given stop? Does it help make decisions for projects such as the ongoing Network Redesign? I’m curious what the person who initially asked the question thinks about this.\r\nI can see two possible insights from the metric: A large number of people boarding at a stop leads to longer dwell times. (Fricker 2011) It takes time for people to step on board, pay or validate their fare, and move through the vehicle, and so the bus will be stopped for a longer time. For schedule planning purposes it is therefore useful to have these numbers. Additionally, it may encourage an agency to think about improvements to vehicles serving such a stop or improving the stop itself: Buses with more doors, faster fare payment and validation, wider aisles, stops with level boarding. And finally, a high number of boardings per bus could indicate that a stop is underserved and needs more frequent buses!\r\nAn additional benefit of the metric is that it removes the prominence of the transfer points in the raw numbers. The four transfer points do not have much transit demand by themselves. This is aerial imagery of the North Transfer Point:\r\nAerial imagery of North Transfer Point. The area around the transfer point has parking lots, low-density residential housing, and the now-closed Oscar Meyer plant. Image: Google Maps.The reason for the high raw number of boardings is merely that the current route system is set up in a way that forces riders from outlying areas to transfer at the transfer points to get downtown or across town. That said, even after adjusting for the number of departures, the tranfers points still have a comparatively high number of boardings per bus.\r\n\r\n\r\n\r\nFricker, Jon D. 2011. “Transportation Research Board 90th Annual MeetingTransportation Research Board.” In. Washington, DC. https://trid.trb.org/view/1091291.\r\n\r\n\r\nPoletti, Flavio, Daniel Herszenhut, Mark Padgham, Tom Buckley, Danton Noriega-Goodwin, Angela Li, Elaine McVey, et al. 2022. Tidytransit: Read, Validate, Analyze, and Map GTFS Feeds. https://CRAN.R-project.org/package=tidytransit.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/boardings-per-bus-per-stop-a-useful-metric/img/boardings_per_departure_preview_map.png",
    "last_modified": "2022-03-27T13:39:56-05:00",
    "input_file": {},
    "preview_width": 572,
    "preview_height": 389
  },
  {
    "path": "posts/a-vision-zero-twitter-bot-for-madison/",
    "title": "A Vision Zero Twitter Bot for Madison",
    "description": "Using R, the rtweet package, and GitHub Actions to automate weekly tweets about traffic safety",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-02-18",
    "categories": [
      "Vision Zero",
      "Madison (WI)",
      "Twitter"
    ],
    "contents": "\r\n\r\nContents\r\nCrash data\r\nSetting up dates\r\nComposing the tweet\r\nAutomation with Github Actions\r\n\r\nI’m a big fan of Twitter bots. Accounts like EveryLot. And of course I myself run the content end of the Cyclists_of_MSN bot. I had ideas for other Twitter bots but lacked the technical skills to make them happen. So when I recently saw an article documenting how to create a Twitter bot by combining an R script with GitHub Actions, I was intrigued and got coding. The goal: A Twitter account automatically posting content related to Vision Zero in Madison, a policy and action plan that strives to eliminate all traffic fatalities and injuries in our city by 2030.\r\n\r\nI want to document that process here, focusing on things not described in other tutorials and helping others to set up their own Vision Zero bots. In general, I recommend following Matt Dray’s guide. Note his most recent update about needing to apply for elevated access for your Twitter development account!\r\nCrash data\r\nData about traffic crashes in Wisconsin is reported by police and collected and processed by the UW-Madison Traffic Operations and Safety Lab. A user-friendly interactive way to access the data is through the Community Maps portal. The web interface has no obvious option to download the underlying data, but I recently realized that there is indeed an API available, documented here1. First, we download a json file with the crash data for the whole county:\r\n\r\n\r\ndownload.file(\"https://CommunityMaps.wi.gov/crash/public/crashesKML.do?filetype=json&startyear=2022&injsvr=K&injsvr=A&county=dane\", \"crashes.json\")\r\n\r\n\r\n\r\nThe parameters for the query are mostly obvious: startyear, filetype, and county. Less obvious is the injsvr parameters: The severity of crashes is classified into five classes:\r\nK: Fatality\r\nA: Suspected Serious Injury\r\nB: Suspected Minor Injury\r\nO: No apparent Injury\r\nSo our query retrieves: a GeoJSON file of crashes flagged as fatal or resulting in serious injury2 that occurred in Dane County in 2022.\r\nWe then read in the file with the sf package:\r\n\r\n\r\ndf <- st_read(\"crashes.json\")\r\nglimpse(df)\r\n\r\n\r\n\r\nWe do a little bit of data cleaning and also drop the geometry data – for now, we’re not using the geocoded crash locations for anything. Now is also a good time to filter the data to crashes that occurred in the City of Madison.\r\n\r\n\r\ncrashes <- df %>%\r\n  mutate(date = mdy(date),\r\n         totfatl = as.numeric(totfatl),\r\n         totinj = as.numeric(totinj)) %>%\r\n  st_drop_geometry() %>% \r\n  filter(muniname == \"MADISON\")\r\n\r\n\r\n\r\nEach row in the data represents one crash – keep that in mind when producing summary data: One crash can lead to multiple injuries or fatalities.\r\nOne thing that still needs fixing is the flags variable. This is where each crash is coded for factors like driver impairment, whether speeding was involved, or if the crash involved a pedestrian or cyclists—or a deer. The problem is that the data is not well formatted. I’m not sure if this is a problem with sf reading the JSON file or something wrong with the JSON file itself. To fix this, we re-read the JSON file with the jsonlite package and append the respective columns to our file.\r\n\r\n\r\ncrashesJSON <- fromJSON(\"crashes.json\")\r\ncrashes <- crashes %>%\r\n  add_column(crashesJSON$features$properties)\r\n\r\n\r\n\r\nSetting up dates\r\nOne challenge I encountered was around setting up time intervals. I don’t know how frequently the data in Community Maps are being updated, but from comparing the data with news reports of crashes, it seems like generally data make it into the portal fairly quickly. Based on that and the overall frequency of deadly and serious crashes, I decided to calculate summary statistics weekly for the preceding week. That makes for a good compromise between timeliness and accuracy.\r\nImplementing this took me some thinking and testing with the lubridate package. First, we set a variable for today:\r\n\r\n\r\nd <- today()\r\n\r\n\r\n\r\nNext, we need to define an interval for last week:\r\n\r\n\r\nlast_week <- interval(start = floor_date(d, unit = \"week\")-8, end = floor_date(d, unit = \"week\")-1)\r\n\r\n\r\n\r\nThe way to do that is by using the rounding functions from lubridate. We round down from today’s date to the nearest week. And then we subtract 8 days to get the beginning of the previous week, and 1 day to get the end of the previous week. It’s possible there is a more elegant way to do this, but this works.\r\nFinally, we create a nicely formatted string for the interval:\r\n\r\n\r\nlast_week_formatted <- paste0(format(last_week@start, \"%d/%m\"),\r\n                         \"-\",\r\n                         format(floor_date(d, unit = \"week\")-1, \"%d/%m\"))\r\n\r\n\r\n\r\nSo this will produce something like “05/02-12/02”, which we’ll use for the content of the tweet.\r\nNext, we set up a number of summary variables that will be used for composing the tweet: The weekly and annual number of crashes and of fatalities/serious injuries resulting from those crashes. Lubridate’s %within% operator makes for easy temporal filtering.\r\n\r\n\r\ncrashes_wk <- crashes %>%\r\n  filter(date %within% last_week)\r\n\r\n#weekly numbers\r\ntot_crashes_wk <- crashes_wk %>% nrow()\r\ntot_fat_wk <- crashes_wk %>%\r\n  summarise(sum(totfatl)) %>%\r\n  pull()\r\ntot_inj_wk <- crashes_wk %>%\r\n  summarise(sum(totinj)) %>%\r\n  pull()\r\n\r\n# annual numbers\r\ntot_crashes_yr <- crashes %>% nrow()\r\ntot_fat_yr <- crashes %>%\r\n  summarise(sum(totfatl)) %>%\r\n  pull()\r\ntot_inj_yr <- crashes %>%\r\n  summarise(sum(totinj)) %>%\r\n  pull()\r\n\r\n\r\n\r\nComposing the tweet\r\nThere are two pieces to the tweet. First we create the text of the tweet, using the variables we just created. Keep in mind Twitter’s character limit when composing the text.\r\n\r\n\r\ntweet_1 <- paste0(\"Last week in Madison (\",\r\n                  last_week_formatted,\r\n                  \"), there were \",\r\n                tot_fat_wk,\r\n                \" traffic fatalities and \",\r\n                tot_inj_wk,\r\n                \" serious injury crashes. Since the beginning of the year, traffic violence has killed \",\r\n                tot_fat_yr,\r\n                \" people and seriously injured \",\r\n                tot_inj_yr,\r\n                \" people in our city. #VisionZero #StopTrafficViolence\")\r\n\r\n\r\n\r\nTo make the tweet more visually interesting, we add an automatically generated image to it. This is easy with the magick package. We start with a public domain png image (cropped to Twitter’s recommended aspect ratio and overlaid with a semi-transparent layer and our account’s Twitter handle in GIMP).\r\nA public domain image of the Wisconsin State Capitol\r\n\r\nbackground <- image_read(\"madison_1200.png\")\r\n\r\n\r\n\r\nAnd then we use image_annotate to add the crash stats to it:\r\n\r\n\r\nimage_text <- paste0(\"Vision Zero update \",\r\n                     last_week_formatted,\r\n                     \"\\n Fatalities: \",\r\n                     tot_fat_wk,\r\n                     \"\\n Serious injuries: \",\r\n                     tot_inj_wk,\r\n                     \"\\n Year-to-date fatalities: \",\r\n                     tot_fat_yr,\r\n                     \"\\n Year-to-date serious injuries:\",\r\n                     tot_inj_yr)\r\n\r\ntweet_1_img <- image_annotate(background,\r\n               image_text,\r\n               size = 60,\r\n               font = \"sans\",\r\n               weight = 700, #bold text\r\n               gravity = \"center\",\r\n               color = \"black\")\r\n\r\n\r\n\r\nGetting the text to fit the image in the right place and with the right size requires some experimenting with the parameters. Once finalized, we save the image as a file with image_write.\r\n\r\n\r\nimage_write(tweet_1_img,\r\n            path = \"tweet_1_img.png\")\r\n\r\n\r\n\r\nNow all that remains to be done in R is to post_tweet:\r\n\r\n\r\npost_tweet(status = tweet_1,\r\n           media = \"tweet_1_img.png\")\r\n\r\n\r\n\r\nThe result should look something like this:\r\nScreenshot of a tweetAutomation with Github Actions\r\nFor setting up the automation of the bot, I again followed the instructions for the London Map Bot. Make sure you load all required packages as part of your yaml file.\r\nA couple things that tripped me up in the process:\r\nStoring the API credentials as Github Secrets. The Settings > Secrets > Actions page in our Github repository has two types of secrets, Environment Secrets and Repository Secrets. You need to set up your credentials as Repository Secrets. The second (more embarrassing) issue I encountered: Matching the names of the secrets between Github, your R script, and the Github Actions yaml file. There’s a lot of copying and pasting, and the error messages from Github Actions aren’t the most helpful. And so it took me way too long to realize that the failure of my bot to run was caused by mismatched variable names. One thing that helped with troubleshooting was the ability to manually trigger the Github Action with the workflow_dispatch option.\r\nOnce everything was working, we change the trigger to a cron job: Once a week, on Wednesdays at 17:33 UTC (i.e. 12:33 PM Central), the script will run an post the tweet.\r\n\r\n\r\non:\r\n  schedule:\r\n    - cron: '33 17 * * Wed'\r\n\r\n\r\n\r\nI have some ideas for additional features for the bot, such as a map of crashes or tweets triggered by the number of crashes crossing certain thresholds. But for now the bot is up and running. For anybody wanting to run a bot for their city or county in Wisconsin, the code is very easy to modify. Just head to the bot’s Github repository and fork it! Feel free to reach out if you have questions or suggestions.\r\n\r\nThis link may break in the future. In that case go to https://transportal.cee.wisc.edu/partners/community-maps/crash/pages/help.jsp and click the link to the “Community Maps JSON/KMZ Data Service User Guide.”↩︎\r\nNote that the severity classification is mostly based on the initial police report. It does appear that crashes to get re-classified when a victim passes away at a later point.↩︎\r\n",
    "preview": {},
    "last_modified": "2022-03-27T13:48:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/do-crashes-have-a-history/",
    "title": "Do crashes have a history?",
    "description": "What the locations of past crashes can and cannot tell us about future crashes",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2022-01-27",
    "categories": [
      "Vision Zero",
      "map",
      "Madison (WI)"
    ],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\nlibrary(tmap)\r\nlibrary(lubridate)\r\n\r\n\r\n\r\nThe City of Madison is about to adopt their first Vision Zero action plan. The plan is central to guiding the city’s efforts to eliminate all fatal and serious traffic crashes by 2030. One main component of the plan is the high injury network. These are locations that in the past have seen a disproportionate number of crashes. As the draft action plan states: “50% of fatal and severe crashes occur on just 4% of city streets.” It intuitively makes sense: If we know that there are locations where a lot of crashes happen, let’s fix those locations and prevent crashes in the future.\r\nHowever, a comment about data from Montreal’s recently released 2020 Vision Zero report (Ville de Montréal 2021) made me curious. The comment pointed out that when you look at the locations of fatal and serious crashes that happened in 2020 and compare them with crash locations between 2015 and 2019, 85% of those crashes occurred at a location with no recent crash history. This raises the question to what extent past crashes should steer our mitigation efforts (and what alternative approaches there may be.) But first we need to know if these numbers also hold true for Madison.\r\nMap of crash locations in Montreal, comparing 2015-19 with 2020 (Image: City of Montreal)Crash data are available from Community Maps. As a first step we can look at the question visually. Here’s a map that compares 2020 and 2015-19 crashes:\r\n\r\n\r\nShow code\r\n\r\ndownload.file(\"https://CommunityMaps.wi.gov/crash/public/crashesKML.do?filetype=json&startyear=2015&en\r\ndyear=2021&injsvr=K&injsvr=A&county=dane\", \"test.json\")\r\n\r\ndf <- st_read(\"test.json\")\r\n\r\nmadison_KA <- df %>% \r\n  filter(muniname == \"MADISON\") %>% \r\n  mutate(date = mdy(date),\r\n         totfatl = as.numeric(totfatl))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"view\")\r\nmadison_KA %>% \r\n  mutate(yr = year(date),\r\n         yr2020 = case_when(yr == \"2020\" ~ \"2020\",\r\n                            yr %in% c(\"2015\", \"2016\", \"2017\", \"2018\", \"2019\") ~ \"2015-19\")) %>% \r\n  filter(!is.na(yr2020)) %>% \r\n  tm_shape() +\r\n  tm_dots(col = \"yr2020\", title = \"\", popup.vars = F, id = \"yr\") +\r\n  tm_layout(title = \"Fatal and serious crashes in Madison\")\r\n\r\n\r\n\r\n\r\nClearly there were crashes in 2020 at new locations. Getting to a percentage requires additional steps. First, we create 125 ft buffers1 around all crash locations.\r\n\r\n\r\nShow code\r\n\r\nmadison_KA_buffered <- madison_KA %>% \r\n  mutate(yr = year(date),\r\n         yr2020 = case_when(yr == \"2020\" ~ \"2020\",\r\n                            yr %in% c(\"2015\", \"2016\", \"2017\", \"2018\", \"2019\") ~ \"2015-19\")) %>% \r\n  filter(!is.na(yr2020)) %>% \r\n  st_transform(8193) %>% \r\n  st_buffer(125)\r\n\r\n\r\n\r\nNow we check for overlap between the buffers. If a 2020 crash buffer intersects with any 2015-19 buffer, we would say it occurred “at the same location.” In other words, a 2020 crash can be at most 250 ft away from a 2015-19 crash to be considered to have happened at the same location.\r\n\r\n\r\nShow code\r\n\r\npoly_1519 <- madison_KA_buffered %>% \r\n  filter(yr2020 == \"2015-19\") %>% \r\n  st_combine() %>% \r\n  st_make_valid()\r\n  \r\n\r\n\r\nmadison_KA_buffered %>% \r\n  filter(yr2020 == \"2020\") %>% \r\n  mutate(new = case_when(lengths(st_intersects(.,poly_1519)) > 0 ~ \"same location\",\r\n                         lengths(st_intersects(.,poly_1519)) == 0 ~ \"different location\")) %>% \r\n  group_by(new) %>% \r\n  st_drop_geometry() %>% \r\n  summarize(n = n()) %>% \r\n  ggplot(aes(new, n, fill = new)) +\r\n  hrbrthemes::theme_modern_rc() +\r\n  geom_col(show.legend = F) +\r\n  geom_text(aes(label = n), nudge_y = 4, color = \"white\")+\r\n  scale_fill_brewer() +\r\n  xlab(element_blank())+\r\n  ylab(\"number of crashes\") +\r\n  labs(title = str_wrap(\"Crashes in 2020 often occured in locations with no crash history\", 40),\r\n       caption = str_wrap(\"Data: CommunityMaps crash data for fatal (K) and severe (A) injuries, 2015-20. Analysis and visualization: Harald Kliems\", 65))\r\n\r\n\r\n\r\n\r\nThe result: Out of the 103 deadly and serious crashes that occurred in 2020, 29 (28%) were at a location where there had also been a crash between 2015 and 2019. The remaining 74 crashes (72%) happened at a location with no previous crash history. This is a lower proportion than what was found in Montreal, where 85% of crashes happened at locations without a crash history. It is possible that some or all of the difference between Madison and Montreal is an artifact of the analysis. For example, the Montreal figures mention “intersections” whereas this analysis does not match crashes to intersections. Changing the buffer around crash locations would also change the proportion (a larger buffer would lead to more matched crashes).\r\nMethodological issues aside, what about the conclusion of the Montreal findings? “This observation reinforces the need to act on the entire road system rather than on sites considered ‘accident-prone.’” Pragmatically, few measures act on the entire road system (e.g. a city-wide speed limit reduction) and we need a process for prioritizing scarce resources. Maybe the takeaway should be that past crash history should be only one part of that prioritization process and that we need to be aware of its limitations.\r\n\r\n\r\n\r\nVille de Montréal. 2021. “État de La Sécurité Routière 2020.” Montreal. https://portail-m4s.s3.montreal.ca/pdf/etat_de_la_securite_routiere_2020vdem_0.pdf.\r\n\r\n\r\nThe City uses 250 ft buffers around intersections to distinguish between intersection crashes and road segment crashes. Any crash within that buffer is counted as occurring at that intersection.↩︎\r\n",
    "preview": "posts/do-crashes-have-a-history/do_crashes_have_history_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-27T17:30:24-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/where-the-shelter-cant-go-using-data-to-dissect-city-policy-making/",
    "title": "Where the shelter can't go: Using data to dissect city policy-making",
    "description": "Policymakers proposed criteria for a where a shelter for homeless men in Madison (Wisconsin) could and couldn't go. This post uses public data to visualize these criteria, showing that in the end the resolution would have meant that the shelter can't go anywhere.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-07-24",
    "categories": [
      "housing",
      "Madison (WI)",
      "map"
    ],
    "contents": "\r\n\r\nContents\r\nCity limits\r\nChildcare locations and schools\r\nZoning districts\r\nClose to the Beacon\r\nWhat’s left?\r\n\r\nThis post is about using publicly available data to examine local policy proposals. A local blogger pointed out a resolution to the Madison Common Council that would have established a number of criteria for possible locations for a shelter for men experiencing homeless. The shelter had been debated for many years, and unsurprisingly it was controversial. The criteria specified in the draft resolution were as follows:\r\nThe site should be greater than .5 mile from any schools or daycares;\r\nThe site should not be located in a Census tract identified as a location of concentrated poverty\r\nThe site should include either an existing building or a lot large enough for new construction;\r\nThe site should include space for future expansion;\r\nThe site should be zoned for commercial or mixed use, and not adjacent to single-family homes;\r\nThe site should be within a few blocks of seven-days-a-week bus service, with a preference for close proximity to BRT routes;\r\nThe site location should be within a walking distance of within 3.5 miles from the Beacon on East Washington; and\r\nThe site should be an active real estate listing that is vacant or soon-to-be-vacant.\r\nThat’s a lot of places where the shelter must not be! It’s also a number of criteria that I could easily map with publicly available data. The meeting at which the resolution would be first discussed was on the same day and I only had the morning before work and my lunch break to put something together. Here’s what I submitted, and there was some post-meeting coverage from a local blogger. Because of the time constraint, the product was not the prettiest, and in this post I make some improvements and include the code.\r\nCity limits\r\nWhat I should have done first but didn’t was to start with the Madison city limits. The shelter is a city shelter and therefore must be within the City of Madison. There are multiple enclaves and exclaves, and so starting with the city limits as the most expansive possible search area should be step 1. Then we can use the other criteria to “stamp out” anything that doesn’t fit. City limits are available on the City of Madison Open Data portal.\r\n\r\n\r\nShow code\r\n\r\ncity_limit <- st_read(\"data/City_Limit.shp\") %>% \r\n  st_transform(6610)\r\n\r\n\r\nReading layer `City_Limit' from data source \r\n  `C:\\Users\\user1\\Documents\\shelter_locations\\data\\City_Limit.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 4 features and 3 fields\r\nGeometry type: POLYGON\r\nDimension:     XY\r\nBounding box:  xmin: -89.57165 ymin: 42.99815 xmax: -89.24663 ymax: 43.17202\r\nGeodetic CRS:  WGS 84\r\n\r\nShow code\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons(\"green\")\r\n\r\n\r\n\r\n\r\nChildcare locations and schools\r\nFirst of all, the idea that children need to be kept away from people experiencing homelessness is wrong. Second, most people don’t realize how many schools and childcare locations there are. For childcare providers, the Wisconsin Department of Health and Human Services provides a dataset of “[p]oint locations and attributes of certified childcare facilities licensed by the state of Wisconsin.” Public school data is available from the National Center for Education Statistics. Conveniently, both datasets are provided as shapefiles. I didn’t look for data sources for private schools.\r\nFirst we read in the child_care data. I noticed that there is a “capacity” attribute in the dataset. To avoid debates of whether the resolution sponsors would consider someone who provides care to only a few children a “childcare facility,” I filtered the data to only those providers with a capacity larger than 9. To create the half-mile buffers around the locations, I need to project the data to a different coordinate reference system (CRS). The crsuggest package helps identify an appropriate CRS. Note that the CRS I chose uses meters as its units, and so 0.5 miles needs to be provided as 805. Finally, I keep only the parts of the buffers that are within city limits.\r\n\r\n\r\nShow code\r\n\r\nchild_care <- st_read(\"data/Wisconsin_Licensed_and_Certified_Childcare.shp\")\r\n\r\n\r\nReading layer `Wisconsin_Licensed_and_Certified_Childcare' from data source `C:\\Users\\user1\\Documents\\shelter_locations\\data\\Wisconsin_Licensed_and_Certified_Childcare.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 4669 features and 31 fields\r\nGeometry type: POINT\r\nDimension:     XY\r\nBounding box:  xmin: -10372680 ymin: 5197160 xmax: -9694399 ymax: 5946135\r\nProjected CRS: WGS 84 / Pseudo-Mercator\r\n\r\nShow code\r\n\r\nchild_care_buffers <- child_care %>% \r\n  filter(COUNTY == \"Dane\") %>% \r\n  filter(CAPACITY >9) %>% \r\n  st_transform(6610) %>% \r\n  st_buffer(dist = 805) %>% \r\n  st_intersection(city_limit)\r\n\r\n\r\n\r\nFor the schools, the process was pretty much the same:\r\n\r\n\r\nShow code\r\n\r\nschools <- readRDS(\"data/EDGE_GEOCODE_PUBLICSCH_1920.RDS\") #https://nces.ed.gov/opengis/rest/services/K12_School_Locations/EDGE_GEOCODE_PUBLICSCH_1920/MapServer\r\n\r\nschools_buffers <- schools %>% \r\n  st_transform(6610) %>% \r\n  st_buffer(dist = 805) %>% \r\n  st_intersection(city_limit)\r\n\r\n\r\n\r\nNow we can “cut out” the schools and childcare buffers from the city limit map:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons(\"green\") +\r\n  tm_shape(child_care_buffers) +\r\n  tm_polygons(\"red\", alpha = 1, border.alpha = 0) +\r\n  tm_shape(schools_buffers) +\r\n  tm_polygons(\"red\", alpha = 1, border.alpha = 0)\r\n\r\n\r\n\r\n\r\nIt’s obvious: This criterion alone eliminate huge swaths of the city as possible locations.\r\nZoning districts\r\n\r\nThe site should be zoned for commercial or mixed use, and not adjacent to single-family homes;\r\n\r\nI’ve posted about zoning in Madison previously and so I’m pretty familiar with the data. Zoning data is on the Open Data portal.\r\n\r\n\r\nShow code\r\n\r\nzoning <- st_read(\"data/Zoning_Districts.shp\")\r\n\r\n\r\nReading layer `Zoning_Districts' from data source \r\n  `C:\\Users\\user1\\Documents\\shelter_locations\\data\\Zoning_Districts.shp' \r\n  using driver `ESRI Shapefile'\r\nSimple feature collection with 2343 features and 9 fields\r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: -89.57153 ymin: 42.99815 xmax: -89.24663 ymax: 43.17202\r\nGeodetic CRS:  WGS 84\r\n\r\nWhich zoning districts should be included? Ordinance does have a high-level category called “Mixed use and commercial districts,” which includes\r\nLMX Limited Mixed-Use\r\nNMX Neighborhood Mixed-Use District\r\nTSS Traditional Shopping Street District\r\nMXC Mixed-Use Center District\r\nCC-T Commercial Corridor - Transitional District\r\nCC Commercial Center District\r\nRMX Regional Mixed-Use District\r\nArguably, many would also consider some of the zoning districts listed under the “Downtown and Urban Core” heading as mixed use. As it has “mixed use” in its name, I’ll include UMX Urban Mixed-Use.\r\n\r\n\r\nShow code\r\n\r\nmixed_use <- c(\"NMX\", \"TSS\", \"MXC\", \"CC-T\", \"CC\", \"UMX\", \"RMX\")\r\n\r\nzoning_mixed_use <- zoning %>% \r\n  filter(!(ZONING_COD %in% mixed_use)) %>% \r\n  st_transform(st_crs(city_limit))\r\n\r\n\r\n\r\nI’ll overlay these in red again and change the color of the previously cut out areas to grey.\r\n\r\n\r\nShow code\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons(\"green\") +\r\n  tm_shape(zoning_mixed_use) +\r\n  tm_polygons(\"red\", alpha = 1, border.alpha = 0) +\r\n  tm_shape(child_care_buffers) +\r\n  tm_polygons(alpha = 1, border.alpha = 0) +\r\n  tm_shape(schools_buffers) +\r\n  tm_polygons(alpha = 1, border.alpha = 0)\r\n\r\n\r\n\r\n\r\nAnd there goes much of the rest of the city.\r\nClose to the Beacon\r\nI probably could have stopped here, but as it’s an interesting one, I’ll add one more: The 3.5 mile walkshed around the Beacon, an existing homeless services day center on East Washington Ave. For sending my letter, I used the OpenRouteService (ORS) web interface to generate the walkshed. However, it’s also possible to access ORS through an API with the R package openrouteservice-r, which I will do here. First, we geocode the Beacon’s address:\r\n\r\n\r\nShow code\r\n\r\nlibrary(openrouteservice)\r\nbeacon <- ors_geocode(\"The Beacon, Madison, Wisconsin, USA\", output = \"sf\", size = 1)\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons()+\r\n  tm_shape(beacon) +\r\n  tm_dots(\"red\")\r\n\r\n\r\n\r\n\r\nNow we generate the 3.5 mile walking distance. The ors_isochrone functions requires the location to be a long/lat coordinate pair.\r\n\r\n\r\nShow code\r\n\r\nbeacon_coord <- st_coordinates(beacon)\r\n\r\nbeacon_walkshed <- ors_isochrones(st_coordinates(beacon), \r\n                                  profile = ors_profile(\"walking\"),\r\n                                  units = \"mi\",\r\n                                  range = 3.5,\r\n                                  range_type = \"distance\",\r\n                                  #smoothing = \"15\", \r\n                                  output = \"sf\") %>% \r\n  st_transform(st_crs(city_limit))\r\n\r\n\r\n\r\nQuick visual check:\r\n\r\n\r\nShow code\r\n\r\ntm_shape(city_limit) +\r\n  tm_polygons(alpha = .1) +\r\ntm_shape(beacon_walkshed) +\r\n  tm_polygons(\"red\", alpha = .3)\r\n\r\n\r\n\r\n\r\nAgain, we’ll change the previously cut out parts to grey and show everything outside the walk shed in red.\r\n\r\n\r\nShow code\r\n\r\ntmap_mode(\"plot\")\r\ntm_shape(city_limit) +\r\n  tm_polygons(\"green\") +\r\n  tm_shape(zoning_mixed_use) +\r\n  tm_polygons(\"red\", alpha = 1, border.alpha = 0) +\r\n  tm_shape(child_care_buffers) +\r\n  tm_polygons(alpha = 1, border.alpha = 0) +\r\n  tm_shape(schools_buffers) +\r\n  tm_polygons(alpha = 1, border.alpha = 0) +\r\n  tm_shape(st_difference(city_limit, beacon_walkshed)) +\r\n  tm_polygons(\"red\", border.alpha = 0)\r\n\r\n\r\n\r\n\r\nWhat’s left?\r\nWe could keep going with the remaining criteria, but a) they’re harder to implement and b) it looks like not much green is left in the map above. This is a good moment to change to interactive viewing and see what those green areas are:\r\n\r\n\r\nShow code\r\n\r\nwhats_left <- st_intersection(beacon_walkshed, city_limit)\r\n\r\n#helper function to \"erase\" geometry y from x\r\nst_erase <-  function(x, y) st_difference(x, st_union(y))\r\n\r\nwhats_left <- st_erase(whats_left, child_care_buffers)\r\n\r\nwhats_left <- st_erase(whats_left, schools_buffers)\r\n\r\nwhats_left <- st_erase(whats_left, zoning_mixed_use)\r\n\r\ntmap_options(check.and.fix = TRUE)\r\n\r\ntmap_mode(\"view\")\r\ntm_shape(whats_left) +\r\n  tm_polygons(\"green\", alpha = .5)\r\n\r\n\r\n\r\n\r\nLeft as possible locations are: The two largest areas are on the water, on Lake Wingra and Monona Bay. Other than some thin slivers of land that are clearly too small (and probably artefacts), there are three remaining spots. Two small triangles in the Eken Park neighborhood near the Oscar Mayer area (one currently a rental car parking lot; the other a single-family home zoned Commercial). And finally a cluster of properties at the intersection of Milwaukee St and Fair Oaks Ave, currently occupied by gas station, a mixed used building with a bridal store, and a restaurant.\r\nWithout investigating further, I’m confident that any of those non-water sites would fail one or more of the other criteria. And even if they didn’t: If your criteria set up that in a city the size of Madison your choices are this narrow, you either didn’t understand the implications of this (no shelter) or you did understand them and wanted to hide your policy goals behind seemingly reasonable criteria.\r\nDid you like this post? Consider making a donation to Urban Triage, a Madison-based organization that provides resources to people experiencing or at risk of homelessness. Or support the Madison Tenant Resource Center.\r\n\r\n\r\n\r\n",
    "preview": "posts/where-the-shelter-cant-go-using-data-to-dissect-city-policy-making/where_shelter_cant_go_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-03-27T17:20:35-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/scraping-4s/",
    "title": "Scraping 4S",
    "description": "Web scraping and text analysis of the Society of Social Studies of Science's 2021 annual meeting panels.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-04-10",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nScraping the data\r\nText Mining SSSS panels\r\nUnigrams, bigrams, trigrams\r\nTerm frequency–inverse document frequency\r\n\r\nVisualizations\r\n\r\nI recently went though Julia Silge’s great interactive tidytext tutorial to learn some basics of text analysis. Last weekend, my SO was reading through the panel descriptions of the Society for the Social Studies of Science’s (4S) annual meeting to figure out which panel to submit an abstract to. There were over 200 panels to choose from, and so this would be a good opportunity to practice some of the techniques I learned.\r\nThe tidytext tutorial works with data sets that are already nice and clean. For the 4S panels we need to do some web scraping and data cleaning first. The web scraping with rvest is also a first for me. The first part of this post will cover the scraping and cleaning. If you want to read about the text analysis, skip ahead to the second part.\r\nScraping the data\r\nAll panels are listed on this page: https://www.4sonline.org/meeting/accepted-open-panels/. We could scrape the panel titles and descriptions right there. Note, however, that the description here is only a snippet, not the full panel description. We’ll deal with this later.\r\nStep one is to load the relevant libraries and read in the html page.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(rvest)\r\nlibrary(rmarkdown)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\npanel_url <- \"https://www.4sonline.org/meeting/accepted-open-panels\"\r\npanels <- read_html(panel_url)\r\npanels\r\n\r\n\r\n{html_document}\r\n<html lang=\"en-US\">\r\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; cha ...\r\n[2] <body class=\"page-template-default page page-id-12988 page-chil ...\r\n\r\nThe next step is to extract the relevant information from that HTML document. If you have a basic understanding of how HTML and CSS work, you could do this by opening the source code of the page in your browser and manually identify the relevant elements.\r\nScreenshot source codeHere you see that the panel titles are all level 3 headings (<h3>) of the class entry-title. An easier way to identify the element is by using your browser’s element picker. In Firefox, press F12 and choose the element picker. Then choose the element you want to identify and look at the code in the Inspector window. Here’s a screenshot of what this looks like for the panel description:\r\nScreenshot element pickerSo the panel description is a div of the class post-inner-content. (There is a third method to identify relevant content, which we’ll use later).\r\nWith this information, we’re ready to extract panel titles and descriptions:\r\n\r\n\r\nShow code\r\n\r\npanel_titles <- panels %>% html_nodes(\".entry-title\") %>% html_text()\r\n\r\npanel_desc <- panels %>% html_nodes(\".post-content-inner\") %>% html_text()\r\n\r\npanels_all <- tibble(title = panel_titles, desc = panel_desc)\r\n\r\n\r\n\r\nA quick bit of data cleaning to remove the numbers from the titles and use them as an ID column instead:\r\n\r\n\r\nShow code\r\n\r\npanels_all <- panels_all %>% \r\n  mutate(id = str_extract(title, \"^\\\\d*\"),\r\n         title = str_remove(title, \"^\\\\d+.\\\\s\"))\r\n  \r\npanels_all\r\n\r\n\r\n# A tibble: 210 x 3\r\n   title                          desc                           id   \r\n   <chr>                          <chr>                          <chr>\r\n 1 (Bio)Engineering Nature: Edit~ Since the discovery in 2015 o~ 1    \r\n 2 (Dis)Trust in Public-Sector D~ STS scholarship on technopoli~ 2    \r\n 3 (Im)material streams in the c~ The deepening datafication of~ 3    \r\n 4 (Re)configuring care practice~ Care practices were addressed~ 4    \r\n 5 (Re)materialising Cancer: Bod~ Bioscientific approaches to i~ 5    \r\n 6 A Good Life, In Theory: Think~ In recent years, an increasin~ 6    \r\n 7 Academic Automation, Machine ~ Over the last 70 years, compu~ 7    \r\n 8 Aerotechnologies: Air as Elem~ “Objects, processes, or event~ 8    \r\n 9 Aging with other-than-humans:~ In recent years, Science and ~ 9    \r\n10 AI and Feminist STS            Much excellent work has docum~ 10   \r\n# ... with 200 more rows\r\n\r\nLooks pretty good, doesn’t it? Let’s do a simple analysis: What are the most frequent words in the panel titles (after removing common stop words)?\r\n\r\n\r\nShow code\r\n\r\npanels_all %>% \r\n  unnest_tokens(word, title, token = \"words\") %>% \r\n  anti_join(get_stopwords()) %>% \r\n  count(word, sort = T)\r\n\r\n\r\n# A tibble: 723 x 2\r\n   word             n\r\n   <chr>        <int>\r\n 1 sts             29\r\n 2 relations       24\r\n 3 science         23\r\n 4 technologies    15\r\n 5 technology      15\r\n 6 global          14\r\n 7 good            13\r\n 8 governance      13\r\n 9 knowledge       12\r\n10 research        12\r\n# ... with 713 more rows\r\n\r\nUnsurprisingly, STS has the top spot – it’s the acronym for the whole field: Science and Technology Studies. Before diving deeper into the analysis, let’s get the complete descriptions. This requires following the links from the panel titles and scraping those pages. This is a good opportunity to use a third option of identifying elements in an HTML page: Rvest’s Selector Gadget. By loading the bookmarklet on the overview page we can identify the URL elements for scraping. We extract the URLs like so:\r\n\r\n\r\nShow code\r\n\r\nurls <- panels %>% \r\n  html_nodes(\"#post-12988 a\") %>% \r\n  html_attr(\"href\") %>% \r\n  tibble()\r\n\r\ntail(urls)\r\n\r\n\r\n# A tibble: 6 x 1\r\n  .                                                                   \r\n  <chr>                                                               \r\n1 https://www.4sonline.org/category/op21/race-and-racism-21/          \r\n2 https://www.4sonline.org/category/op21/science-communication-public~\r\n3 https://www.4sonline.org/category/op21/social-justice-social-moveme~\r\n4 https://www.4sonline.org/category/op21/transnational-sts-21/        \r\n5 https://www.4sonline.org/category/op21/other-21/                    \r\n6 #                                                                   \r\n\r\nThe list of URLs is fairly clean, but there are a few irrelevant URLs that slipped through. We’ll filter them out with a regular expression.\r\n\r\n\r\nShow code\r\n\r\nurls <- urls %>% \r\n  rename(url = \".\")\r\n\r\n#filter out irrelevant URLs\r\nurls <- urls %>% \r\n  filter(str_detect(url, \"https://www.4sonline.org/\\\\d\")) %>% \r\n  pull(url)\r\n\r\n\r\n\r\nNow we can get all pages by using map_df across all URLs and a custom function to extract the relevant information. Let’s develop the function using the first URL. Again using the Selector Gadget, it seems like these should be the relevant elements to extract:\r\n\r\n\r\nShow code\r\n\r\npage <- read_html(urls[1])\r\n\r\npage_2 <- page %>% \r\n  html_nodes(\".et_pb_text_inner , .et_pb_post_content_0_tb_body p, .et_pb_module_header\") %>% \r\n  html_text() \r\n\r\n\r\ntibble(\r\n  title = page_2[1],\r\n  organizer = page_2[2],\r\n  posted = page_2[3],\r\n  desc = page_2[4],\r\n  contact = page_2[5],\r\n  keywords = page_2[6]\r\n)\r\n\r\n\r\n# A tibble: 1 x 6\r\n  title       organizer    posted   desc       contact   keywords     \r\n  <chr>       <chr>        <chr>    <chr>      <chr>     <chr>        \r\n1 1. (Bio)En~ Elliott Rei~ \"Posted~ \"Since th~ Contact:~ Keywords: ge~\r\n\r\nNow we can just run the function over the length of the URL vector with map_df. Don’t run this yet, though.\r\n\r\n\r\nShow code\r\n\r\npages_full <- map_df(1:length(urls), function(i) {\r\n  page <- read_html(urls[i])\r\n\r\npage_2 <- page %>% \r\n  html_nodes(\".et_pb_text_inner , .et_pb_post_content_0_tb_body p, .et_pb_module_header\") %>% \r\n  html_text() \r\n\r\ntibble(\r\n  title = page_2[1],\r\n  organizer = page_2[2],\r\n  posted = page_2[3],\r\n  desc = page_2[4],\r\n  contact = page_2[5],\r\n  keywords = page_2[6]\r\n)\r\n  \r\n}\r\n  )\r\n\r\n\r\n\r\nIf you ran the code and took a look at the resulting data frame, you’d see that the code breaks for panels that have multiple <p> paragraphs in the description. There may be a more elegant fix, but for now we change the code for the html_elements to be less specific and not split out individual paragraphs. The downside is that the contact info and keywords get lumped in with the description. We can fix that later.\r\n\r\n\r\nShow code\r\n\r\npages_full <- map_df(1:length(urls), function(i) {\r\n  page <- read_html(urls[i])\r\n\r\npage_2 <- page %>% \r\n  html_nodes(\".et_pb_post_content_0_tb_body , .et_pb_text_inner, .et_pb_module_header\") %>% \r\n  html_text() \r\n\r\ntibble(\r\n  title = page_2[1],\r\n  organizer = page_2[2],\r\n  posted = page_2[3],\r\n  desc = page_2[4]\r\n)\r\n\r\n  \r\n}\r\n  )\r\n\r\n\r\n\r\nAs scraping 210 pages takes a long time, we’ll save the results as an rds file.\r\n\r\n\r\nShow code\r\n\r\nwrite_rds(pages_full, paste0(\"data/pages_full_\", Sys.Date(), \".rds\"))\r\n\r\n\r\n\r\nNext we’ll clean the panel data, including fixing the lumped together panel description, contact info, and keywords. This requires some regular expression magic.\r\n\r\n\r\nShow code\r\n\r\npages_full <- read_rds(\"data/pages_full_2021-03-07.rds\")\r\n\r\npanels_full_wide <- pages_full %>% \r\n  separate(organizer, c(\"organizer_1\", \"organizer_2\", \"organizer_3\", \"organizer_4\", \"organizer_5\", \"organizer_6\", \"organizer_7\"), \"; \") %>% \r\n  mutate(id = str_extract(title, \"^\\\\d*\"),\r\n         title = str_remove(title, \"^\\\\d*.\\\\s\"),\r\n         keywords = str_extract(desc, \"(?<=(Keywords\\\\:\\\\s))(.*)\"),\r\n         desc = str_extract(desc, \"(.|\\\\n)*(?=\\\\n\\\\nContact)\")) %>% #return everything before contact\r\n  separate(keywords, c(\"keyword_1\", \"keyword_2\", \"keyword_3\", \"keyword_4\", \"keyword_5\", \"keyword_6\", \"keyword_7\", \"keyword_8\"), \",|;\")\r\n\r\npanels_full_wide\r\n\r\n\r\n# A tibble: 210 x 19\r\n   title  organizer_1  organizer_2 organizer_3 organizer_4 organizer_5\r\n   <chr>  <chr>        <chr>       <chr>       <chr>       <chr>      \r\n 1 (Bio)~ \"Elliott Re~ <NA>        <NA>        <NA>        <NA>       \r\n 2 (Dis)~ \"danah boyd~ Janet Vert~ Alondra Ne~ <NA>        <NA>       \r\n 3 (Im)m~ \"Morgan Mou~ Ryan Burns~ <NA>        <NA>        <NA>       \r\n 4 (Re)c~ \"Stefan Nic~ Cristina P~ <NA>        <NA>        <NA>       \r\n 5 (Re)m~ \"Emily Ross~ Julia Swal~ <NA>        <NA>        <NA>       \r\n 6 A Goo~ \"Claire Oli~ <NA>        <NA>        <NA>        <NA>       \r\n 7 Acade~ \"Jeremy Hun~ <NA>        <NA>        <NA>        <NA>       \r\n 8 Aerot~ \"Boyd Ruamc~ Jia Hui Le~ <NA>        <NA>        <NA>       \r\n 9 Aging~ \"Daniel Lop~ Nete Schwe~ <NA>        <NA>        <NA>       \r\n10 AI an~ \"Rachel Ber~ <NA>        <NA>        <NA>        <NA>       \r\n# ... with 200 more rows, and 13 more variables: organizer_6 <chr>,\r\n#   organizer_7 <chr>, posted <chr>, desc <chr>, id <chr>,\r\n#   keyword_1 <chr>, keyword_2 <chr>, keyword_3 <chr>,\r\n#   keyword_4 <chr>, keyword_5 <chr>, keyword_6 <chr>,\r\n#   keyword_7 <chr>, keyword_8 <chr>\r\n\r\nIn addition to the what is contained in the individual panel pages, there are also topic areas/themes for panels:\r\nTopic areasGetting theme urls and labels is easy:\r\n\r\n\r\nShow code\r\n\r\ntheme_urls <- panels %>% \r\n  html_elements(\"#menu-op21 a\") %>%\r\n  html_attr(\"href\")\r\n\r\ntheme_labels <- panels %>% \r\n  html_elements(\"#menu-op21 a\") %>%\r\n  html_text()\r\n\r\n\r\n\r\nWe’ll download all theme pages to then extract the panel IDs.\r\n\r\n\r\nShow code\r\n\r\nthemes_full <- map_df(1:length(theme_urls), function(i) {\r\n  page <- read_html(theme_urls[i])\r\ntheme <- theme_labels[[i]]\r\n\r\npage_2 <- page %>% \r\n  html_nodes(\".entry-title\") %>% \r\n  html_text()\r\n\r\ntibble(title = page_2, theme) %>% \r\n  mutate(id = str_extract(title, \"^\\\\d*\"),\r\n         title = str_remove(title, \"^\\\\d*.\\\\s\"))\r\n}\r\n)\r\n\r\nglimpse(themes_full)\r\n\r\n\r\nRows: 582\r\nColumns: 3\r\n$ title <chr> \"The Prediction Factor: Medical Decision in the Age of~\r\n$ theme <chr> \"Big Data, AI and Machine Learning\", \"Big Data, AI and~\r\n$ id    <chr> \"187\", \"197\", \"200\", \"177\", \"180\", \"143\", \"144\", \"152\"~\r\n\r\nWe can see that there are 582 observations for 210 panels. So a panel can be listed in more than one theme. Different types of analysis require different data formats, and so we’ll create two data frames: For the first one, we want to keep one row per panel. This requires a sequence of pivot_wider, unite, and separate on the themes data before doing a join with the panels_full_wide data frame.\r\n\r\n\r\nShow code\r\n\r\nthemes_full_wide <- themes_full %>% \r\n  pivot_wider(id_cols = c(title, id), names_from = \"theme\", values_from = theme) %>% \r\n  unite(\"theme\", 3:26, sep = \";\", remove = T, na.rm = T) %>% \r\n  separate(theme, into = c(\"theme_1\", \"theme_2\", \"theme_3\"), sep = \";\")\r\n\r\npanels_full_wide <- panels_full_wide %>% \r\n  left_join(themes_full_wide, by = c(\"id\", \"title\"))\r\n\r\n\r\n\r\nWith the multiple keyword_n, organizer_n, and theme_n columns, the data does not lend itself to an analysis by these variables. For that, we need to pivot the data to a longer format.\r\n\r\n\r\nShow code\r\n\r\npanels_full_long <- panels_full_wide %>% \r\n  pivot_longer(starts_with(\"organizer_\"), names_prefix = \"organizer_\", names_to = \"organizer_order\", values_to = \"Organizer_name\", values_drop_na = T) %>% \r\n  pivot_longer(starts_with(\"keyword_\"), names_prefix = \"keyword_\", names_to = \"keyword_order\", values_to = \"keyword\", values_drop_na = T) %>% \r\n  pivot_longer(starts_with(\"theme_\"), names_prefix = \"theme_\", names_to = \"theme_order\", values_to = \"theme\", values_drop_na = T)\r\n\r\nglimpse(panels_full_long)\r\n\r\n\r\nRows: 6,305\r\nColumns: 10\r\n$ title           <chr> \"(Bio)Engineering Nature: Editing environmen~\r\n$ posted          <chr> \"Posted: January 27, 2021\\n\", \"Posted: Janua~\r\n$ desc            <chr> \"\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\tSince the dis~\r\n$ id              <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",~\r\n$ organizer_order <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",~\r\n$ Organizer_name  <chr> \"Elliott Reichardt, Stanford University\", \"E~\r\n$ keyword_order   <chr> \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"3\", \"3\", \"3\",~\r\n$ keyword         <chr> \"gene drives\", \"gene drives\", \"gene drives\",~\r\n$ theme_order     <chr> \"1\", \"2\", \"3\", \"1\", \"2\", \"3\", \"1\", \"2\", \"3\",~\r\n$ theme           <chr> \"Environmental/Multispecies Studies\", \"Genet~\r\n\r\nThat’s it for data prep! For your convenience, here are the two data frames:\r\nOne row per panel (wide format)\r\nrds\r\ncsv\r\n\r\nmultiple rows per panel (long format)\r\nrds\r\ncsv\r\n\r\nText Mining SSSS panels\r\nLet’s do some basic text mining on the cleaned panel data. What are the words most commonly used in the panel descriptions, with stop words like “the,” “you,” “a,” … removed?\r\nUnigrams, bigrams, trigrams\r\n\r\n\r\nShow code\r\n\r\npanels_full_wide %>% \r\n  unnest_tokens(word, desc, token = \"words\") %>% \r\n  anti_join(get_stopwords()) %>% \r\n  count(word, sort = TRUE)\r\n\r\n\r\n# A tibble: 7,420 x 2\r\n   word          n\r\n   <chr>     <int>\r\n 1 sts         280\r\n 2 panel       267\r\n 3 social      244\r\n 4 science     233\r\n 5 practices   228\r\n 6 data        188\r\n 7 research    186\r\n 8 relations   185\r\n 9 can         176\r\n10 new         165\r\n# ... with 7,410 more rows\r\n\r\nNot terribly exciting, is it? Let’s do the same but for bigrams.\r\n\r\n\r\nShow code\r\n\r\npanels_full_wide %>% \r\n  unnest_tokens(word, desc, token = \"ngrams\", n = 2) %>% \r\n  count(word, sort = TRUE)\r\n\r\n\r\n# A tibble: 35,269 x 2\r\n   word            n\r\n   <chr>       <int>\r\n 1 of the        218\r\n 2 in the        213\r\n 3 this panel    168\r\n 4 and the       145\r\n 5 to the        117\r\n 6 on the         99\r\n 7 as a           87\r\n 8 such as        79\r\n 9 papers that    67\r\n10 with the       62\r\n# ... with 35,259 more rows\r\n\r\n\r\n\r\nShow code\r\n\r\nbigram <- panels_full_wide %>% \r\n  unnest_tokens(bigram, desc, token = \"ngrams\", n = 2)\r\n\r\nbigrams_separated <- bigram %>% separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\r\n\r\nbigrams_filtered <- bigrams_separated %>% \r\n    filter(!word1 %in% stop_words$word) %>%\r\n  filter(!word2 %in% stop_words$word)\r\n\r\n\r\n\r\nbigrams_filtered %>% count(word1, word2, sort = TRUE) %>% \r\n  head(500) %>% \r\n  paged_table()\r\n\r\n\r\n\r\n\r\n\r\nMuch better? If two is better than one, is three better than two? Let’s try trigrams, without stop word filtering.\r\n\r\n\r\nShow code\r\n\r\npanels_full_wide %>% \r\n  unnest_tokens(word, desc, token = \"ngrams\", n = 3) %>% \r\n  count(word, sort = TRUE)\r\n\r\n\r\n# A tibble: 49,575 x 2\r\n   word                       n\r\n   <chr>                  <int>\r\n 1 as well as                40\r\n 2 what are the              31\r\n 3 in this panel             30\r\n 4 science and technology    30\r\n 5 this panel we             29\r\n 6 this open panel           27\r\n 7 in order to               23\r\n 8 we invite papers          23\r\n 9 not limited to            22\r\n10 the covid 19              22\r\n# ... with 49,565 more rows\r\n\r\nThis does still have a lot of not-so-meaningful rows, and we’ll see what happens if we filter for stop words:\r\n\r\n\r\nShow code\r\n\r\ntrigram <- panels_full_wide %>% \r\n  unnest_tokens(trigram, desc, token = \"ngrams\", n = 3)\r\n\r\ntrigrams_separated <- trigram %>% separate(trigram, c(\"word1\", \"word2\", \"word3\"), sep = \" \")\r\n\r\ntrigrams_filtered <- trigrams_separated %>% \r\n    filter(!word1 %in% stop_words$word) %>%\r\n  filter(!word2 %in% stop_words$word) %>% \r\n  filter(!word3 %in% stop_words$word)\r\n\r\ntrigrams_filtered %>% \r\n  count(word1, word2, word3, sort = T) %>% \r\n  filter(n<1) %>% \r\n  paged_table()\r\n\r\n\r\n\r\n\r\n\r\nFor trigrams, filtering for stop words is tricky. The filtered analysis has fewer irrelevant trigrams such as “in order to” or “as well as,” but it also filters out meaningful phrases like “the global south” that otherwise would feature prominently.\r\nTerm frequency–inverse document frequency\r\nGoing further than just counting words and ngrams, we can look at their relevance in comparison to the whole of all panel descriptions. This addresses the issue that terms like “science and technology” or “this open panel” are going to feature in a large number of panel descriptions and therefore don’t add a lot of information. To do this, we look at the term frequency–inverse document frequency (tf-idf), “a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.”(“Tfidf” 2021) The tidytext package has the convenient bind_tf_idf() function for this:\r\n\r\n\r\nShow code\r\n\r\npanels_full_wide %>% \r\n  unnest_tokens(word, desc, token = \"words\") %>% \r\n  count(word, id) %>% \r\n  bind_tf_idf(word, id, n) %>%\r\n  left_join(panels_full_wide, by = \"id\") %>% \r\n  select(word, n, tf_idf, title) %>% \r\n  arrange(-tf_idf) %>% \r\n  head(500) %>% \r\n  paged_table()\r\n\r\n\r\n\r\n\r\n\r\nWe can do the same analysis but group the panels by theme to identify distinctive trigrams for each of the themes. This is where the long format of the panel data is needed. We won’t filter the trigrams for stop words here, as the tf-idf function will by itself get rid of phrases such as “as well as” or “this open panel.”\r\n\r\n\r\nShow code\r\n\r\npanels_full_long %>% \r\n  unnest_tokens(word, desc, token = \"ngrams\") %>% \r\n  group_by(theme) %>% \r\n  count(word, id) %>% \r\n  bind_tf_idf(word, id, n) %>%\r\n  select(theme, word, n, tf_idf) %>% \r\n  arrange(-tf_idf) %>% \r\n  head(500) %>% \r\n  paged_table()\r\n\r\n\r\n\r\n\r\n\r\nVisualizations\r\nTables are nice, but graphs are great too. Let’s create a network graph of the most common bigrams, using the igraph and ggraph packages.\r\n\r\n\r\nShow code\r\n\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\nbigram_count <- bigrams_filtered %>% count(word1, word2, sort = TRUE)\r\n\r\n\r\nbigram_graph <- bigram_count %>%\r\n  filter(n > 8) %>%\r\n  graph_from_data_frame()\r\n\r\na <- grid::arrow(type = \"closed\", length = unit(.10, \"inches\"))\r\n\r\nggraph(bigram_graph, layout = \"fr\") +\r\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,\r\n                 arrow = a, end_cap = circle(.05, 'inches')) +\r\n  geom_node_point(color = \"lightblue\", size = 5) +\r\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\r\n  theme_void()\r\n\r\n\r\n\r\n\r\nAnd let’s end on the OG of text visualizations: the word cloud (or in this case: the bigram cloud). To make it pretty, we filtered for a number of manually defined stop words such as “sts,” “panel,” or “university press.”\r\n\r\n\r\nShow code\r\n\r\nlibrary(wordcloud)\r\ncustom_stop_words <- c(\"panel\",\r\n                       \"panels\",\r\n                       \"sts\",\r\n                       \"paper\",\r\n                       \"papers\",\r\n                       \"mit\",\r\n                       \"press\")\r\ncustom_stop_bigrams <- c(\"university press\",\r\n                       \"conference theme\",\r\n                       \"durham duke\",\r\n                       \"duke university\",\r\n                       \"encourage papers\",\r\n                       \"science technology\",\r\n                       \"science studies\",\r\n                       \"social studies\",\r\n                       \"technology studies\")\r\n\r\nbigrams_filtered %>% \r\n  filter(!word1 %in% custom_stop_words) %>% \r\n  filter(!word2 %in% custom_stop_words) %>% \r\n  mutate(word = paste(word1, word2)) %>% \r\n  filter(!word %in% custom_stop_bigrams) %>% \r\n  count(word, sort = T) %>% \r\n  with(wordcloud(word, n, max.words = 25, scale = c(2.5, .5),random.order = FALSE, colors = brewer.pal(8,\"Dark2\")))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n“Tfidf.” 2021. https://en.wikipedia.org/w/index.php?title=Tf.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/scraping-4s/exploring_SSSS_panels_files/figure-html5/unnamed-chunk-23-1.png",
    "last_modified": "2021-04-10T15:32:26-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/madison-common-council-advisory-referendum-2021/",
    "title": "Madison Common Council Advisory Referendum 2021",
    "description": "How precincts voted about the questions whether Madison Alder should be a full-time job and how long terms should be",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-04-07",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nPrecincts with fewer than 25 total votes are not shown. Data: Dane County Elections\r\nShould Alder be a full-time job?\r\n\r\n\r\n\r\n\r\n\r\n\r\nShould terms for Alders be two or four years?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/madison-common-council-advisory-referendum-2021/screenshot_map.png",
    "last_modified": "2021-04-07T11:01:04-05:00",
    "input_file": {},
    "preview_width": 927,
    "preview_height": 569
  },
  {
    "path": "posts/dane-county-migration-flows/",
    "title": "Dane County Migration Flows",
    "description": "The `tidycensus` package got an exciting new function",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-04-02",
    "categories": [],
    "contents": "\r\nI’m a heavy user of the tidycensus package, and just yesterday I learned that its development version has a great new feature: You can now retrieve migration flows for counties and metro areas! Of course I needed to test this out, looking at Madison (or to be precise: Dane County). Our city and region have been steadily growing, and so where have people been moving from? And where do the people go who leave? It now only takes a few lines of code to find out.\r\n\r\nNote that you need the install the Github version of tidycensus for this to work. To do so, use remotes::install_github(\"https://github.com/walkerke/tidycensus/\").\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidycensus)\r\nlibrary(tidyverse)\r\nlibrary(mapdeck)\r\ndane_flows <- get_flows(\r\n  geography = \"county\",\r\n  state = \"WI\",\r\n  county = \"Dane\",\r\n  year = 2018, #this is the latest data available currently\r\n  geometry = TRUE\r\n  )\r\n\r\n\r\n\r\nWhere people are moving from\r\n\r\n\r\nShow code\r\n\r\ntop_move_in_table <- dane_flows %>% \r\n  filter(variable == \"MOVEDIN\") %>% \r\n  slice_max(n = 25, order_by = estimate) %>% \r\n  mutate(\r\n    width = estimate / 400,\r\n    tooltip = paste0(\r\n      scales::comma(estimate * 5, 1),\r\n      \" people moved from \", FULL2_NAME,\r\n      \" to \", FULL1_NAME, \" between 2014 and 2018\"\r\n      )\r\n    )\r\nrmarkdown::paged_table(top_move_in_table %>% select(FULL2_NAME, estimate))\r\n\r\n\r\n\r\n\r\n\r\nThe top county for in-migration is actually a continent: Asia. And another continent, Europe, also makes the top-10. Most of the counties people are coming from are in-state, and probably a good portion is students moving to attend UW-Madison. Cook County – in other words: Chicago – takes first spot for out-of-state mover, and Hennepin County (Minneapolis) follows closely. Farther away, there is Hamilton County in Ohio (Cincinatti), Middlesex County in Massachussetts, just outside Boston, and one West Coast county: San Diego, California.\r\nWe can map these flows, but for non-US origins/destinations, no geometry is provided. If we wanted to map these, we’d manually have to add these. For this article, we’ll filter them out.\r\n\r\n\r\nShow code\r\n\r\ntop_move_in_table %>% \r\n  filter(!is.na(GEOID2)) %>% \r\n  mapdeck(style = mapdeck_style(\"light\"), pitch = 45) %>% \r\n  add_arc(\r\n    origin = \"centroid1\",\r\n    destination = \"centroid2\",\r\n    stroke_width = \"width\",\r\n    auto_highlight = TRUE,\r\n    highlight_colour = \"#8c43facc\",\r\n    tooltip = \"tooltip\"\r\n  ) \r\n\r\n\r\n\r\n\r\nWhere people are moving to\r\n\r\n\r\nShow code\r\n\r\ntop_move_out_table <- dane_flows %>% \r\n  filter(variable == \"MOVEDOUT\") %>% \r\n  slice_max(n = 25, order_by = estimate) %>% \r\n  mutate(\r\n    width = estimate / 400,\r\n    tooltip = paste0(\r\n      scales::comma(estimate * 5, 1),\r\n      \" people moved from \", FULL1_NAME,\r\n      \" to \", FULL2_NAME, \" between 2014 and 2018\"\r\n      )\r\n    )\r\n\r\nrmarkdown::paged_table(top_move_out_table %>% select(FULL2_NAME, estimate))\r\n\r\n\r\n\r\n\r\n\r\nOne limitation of the data is that for out-migration (and consequently net migration), there is no data for people leaving the US – someone who has moved to South America won’t receive an American Community Survey. With that in mind, destinations are spread more widely. The West Coast draws heavily with Seattle, the Bay Area, Los Angeles, and San Diego. Austin (Texas), and of course New York City, and the head scratcher of Harrisburg (Pennsylvania). Closer by, the same places that show up for in-migration also feature here: Minneapolis, Chicago, Milwaukee, and various counties across Wisconsin.\r\n\r\n\r\nShow code\r\n\r\ntop_move_out_table %>% \r\n  filter(!is.na(GEOID2)) %>% \r\n  mapdeck(style = mapdeck_style(\"light\"), pitch = 45) %>% \r\n  add_arc(\r\n    origin = \"centroid1\",\r\n    destination = \"centroid2\",\r\n    stroke_width = \"width\",\r\n    auto_highlight = TRUE,\r\n    highlight_colour = \"#8c43facc\",\r\n    tooltip = \"tooltip\"\r\n  ) \r\n\r\n\r\n\r\n\r\nNet migration\r\nThe data also include a variable for net migration, MOVEDNET.\r\n\r\n\r\nShow code\r\n\r\ntop_move_net_table <- dane_flows %>% \r\n  filter(variable == \"MOVEDNET\") %>% \r\n  slice_max(n = 25, order_by = estimate) %>% \r\n  mutate(\r\n    width = estimate / 400,\r\n    tooltip = paste0(\r\n      scales::comma(estimate * 5, 1),\r\n      \" more people moved from \", FULL2_NAME,\r\n      \" to \", FULL1_NAME, \" between 2014 and 2018 than in the reverse direction\"\r\n      )\r\n    )\r\nrmarkdown::paged_table(top_move_net_table %>% select(FULL2_NAME, estimate))\r\n\r\n\r\n\r\n\r\n\r\nThe numbers here are smaller, but you can see that Waukesha, which was at the top of the total in-migration list also tops the net migration. That is, 1445 more people moved from Waukesha to Dane County than in the other direction. Overall, the numbers here are much smaller, and the maps looks a little different again.\r\n\r\n\r\nShow code\r\n\r\ntop_move_net_table %>% \r\n  filter(!is.na(GEOID2)) %>% \r\n  mapdeck(style = mapdeck_style(\"light\"), pitch = 45) %>% \r\n  add_arc(\r\n    origin = \"centroid1\",\r\n    destination = \"centroid2\",\r\n    stroke_width = \"width\",\r\n    auto_highlight = TRUE,\r\n    highlight_colour = \"#8c43facc\",\r\n    tooltip = \"tooltip\"\r\n  ) \r\n\r\n\r\n\r\n\r\nAge groups\r\nThe UW-Madison is probably a big driver of migration in Dane County, and we can look at this by grouping in-migration by age group.\r\n\r\nNote that this is 2011–2015 data, not the 2014–2018 data used in the rest of the article.\r\n\r\n\r\nShow code\r\n\r\nage_flows <- get_flows(\r\n  geography = \"county\",\r\n  county = \"Dane\",\r\n  state = \"WI\",\r\n  breakdown = \"AGE\",\r\n  breakdown_labels = TRUE,\r\n  year = 2015\r\n  )\r\n\r\n\r\nage_flows %>% \r\n  filter(variable == \"MOVEDIN\", AGE_label != \"All ages\") %>% \r\n  group_by(AGE_label, FULL2_NAME) %>% \r\n  summarize(estimate) %>% \r\n  slice_max(n = 5, order_by = estimate) %>% \r\n  arrange(AGE_label,-estimate) %>% \r\n  DT::datatable(rownames = FALSE)\r\n\r\n\r\n\r\n\r\nThese are the top-5 origins for each age group. If you sort by the estimate column, you see the largest in-migrant group are 20–24 year-olds from Asia, and overall the top groups are indeed heavily undergraduate- and graduate-student aged. Electronic health record company Epic, one of the largest employers in the country, is probably driving some of these numbers as wel. Sort by AGE_label to see the top origins for each age group.\r\n\r\n\r\n\r\n",
    "preview": "posts/dane-county-migration-flows/img/screenshot_flow_map.png",
    "last_modified": "2021-04-02T08:56:56-05:00",
    "input_file": {},
    "preview_width": 935,
    "preview_height": 576
  },
  {
    "path": "posts/raceethnicity-by-ward/",
    "title": "Race demographics by aldermanic district",
    "description": "What are the racial demographics of Madison's aldermanic districts",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-02-11",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe American Community Survey includes data on race and ethnicity. Madison’s aldermanic districts are comprised of census blocks, but for privacy reason, most ACS data is not available down to the block or even block group level. Data on race is available at the block group level, and we can approximate district boundaries based on this. Note that the maps below only display race, not ethnicity. Ethnicity data unfortunately is not available at the block group level. Data are 5-year estimates, covering 2015–19.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNote that each map has its own legend and the colors denote different percentages! Click on the aldermanic districts to see the percentages for each group.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-11T13:13:09-06:00",
    "input_file": {}
  },
  {
    "path": "posts/commuteflowsinmadison/",
    "title": "Visualizing commute flows in Wisconsin",
    "description": "LODES provides origin-destination data for connecting work and home. This article looks at patterns in those flows in Wisconsin, and especially in Madison",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIn previous posts I have worked with data from the US Census Bureau’s American Community Survey (ACS). ACS is great for data on people and households. For employment data, however, there is a better (or maybe complementary) source: The LEHD Origin-Destination Employment Statistics (LODES). The data can be accessed via a somewhat clunky web interface, On the Map or downloaded and then analyzed in your software of choice. One unique feature of LODES is the “OD” part in its name: It provides the number of people who commute from one census block to another. This can be helpful in transportation or general urban planning. For example, are there major employment centers that have a large share of its workers living in the same part of town, and could they therefore easily be served by transit?\r\nIn this post I will look at LODES data for Madison and the state of Wisconsin as a whole. The latest LODES data available is from 2018.\r\n\r\n\r\n\r\nData preparation\r\n\r\n\r\n\r\nThe file structure is explained here, page 6. There are two geography identifier variables, w_geocode and h_geocode, and a number of variables for the number of jobs for that geography pair. S000 is the one for total jobs.\r\n\r\n\r\n\r\nThe mapdeck arc visualization requires either two pairs of coordinates or sfc columns.\r\nI will get TIGER geographies with the tigris package, calculate centroids with sf, and then merge with the LODES data. Note that LODES uses the 2010 vintage for its geographies, whereas tigris defaults to the most recent vintage.\r\n\r\n\r\n\r\n\r\n\r\n\r\nLet’s look at the distribution of the number of jobs per origin-destination pair:\r\n\r\n\r\n\r\nThis does not look terribly promising–maybe block pairs are the wrong unit of analysis, with a huge number of them only having a single job-home pair. Aggregation to block groups or tracts may make more sense, but for now I’ll proceed with the blocks.\r\nCommute flows in Wisconsin\r\nThe number of block pairs is large, and so I’ll sample the 100 blocks with the largest number of commutes.\r\n\r\n\r\n\r\nNow we can do a double join with the block centroids: First, join on the work geocode, then do another join on the home geocode. This will create a new dataframe with two geometry columns. In order to work with the mapdeck package, the dataframe needs to be turned into an sf object. Finally, we add a thickness helper variable based on the number of jobs in the OD pair. This variable will specify the width of the arcs in the visualization.\r\n\r\n\r\n\r\nCensus blocks can be oddly shaped, and so it’s a good idea to plot the actual blocks and not just their centroids.\r\n\r\n\r\n\r\nVisualizing the flows\r\nNow we can visualize the top-100 commute flows in the state:\r\n\r\n\r\n\r\nWow, it is immediately obvious how dominant commute flows in the Madison area are! And by zooming in you can identify some other patterns:\r\nIn Madison two employers capture almost all commute flows: The UW-Madison downtown1 and electronic health record company Epic in suburban Verona. The only other employer making an appearance is the university hospital (which is a separate entity from the university)\r\nIn Milwaukee, by far the state’s largest city, there are only three OD pairs in total: Two point at the 32-story Northwestern Mutual Tower downtown; the third leads to the large suburban medical campus that includes the Medical College of Wisconsin, Froedtert Hospital, and the Childrens’ Hospital of Wisconsin.\r\nSheboygan is probably best known for plumbing product company Kohler, and the Kohler Co. headquarters make a prominent appearance on the map, with 8 arcs pointing at it.\r\nFor the remainder of OD pairs around the state, it’s primarily meat processing, healthcare facilities, and tribal facilities that appear as destinations.\r\nFor two OD pairs, home and work are actually in the same block: One is the UW in Platteville (presumably students living and working on campus), and a strangely empty looking wooded block near Hayward. My best guess is that it’s a farm or forestry business.\r\nMadison flows\r\nGiven the visual prominence of Madison on the map (and my personal interest in Madison), it makes sense to do a separate analysis just for flows within Dane County.\r\n\r\n\r\n\r\n\r\n\r\n\r\nWell, this doesn’t look all that different, does it? More arcs, but all of them still point at the UW, Epic, and the university hospital. This doesn’t change even if you look at the top-150 OD pairs. Only once you bump it up to 300, a few more work locations appear.\r\n\r\n[1] \"550250011021004\" \"550250108003008\" \"550250032001008\"\r\n[4] \"550250114022021\" \"550250114022035\" \"550250017042003\"\r\n\r\nPlotting 300 arcs of course is a bit of a mess:\r\n\r\n\r\n\r\nIf you scroll around enough you see the City-County Building, American Family Insurance, and a block with several employers near the Am Fam campus.\r\nAmerican Family Insurance and some other employment at the nothern edge of townSome observations:\r\nThere are about 20 contiguous blocks in the Bassett and Mifflin neighborhood that have a lot of people commuting to Epic. That area is well served by the 75 bus providing peak-hour service to and from Epic in 35 to 40 minutes\r\nThere are surprisingly few commute blocks on the east side. For Epic employees that makes sense, as traveling through the isthmus takes a good amount of time, but I would have expected more prominent UW flows\r\nThe “Epic blocks” downtownAggregating to block groups\r\nI mentioned at the beginning of the article that census blocks tend to be fairly small. What happens when we aggregate them into block groups?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWith the cut-off set to the 100 most frequent OD pairs, a few additional employment centers make an appearance, along the Beltline in Middleton. Other than that, there is not much insight to be gained. And especially along the edges of town, the block groups are quite large. If, for example, your goal is transit planning, using blocks is probably the better choice.\r\nConclusion\r\nThis is my first time extensively working with LODES origin-destination data and I have learned a lot in terms of data prep and visualization. In terms of actual insights, I am a bit disappointed. Everything being so centered on the UW and Epic leaves little opportunity to learn about other employment centers. But maybe the realization just how dominant these employers are in terms of commute flows is a valuable insight in itself. How, for example, could all those Epic employees on the west side of Madison and in Middleton be better served by transit? And why do Epic employees seemingly cluster more in where they live? The trope of “Epic luxury apartments” downtown is well known—and appears to have some truth to it.\r\n\r\nFor some reason, the university reports all(?) of its employees as working at a single location. In reality, UW employment is more spread out.↩︎\r\n",
    "preview": "posts/commuteflowsinmadison/images/preview.png",
    "last_modified": "2021-01-25T17:10:35-06:00",
    "input_file": {},
    "preview_width": 1225,
    "preview_height": 714
  },
  {
    "path": "posts/getting-to-work-in-madison/",
    "title": "Getting to work in Madison",
    "description": "Commute mode share by minority status and income",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2020-12-25",
    "categories": [],
    "contents": "\r\nHow do people get to work and back in Madison? And how does this differ for people of different economic status and racial and ethnic backgrounds? I had some idea about what this looked like at the national level, but I was always curious what the numbers would be for Madison. I have previously written about commute mode share in Madison over time, based on data from the American Community Survey (ACS). The ACS does have detailed information on race, ethnicity, and income, but what is publicly available on the ACS website doesn’t lend itself to this type of analysis. Fortunately, there is another data source: CTPP. This stands for Census Transportation Planning Products, “a State DOT-funded, cooperative program that produces special tabulations of American Community Survey (ACS) data that have enhanced value for transportation planning, analysis, and strategic direction.” So same data source, but organized in a different way. And the latest data available is for 2012–2016.\r\n\r\nI recommend reading my other article on bike mode share in Madison to get a sense of the limitations of ACS data. It only captures commute trips, and the margins of error can be sizable.\r\nCommuting and minority status\r\nThe way CTPP treats race and ethnicity is by putting people into two categories: White people who aren’t Latinx/Hispanic, and everyone else. There are all kinds of issues with splitting things up this way, but using a more fine-grained approach to race and ethnicity would lead to small groups in each category and less reliable data.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis is the same data but in a table.\r\n\r\n\r\nCommute mode\r\n\r\n\r\nNon-Hispanic White\r\n\r\n\r\nRacial/ethnic minority\r\n\r\n\r\nTotal\r\n\r\n\r\nDifference\r\n\r\n\r\nDrove alone\r\n\r\n\r\n65.3%\r\n\r\n\r\n55.8%\r\n\r\n\r\n63.4%\r\n\r\n\r\n9.5%\r\n\r\n\r\nBus\r\n\r\n\r\n7.7%\r\n\r\n\r\n15.4%\r\n\r\n\r\n9.3%\r\n\r\n\r\n-7.7%\r\n\r\n\r\nCarpooled\r\n\r\n\r\n6.5%\r\n\r\n\r\n11.7%\r\n\r\n\r\n7.6%\r\n\r\n\r\n-5.1%\r\n\r\n\r\nBiked\r\n\r\n\r\n5.7%\r\n\r\n\r\n3.1%\r\n\r\n\r\n5.2%\r\n\r\n\r\n2.6%\r\n\r\n\r\nWorked at home\r\n\r\n\r\n4.6%\r\n\r\n\r\n2.4%\r\n\r\n\r\n4.1%\r\n\r\n\r\n2.2%\r\n\r\n\r\nWalked\r\n\r\n\r\n9.3%\r\n\r\n\r\n10.3%\r\n\r\n\r\n9.5%\r\n\r\n\r\n-1.0%\r\n\r\n\r\nother\r\n\r\n\r\n0.8%\r\n\r\n\r\n1.4%\r\n\r\n\r\n1.0%\r\n\r\n\r\n-0.5%\r\n\r\n\r\nThe rightmost column shows the difference in percentage points between the two population groups (i.e. the distance between the two dots on the previous plot). That difference is largest for driving to work alone: The rate of driving alone is almost 10 percentage points higher for non-Hispanic White commuters. Depending on what question we’re trying to answer, it may be more useful to put this difference in the context of overall commute rates. Yes, there is a large difference in driving alone rates, but it also the overall most common commute mode. Compare that to bus commuters: The rate of bus commuting for people belonging to a racial or ethnic minority is twice as high as that for non-Hispanic White commuters, 15.4% versus 7.7%. With the difference in the reverse direction, the rate of bike commuters and people working from home is close to twice as high for non-Hispanic White workers. The only mode where rates are more or less the same for both groups is walking to work, at around 10 percent.\r\nIncome\r\nA different way to look at commute mode is by household income. Of course, income and race/ethnicity are not independent of each other. But especially with the coarse distinction between non-Hispanic White versus everyone else, looking at income can bring additional insights.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFor most commute modes, there is a clear trend across income groups: The more you make, the more likely you are to drive alone, and the less likely you are to walk or take the bus to work. It’s a little more complicated for other modes:\r\nBiking is most common for people in low-income households and then drops for with increasing income. However, at the top of the scale, in households with an income over $150,000, the trend reverses and more people bike to work.\r\nThe inverse is true for working from home: The rate is highest at the highest incomes and then drops with lower income. But for people in households making less than $15,000, it goes up again.\r\nFinally, for carpooling there doesn’t seem to be a clear trend. Between $15,000 and $100,000 household income, the rates are more or less the same. Very low income household don’t carpool much; and for some reason, incomes between $125,000 and $150,000 have the highest rate of carpooling.\r\n\r\n\r\n\r\n",
    "preview": "posts/getting-to-work-in-madison/article_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-12-25T15:25:33-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/bus-boardings-in-madison/",
    "title": "Bus boardings in Madison",
    "description": "Using the `mapdeck` package to visualize Metro Transit bus boardings in Madison (WI)",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2020-10-18",
    "categories": [],
    "contents": "\r\nMadison Metro publishes data boardings per stop on the City of Madison Open Data Portal. The data is also made available on what colloquially is known as the “red dot map” – which isn’t the most appealing visualization format:\r\nThe “red dot map.” Screenshot from City of Madison Open Data PortalHere I’m using the mapdeck package to create an interactive 3D map visualization of the data, augmented with bus routes and a population density heatmap.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBoardings only\r\nZoom in to see the data. Hover over the columns for more information. Right-click and drag to change the view angle and orientation.\r\n\r\n\r\n\r\nThe map really shows how much ridership is driven by the UW Campus and downtown. Also very visible are the four transfer points.\r\nAdd Metro routes\r\nI’m using the Metro GTFS data to add all routes to the map. You will notice that there are stops on the map that aren’t on any route. This is because the boardings data is from just before the pandemic started, whereas the routes are current and include the ones that have been cut.\r\n\r\n\r\n\r\n\r\n\r\n\r\nAdd population density\r\nDensity is one important condition for high transit ridership. So let’s add a heatmap with population density as a layer. Job density would be another interesting variable to look at, but I’ll leave that for another day.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/bus-boardings-in-madison/data/screenshot_red_dot_map.png",
    "last_modified": "2021-01-28T10:31:18-06:00",
    "input_file": {},
    "preview_width": 1917,
    "preview_height": 976
  },
  {
    "path": "posts/2020-03-05-transportation-funding-wisconsin/",
    "title": "Transportation funding in Wisconsin remains car centric",
    "description": "A look at the 2020 Multimodal Local Supplement Awards in Wisconsin to assess how multimodal they really are",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2020-03-05",
    "categories": [],
    "contents": "\r\nTony Evers, Wisconsin’s Democratic governor, announced $75 million of funding for local transportation projects yesterday. The title of the program, “Multimodal Local Supplement Awards,” may imply that the money supports projects that aren’t just road repairs and highway expansions. But of course it’s good to be skeptical and look at the actual project list.\r\nThe list is available as a PDF document and so the first step is to use Acrobat to convert it into an Excel sheet. (If there is an R package to directly read in PDFs, let me know.). The resulting XLSX file comes out pretty clean and doesn’t need much prep after reading it in with readxl.\r\n\r\n\r\n\r\nOne thing that needs to be fixed are some of the factors of the modal_type variable. I’ll combine all projects that don’t include anything related to pedestrians and bikes into one factor and all multimodal projects that do include a ped/bike component into a second factor; all other factors are kept as is. The fct_collapse function from the forcats package is great for this:\r\n\r\n\r\n\r\nNow all that remains is to create summary measures by modal_type:\r\n\r\n\r\n\r\nFor plotting it’s a good idea to reorder the bars by total amount with fct_reorder. And here’s the final product:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-03-05-transportation-funding-wisconsin/transportation-funding-wisconsin_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-12-25T18:16:30-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-06-25-some-furter-zoning-analyses-for-madison/",
    "title": "Some furter zoning analyses for Madison",
    "description": "A follow-up post on zoning and density in Madison and how it compares to other cities in the US.",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2019-06-25",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nI got some good feedback after posting my article on Madison’s zoning restrictions on Facebook and Reddit, prompting some additional analyses.\r\nWhy do you look only at residentially zoned land?\r\nSeveral people pointed out that comparing the zoning districts that only allow detached single-family homes (SFR) to all residential zoning districts may be misleading. In many cities the densest parts are not in residential zoning districts but in downtown areas zoned for mixed use. I think that’s a fair point, but it was the original NYT article that chose only residential areas as the comparator, and I wanted to make Madison comparable to the other cities in the NYT analysis.\r\nNonetheless, I quickly ran the analysis comparing SFR to any district except “special districts.” These includes parks, conservancies, agricultural land, airports, or the UW-Campus—in other words districts that are highly unlikely to ever have residential use.\r\n\r\n\r\n\r\n\r\n\r\n#detached single-family only\r\nres_sfr <- data %>%\r\n  filter(ZONING_CODE %in% sfr_zones) %>%\r\n  summarize(sum(ShapeSTArea))\r\n# only special district area\r\nother_area <- data %>%\r\n  filter(ZONING_CODE %in% other) %>%\r\n  summarise(sum(ShapeSTArea))\r\ntotal <- data %>% summarize(sum(ShapeSTArea))\r\nres_sfr/(total-other_area)\r\n\r\n\r\n  sum(ShapeSTArea)\r\n1        0.4873358\r\n\r\nThe result? 49% of all land that reasonably could have residential units on it allows only single-family attached housing. Since there is no point of comparison to other cities, it’s hard to interpret that number.\r\nIs there a correlation between the percentage of SFR zoning and overall density?\r\nIt’s a good question: Does the metric the NYT used actually have an association with overall density of a city? (Note that even if there is a correlation, this absolutely doesn’t mean there is a causal connection! There are many, many factors that impact a city’s overall density.)\r\nThere are few data points to work with. I retrieved density information for all cities in the NYT article and Madison from Wikipedia:\r\n\r\n\r\nCity\r\n\r\n\r\nDensity (pop./sq.mi.)\r\n\r\n\r\nSingle-family (%)\r\n\r\n\r\nNew York\r\n\r\n\r\n27751\r\n\r\n\r\n15\r\n\r\n\r\nWashington\r\n\r\n\r\n11367\r\n\r\n\r\n36\r\n\r\n\r\nSeattle\r\n\r\n\r\n8642\r\n\r\n\r\n81\r\n\r\n\r\nLos Angeles\r\n\r\n\r\n8483\r\n\r\n\r\n75\r\n\r\n\r\nMinneapolis\r\n\r\n\r\n7821\r\n\r\n\r\n70\r\n\r\n\r\nSan Jose (CA)\r\n\r\n\r\n5776\r\n\r\n\r\n94\r\n\r\n\r\nPortland (OR)\r\n\r\n\r\n4504\r\n\r\n\r\n77\r\n\r\n\r\nArlington (TX)\r\n\r\n\r\n3810\r\n\r\n\r\n89\r\n\r\n\r\nMadison\r\n\r\n\r\n3233\r\n\r\n\r\n75\r\n\r\n\r\nSandy Springs (GA)\r\n\r\n\r\n2707\r\n\r\n\r\n85\r\n\r\n\r\nCharlotte (NC)\r\n\r\n\r\n2400\r\n\r\n\r\n84\r\n\r\n\r\nLet’s see what the overall correlation is:\r\n\r\n# A tibble: 1 x 1\r\n  `cor(density, sfr)`\r\n                <dbl>\r\n1              -0.885\r\n\r\nHm, -0.89. That’s a very, very high correlation. But it’s always good to look at your data points in a scatterplot to see what’s actually going on:\r\n\r\n\r\n\r\nOkay, New York is clearly way out there. Let’s exclude NYC and do the correlation again:\r\n\r\n\r\n dens_zon %>% \r\n   spread(Measure, Value) %>%\r\n   filter(City != \"New York\") %>%\r\n   summarize(cor(density, sfr))\r\n\r\n\r\n# A tibble: 1 x 1\r\n  `cor(density, sfr)`\r\n                <dbl>\r\n1              -0.693\r\n\r\nThat gets us a correlation of -0.69. Much lower, but still pretty high. Is DC also an outlier?\r\n\r\n\r\n dens_zon %>% \r\n   spread(Measure, Value) %>%\r\n   filter(City != \"New York\" & City != \"Washington\") %>%\r\n   summarize(cor(density, sfr))\r\n\r\n\r\n# A tibble: 1 x 1\r\n  `cor(density, sfr)`\r\n                <dbl>\r\n1              -0.366\r\n\r\nThis lower the correlation to -0.37. That is, there is a small negative association between how dense a city is and how much of its residential land is zoned exclusively for detached single-family homes. But more data is needed to confirm this.\r\n\r\n\r\n\r\n",
    "preview": "posts/2019-06-25-some-furter-zoning-analyses-for-madison/some-furter-zoning-analyses-for-madison_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2020-12-27T19:33:59-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/how-much-of-madison-allows-only-detached-single-family-housing/",
    "title": "How much of Madison allows only detached single-family housing?",
    "description": "Recreating a _New York Times_ analysis of how much of Madison's residential areas only allowed single-family detached homes",
    "author": [
      {
        "name": "Harald Kliems",
        "url": "https://haraldkliems.netlify.app/"
      }
    ],
    "date": "2019-06-19",
    "categories": [],
    "contents": "\r\nBackground\r\nThe New York Times recently published an analaysis of how much land in several US cities only allows the construction of detached single-family houses. https://www.nytimes.com/interactive/2019/06/18/upshot/cities-across-america-question-single-family-zoning.html\r\nThe cities analyzed were\r\nCity\r\nProportion detached single-family to all housing\r\nNew York\r\n15%\r\nWashington\r\n36%\r\nMinneapolis\r\n70%\r\nLos Angeles\r\n75%\r\nPortland (OR)\r\n77%\r\nSeattle\r\n81%\r\nCharlotte (NC)\r\n84%\r\nSandy Springs (GA)\r\n85%\r\nArlington (TX)\r\n89%\r\nSan Jose (CA)\r\n94%\r\nI immediately wondered how my current hometown, Madison (WI), would compare.\r\nData sources\r\nMadison zoning data is available from the city’s Open Data portal: https://data-cityofmadison.opendata.arcgis.com/datasets/zoning-districts\r\nlicensed under the [City of Madison Data Policy}(https://www.cityofmadison.com/policy/data)\r\nalready contains the area for each polygon\r\n\r\nThe meaning of each zoning district is documented in MuniCode, specifically 28.032 and 28.033\r\nAnalysis\r\nI’ll work with R’s tidyverse package.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nLoad the data with the read_csv function.\r\n\r\n\r\ndata <- read_csv(\r\n  \"https://opendata.arcgis.com/datasets/4bfd0deffa8f4d5f8eefe868ab91493c_9.csv\",\r\n  col_types = \"iccccccdd\")\r\n\r\n\r\n\r\nDetermine what counts as residential\r\nThe New York Times analysis focused exclusively on residential districts, leaving out commercial or mixed use districts: “These maps highlight the land exclusively set aside for housing.” Ordinance 28.032 defines the following as residential districts: SR-C1, SR-C2, SR-C3, SR-V1, SR-V2, TR-C1, TR-C2, TR-C3, TR-C4, TR-V1, TR-V2, TR-U1, TR-U2, TR-R, TR-P\r\nI will therefore excldue all other Madison zoning codes from the analysis.\r\n\r\n\r\n#create variables for residential zoning and single-family zoning codes\r\nres_zones <- c(\"SR-C1\", \r\n                 \"SR-C2\", \r\n                 \"SR-C3\", \r\n                 \"SR-V1\",\r\n                 \"SR-V2\",\r\n                 \"TR-C1\",\r\n                 \"TR-C2\", \r\n                 \"TR-C3\",\r\n                 \"TR-C4\",\r\n                 \"TR-V1\",\r\n                 \"TR-V2\", \r\n                 \"TR-U1\", \r\n                 \"TR-U2\", \r\n                 \"TR-R\",\r\n                 \"TR-P\")\r\n\r\n\r\n\r\nDetermine what counts as “zoned for detached single-family homes”\r\nThe Times focuses on “codes devoted to detached single-family homes, grouping rowhouses more common in older East Coast cities like Washington and New York into a second category covering all other housing types.”\r\nOrdinance 28.033 lists the following building forms:\r\nTable of Residential Building FormsDetermining what falls under detached single-family homes appears straightforward: SR-C1, SR-C2, TR-C1, TR-C2, TR-C3, TR-R.\r\n\r\n\r\nsfr_zones <- c(\"SR-C1\", \r\n         \"SR-C2\",\r\n         \"TR-C1\", \r\n         \"TR-C2\", \r\n         \"TR-C3\", \r\n         \"TR-R\")\r\n\r\n\r\n\r\nNow all that remains is to sum up the areas for all single-family detached and divide by the total residential area.\r\n\r\n\r\nres_total <- data %>%\r\n  filter(ZONING_CODE %in% res_zones) %>%\r\n  summarize(sum(ShapeSTArea))\r\n\r\nres_sfr <- data %>%\r\n  filter(ZONING_CODE %in% sfr_zones) %>%\r\n  summarize(sum(ShapeSTArea))\r\n\r\nSFR_ratio <- round((res_sfr/res_total)*100, digits = 0)\r\n\r\n\r\n\r\nResults\r\nSo pretty much exactly 75 percent of all residentially zoned land in Madison allows only detached single-family housing. When I had first read the NYT article, my guess was Madison would be between somewhere 75 and 85 percent. I’m happy to see that the actual number is at the lower bounds of my guess, and fairly good compared to the cities mentioned in the original article:\r\nCity\r\nProportion single-family detached to all housing\r\nNew York\r\n15%\r\nWashington\r\n36%\r\nMinneapolis\r\n70%\r\nMadison (WI)\r\n75%\r\nLos Angeles\r\n75%\r\nPortland (OR)\r\n77%\r\nSeattle\r\n81%\r\nCharlotte (NC)\r\n84%\r\nSandy Springs (GA)\r\n85%\r\nArlington (TX)\r\n89%\r\nSan Jose (CA)\r\n94%\r\nBut of course it also means that a huge proportion of our residential lands is off limits for even medium density development.\r\nA map created in QGIS and styled to look roughly similar to the NYT maps helps visualize this:\r\nMap of all residential and single-family detached zoning in MadisonLimitations\r\nIn a Twitter thread, Christopher Schmidt pointed out that even cities that seemingly don’t limit most parts of the city to single-family homes, requirements such as lot sizes or building setbacks effectively still make it impossible to build anything but single-family homes.\r\n\r\n\r\n\r\n",
    "preview": "posts/how-much-of-madison-allows-only-detached-single-family-housing/images/residential-zoning-map-madison.png",
    "last_modified": "2020-12-25T18:00:11-06:00",
    "input_file": {},
    "preview_width": 3507,
    "preview_height": 2480
  }
]
